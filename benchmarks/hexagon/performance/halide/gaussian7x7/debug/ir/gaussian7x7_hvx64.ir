+ ./tmp/gaussian7x7_generator -o tmp -e o,h,assembly,bitcode -g gaussian7x7 -f gaussian7x7_hvx64 target=hexagon-32-noos-no_bounds_query-no_asserts-hvx_64
Section: 
Section: .interp
Section: .note.ABI-tag
Section: .note.gnu.build-id
Section: .gnu.hash
Section: .dynsym
Section: .dynstr
Section: .gnu.version
Section: .gnu.version_r
Section: .rela.dyn
Section: .rela.plt
Section: .init
Section: .plt
Section: .plt.got
Section: .text
Section: .fini
Section: .rodata
Section: .eh_frame_hdr
Section: .eh_frame
Section: .gcc_except_table
Section: .tbss
Section: .init_array
Section: .fini_array
Section: .data.rel.ro
Section: .dynamic
Section: .got
Section: .data
Section: .bss
Section: .comment
Section: .debug_aranges
Section: .debug_info
Section: .debug_abbrev
Section: .debug_line
Section: .debug_str
Section: .debug_loc
Section: .debug_ranges
Section: .symtab
Section: .strtab
Section: .shstrtab
Generator gaussian7x7 has base_path tmp/gaussian7x7_hvx64
Creating initial loop nests...
Injecting realization of { output }
for (.__root, 0, 1) {
  produce output {
    let output.s0.y.loop_max = output.s0.y.max
    let output.s0.y.loop_min = output.s0.y.min
    let output.s0.y.loop_extent = ((output.s0.y.max + 1) - output.s0.y.min)
    let output.s0.x.loop_max = output.s0.x.max
    let output.s0.x.loop_min = output.s0.x.min
    let output.s0.x.loop_extent = ((output.s0.x.max + 1) - output.s0.x.min)
    let output.s0.__outermost.loop_extent = 1
    let output.s0.__outermost.loop_max = 0
    let output.s0.__outermost.loop_min = 0
    let output.s0.x.x.loop_extent = (((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64)
    let output.s0.x.x.loop_max = ((((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64) - 1)
    let output.s0.x.x.loop_min = 0
    let output.s0.x.xi.loop_extent = 64
    let output.s0.x.xi.loop_max = (64 - 1)
    let output.s0.x.xi.loop_min = 0
    let output.s0.y.y.loop_extent = (((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4)
    let output.s0.y.y.loop_max = ((((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4) - 1)
    let output.s0.y.y.loop_min = 0
    let output.s0.y.yi.loop_extent = 4
    let output.s0.y.yi.loop_max = (4 - 1)
    let output.s0.y.yi.loop_min = 0
    for<Hexagon> (output.s0.__outermost, output.s0.__outermost.loop_min, output.s0.__outermost.loop_extent) {
      for (output.s0.y.y, output.s0.y.y.loop_min, output.s0.y.y.loop_extent) {
        prefetch input()
        let output.s0.y.yi.base = ((output.s0.y.y*4) + output.s0.y.loop_min)
        for (output.s0.x.x, output.s0.x.x.loop_min, output.s0.x.x.loop_extent) {
          let output.s0.x.xi.base = ((output.s0.x.x*64) + output.s0.x.loop_min)
          unrolled (output.s0.y.yi, output.s0.y.yi.loop_min, output.s0.y.yi.loop_extent) {
            let output.s0.y = (output.s0.y.yi.base + output.s0.y.yi)
            vectorized (output.s0.x.xi, output.s0.x.xi.loop_min, output.s0.x.xi.loop_extent) {
              let output.s0.x = (output.s0.x.xi.base + output.s0.x.xi)
              output((output.s0.x.xi.base + output.s0.x.xi), (output.s0.y.yi.base + output.s0.y.yi)) = uint8(max(min(shift_right(cols((output.s0.x.xi.base + output.s0.x.xi), (output.s0.y.yi.base + output.s0.y.yi)), (uint32)12), 255), 0))
            }
          }
        }
      }
    }
  }
}

Inlining cols
for (.__root, 0, 1) {
  produce output {
    let output.s0.y.loop_max = output.s0.y.max
    let output.s0.y.loop_min = output.s0.y.min
    let output.s0.y.loop_extent = ((output.s0.y.max + 1) - output.s0.y.min)
    let output.s0.x.loop_max = output.s0.x.max
    let output.s0.x.loop_min = output.s0.x.min
    let output.s0.x.loop_extent = ((output.s0.x.max + 1) - output.s0.x.min)
    let output.s0.__outermost.loop_extent = 1
    let output.s0.__outermost.loop_max = 0
    let output.s0.__outermost.loop_min = 0
    let output.s0.x.x.loop_extent = (((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64)
    let output.s0.x.x.loop_max = ((((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64) - 1)
    let output.s0.x.x.loop_min = 0
    let output.s0.x.xi.loop_extent = 64
    let output.s0.x.xi.loop_max = (64 - 1)
    let output.s0.x.xi.loop_min = 0
    let output.s0.y.y.loop_extent = (((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4)
    let output.s0.y.y.loop_max = ((((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4) - 1)
    let output.s0.y.y.loop_min = 0
    let output.s0.y.yi.loop_extent = 4
    let output.s0.y.yi.loop_max = (4 - 1)
    let output.s0.y.yi.loop_min = 0
    for<Hexagon> (output.s0.__outermost, output.s0.__outermost.loop_min, output.s0.__outermost.loop_extent) {
      for (output.s0.y.y, output.s0.y.y.loop_min, output.s0.y.y.loop_extent) {
        prefetch input()
        let output.s0.y.yi.base = ((output.s0.y.y*4) + output.s0.y.loop_min)
        for (output.s0.x.x, output.s0.x.x.loop_min, output.s0.x.x.loop_extent) {
          let output.s0.x.xi.base = ((output.s0.x.x*64) + output.s0.x.loop_min)
          unrolled (output.s0.y.yi, output.s0.y.yi.loop_min, output.s0.y.yi.loop_extent) {
            let output.s0.y = (output.s0.y.yi.base + output.s0.y.yi)
            vectorized (output.s0.x.xi, output.s0.x.xi.loop_min, output.s0.x.xi.loop_extent) {
              let output.s0.x = (output.s0.x.xi.base + output.s0.x.xi)
              output((output.s0.x.xi.base + output.s0.x.xi), (output.s0.y.yi.base + output.s0.y.yi)) = (let t0 = (output.s0.x.xi.base + output.s0.x.xi) in (let t1 = (output.s0.y.yi.base + output.s0.y.yi) in uint8(max(min(shift_right(((((((rows((t0 - 3), t1) + (rows((t0 - 2), t1)*6)) + (rows((t0 - 1), t1)*15)) + (rows(t0, t1)*20)) + (rows((t0 + 1), t1)*15)) + (rows((t0 + 2), t1)*6)) + rows((t0 + 3), t1)), (uint32)12), 255), 0))))
            }
          }
        }
      }
    }
  }
}

Injecting realization of { rows }
for (.__root, 0, 1) {
  produce output {
    let output.s0.y.loop_max = output.s0.y.max
    let output.s0.y.loop_min = output.s0.y.min
    let output.s0.y.loop_extent = ((output.s0.y.max + 1) - output.s0.y.min)
    let output.s0.x.loop_max = output.s0.x.max
    let output.s0.x.loop_min = output.s0.x.min
    let output.s0.x.loop_extent = ((output.s0.x.max + 1) - output.s0.x.min)
    let output.s0.__outermost.loop_extent = 1
    let output.s0.__outermost.loop_max = 0
    let output.s0.__outermost.loop_min = 0
    let output.s0.x.x.loop_extent = (((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64)
    let output.s0.x.x.loop_max = ((((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64) - 1)
    let output.s0.x.x.loop_min = 0
    let output.s0.x.xi.loop_extent = 64
    let output.s0.x.xi.loop_max = (64 - 1)
    let output.s0.x.xi.loop_min = 0
    let output.s0.y.y.loop_extent = (((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4)
    let output.s0.y.y.loop_max = ((((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4) - 1)
    let output.s0.y.y.loop_min = 0
    let output.s0.y.yi.loop_extent = 4
    let output.s0.y.yi.loop_max = (4 - 1)
    let output.s0.y.yi.loop_min = 0
    for<Hexagon> (output.s0.__outermost, output.s0.__outermost.loop_min, output.s0.__outermost.loop_extent) {
      for (output.s0.y.y, output.s0.y.y.loop_min, output.s0.y.y.loop_extent) {
        prefetch input()
        let output.s0.y.yi.base = ((output.s0.y.y*4) + output.s0.y.loop_min)
        realize rows([rows.x.min_realized, rows.x.extent_realized], [rows.y.min_realized, rows.y.extent_realized]) {
          produce rows {
            let rows.s0.y.loop_max = rows.s0.y.max
            let rows.s0.y.loop_min = rows.s0.y.min
            let rows.s0.y.loop_extent = ((rows.s0.y.max + 1) - rows.s0.y.min)
            let rows.s0.x.loop_max = rows.s0.x.max
            let rows.s0.x.loop_min = rows.s0.x.min
            let rows.s0.x.loop_extent = ((rows.s0.x.max + 1) - rows.s0.x.min)
            let rows.s0.__outermost.loop_extent = 1
            let rows.s0.__outermost.loop_max = 0
            let rows.s0.__outermost.loop_min = 0
            let rows.s0.x.x.loop_extent = (((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64)
            let rows.s0.x.x.loop_max = ((((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64) - 1)
            let rows.s0.x.x.loop_min = 0
            let rows.s0.x.xi.loop_extent = 64
            let rows.s0.x.xi.loop_max = (64 - 1)
            let rows.s0.x.xi.loop_min = 0
            let rows.s0.y.y.loop_extent = (((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4)
            let rows.s0.y.y.loop_max = ((((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4) - 1)
            let rows.s0.y.y.loop_min = 0
            let rows.s0.y.yi.loop_extent = 4
            let rows.s0.y.yi.loop_max = (4 - 1)
            let rows.s0.y.yi.loop_min = 0
            for (rows.s0.__outermost, rows.s0.__outermost.loop_min, rows.s0.__outermost.loop_extent) {
              for (rows.s0.y.y, rows.s0.y.y.loop_min, rows.s0.y.y.loop_extent) {
                let rows.s0.y.yi.base = ((rows.s0.y.y*4) + rows.s0.y.loop_min)
                for (rows.s0.x.x, rows.s0.x.x.loop_min, rows.s0.x.x.loop_extent) {
                  let rows.s0.x.xi.base = ((rows.s0.x.x*64) + rows.s0.x.loop_min)
                  unrolled (rows.s0.y.yi, rows.s0.y.yi.loop_min, rows.s0.y.yi.loop_extent) {
                    let rows.s0.y = (rows.s0.y.yi.base + rows.s0.y.yi)
                    vectorized (rows.s0.x.xi, rows.s0.x.xi.loop_min, rows.s0.x.xi.loop_extent) {
                      let rows.s0.x = (rows.s0.x.xi.base + rows.s0.x.xi)
                      rows((rows.s0.x.xi.base + rows.s0.x.xi), (rows.s0.y.yi.base + rows.s0.y.yi)) = ((((((input_32((rows.s0.x.xi.base + rows.s0.x.xi), ((rows.s0.y.yi.base + rows.s0.y.yi) - 3)) + (input_32((rows.s0.x.xi.base + rows.s0.x.xi), ((rows.s0.y.yi.base + rows.s0.y.yi) - 2))*6)) + (input_32((rows.s0.x.xi.base + rows.s0.x.xi), ((rows.s0.y.yi.base + rows.s0.y.yi) - 1))*15)) + (input_32((rows.s0.x.xi.base + rows.s0.x.xi), (rows.s0.y.yi.base + rows.s0.y.yi))*20)) + (input_32((rows.s0.x.xi.base + rows.s0.x.xi), ((rows.s0.y.yi.base + rows.s0.y.yi) + 1))*15)) + (input_32((rows.s0.x.xi.base + rows.s0.x.xi), ((rows.s0.y.yi.base + rows.s0.y.yi) + 2))*6)) + input_32((rows.s0.x.xi.base + rows.s0.x.xi), ((rows.s0.y.yi.base + rows.s0.y.yi) + 3)))
                    }
                  }
                }
              }
            }
          }
          consume rows {
            for (output.s0.x.x, output.s0.x.x.loop_min, output.s0.x.x.loop_extent) {
              let output.s0.x.xi.base = ((output.s0.x.x*64) + output.s0.x.loop_min)
              unrolled (output.s0.y.yi, output.s0.y.yi.loop_min, output.s0.y.yi.loop_extent) {
                let output.s0.y = (output.s0.y.yi.base + output.s0.y.yi)
                vectorized (output.s0.x.xi, output.s0.x.xi.loop_min, output.s0.x.xi.loop_extent) {
                  let output.s0.x = (output.s0.x.xi.base + output.s0.x.xi)
                  output((output.s0.x.xi.base + output.s0.x.xi), (output.s0.y.yi.base + output.s0.y.yi)) = (let t0 = (output.s0.x.xi.base + output.s0.x.xi) in (let t1 = (output.s0.y.yi.base + output.s0.y.yi) in uint8(max(min(shift_right(((((((rows((t0 - 3), t1) + (rows((t0 - 2), t1)*6)) + (rows((t0 - 1), t1)*15)) + (rows(t0, t1)*20)) + (rows((t0 + 1), t1)*15)) + (rows((t0 + 2), t1)*6)) + rows((t0 + 3), t1)), (uint32)12), 255), 0))))
                }
              }
            }
          }
        }
      }
    }
  }
}

Inlining input_32
for (.__root, 0, 1) {
  produce output {
    let output.s0.y.loop_max = output.s0.y.max
    let output.s0.y.loop_min = output.s0.y.min
    let output.s0.y.loop_extent = ((output.s0.y.max + 1) - output.s0.y.min)
    let output.s0.x.loop_max = output.s0.x.max
    let output.s0.x.loop_min = output.s0.x.min
    let output.s0.x.loop_extent = ((output.s0.x.max + 1) - output.s0.x.min)
    let output.s0.__outermost.loop_extent = 1
    let output.s0.__outermost.loop_max = 0
    let output.s0.__outermost.loop_min = 0
    let output.s0.x.x.loop_extent = (((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64)
    let output.s0.x.x.loop_max = ((((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64) - 1)
    let output.s0.x.x.loop_min = 0
    let output.s0.x.xi.loop_extent = 64
    let output.s0.x.xi.loop_max = (64 - 1)
    let output.s0.x.xi.loop_min = 0
    let output.s0.y.y.loop_extent = (((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4)
    let output.s0.y.y.loop_max = ((((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4) - 1)
    let output.s0.y.y.loop_min = 0
    let output.s0.y.yi.loop_extent = 4
    let output.s0.y.yi.loop_max = (4 - 1)
    let output.s0.y.yi.loop_min = 0
    for<Hexagon> (output.s0.__outermost, output.s0.__outermost.loop_min, output.s0.__outermost.loop_extent) {
      for (output.s0.y.y, output.s0.y.y.loop_min, output.s0.y.y.loop_extent) {
        prefetch input()
        let output.s0.y.yi.base = ((output.s0.y.y*4) + output.s0.y.loop_min)
        realize rows([rows.x.min_realized, rows.x.extent_realized], [rows.y.min_realized, rows.y.extent_realized]) {
          produce rows {
            let rows.s0.y.loop_max = rows.s0.y.max
            let rows.s0.y.loop_min = rows.s0.y.min
            let rows.s0.y.loop_extent = ((rows.s0.y.max + 1) - rows.s0.y.min)
            let rows.s0.x.loop_max = rows.s0.x.max
            let rows.s0.x.loop_min = rows.s0.x.min
            let rows.s0.x.loop_extent = ((rows.s0.x.max + 1) - rows.s0.x.min)
            let rows.s0.__outermost.loop_extent = 1
            let rows.s0.__outermost.loop_max = 0
            let rows.s0.__outermost.loop_min = 0
            let rows.s0.x.x.loop_extent = (((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64)
            let rows.s0.x.x.loop_max = ((((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64) - 1)
            let rows.s0.x.x.loop_min = 0
            let rows.s0.x.xi.loop_extent = 64
            let rows.s0.x.xi.loop_max = (64 - 1)
            let rows.s0.x.xi.loop_min = 0
            let rows.s0.y.y.loop_extent = (((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4)
            let rows.s0.y.y.loop_max = ((((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4) - 1)
            let rows.s0.y.y.loop_min = 0
            let rows.s0.y.yi.loop_extent = 4
            let rows.s0.y.yi.loop_max = (4 - 1)
            let rows.s0.y.yi.loop_min = 0
            for (rows.s0.__outermost, rows.s0.__outermost.loop_min, rows.s0.__outermost.loop_extent) {
              for (rows.s0.y.y, rows.s0.y.y.loop_min, rows.s0.y.y.loop_extent) {
                let rows.s0.y.yi.base = ((rows.s0.y.y*4) + rows.s0.y.loop_min)
                for (rows.s0.x.x, rows.s0.x.x.loop_min, rows.s0.x.x.loop_extent) {
                  let rows.s0.x.xi.base = ((rows.s0.x.x*64) + rows.s0.x.loop_min)
                  unrolled (rows.s0.y.yi, rows.s0.y.yi.loop_min, rows.s0.y.yi.loop_extent) {
                    let rows.s0.y = (rows.s0.y.yi.base + rows.s0.y.yi)
                    vectorized (rows.s0.x.xi, rows.s0.x.xi.loop_min, rows.s0.x.xi.loop_extent) {
                      let rows.s0.x = (rows.s0.x.xi.base + rows.s0.x.xi)
                      rows((rows.s0.x.xi.base + rows.s0.x.xi), (rows.s0.y.yi.base + rows.s0.y.yi)) = (let t2 = (rows.s0.x.xi.base + rows.s0.x.xi) in (let t3 = (rows.s0.y.yi.base + rows.s0.y.yi) in ((((((int32(input_im(t2, (t3 - 3))) + (int32(input_im(t2, (t3 - 2)))*6)) + (int32(input_im(t2, (t3 - 1)))*15)) + (int32(input_im(t2, t3))*20)) + (int32(input_im(t2, (t3 + 1)))*15)) + (int32(input_im(t2, (t3 + 2)))*6)) + int32(input_im(t2, (t3 + 3))))))
                    }
                  }
                }
              }
            }
          }
          consume rows {
            for (output.s0.x.x, output.s0.x.x.loop_min, output.s0.x.x.loop_extent) {
              let output.s0.x.xi.base = ((output.s0.x.x*64) + output.s0.x.loop_min)
              unrolled (output.s0.y.yi, output.s0.y.yi.loop_min, output.s0.y.yi.loop_extent) {
                let output.s0.y = (output.s0.y.yi.base + output.s0.y.yi)
                vectorized (output.s0.x.xi, output.s0.x.xi.loop_min, output.s0.x.xi.loop_extent) {
                  let output.s0.x = (output.s0.x.xi.base + output.s0.x.xi)
                  output((output.s0.x.xi.base + output.s0.x.xi), (output.s0.y.yi.base + output.s0.y.yi)) = (let t0 = (output.s0.x.xi.base + output.s0.x.xi) in (let t1 = (output.s0.y.yi.base + output.s0.y.yi) in uint8(max(min(shift_right(((((((rows((t0 - 3), t1) + (rows((t0 - 2), t1)*6)) + (rows((t0 - 1), t1)*15)) + (rows(t0, t1)*20)) + (rows((t0 + 1), t1)*15)) + (rows((t0 + 2), t1)*6)) + rows((t0 + 3), t1)), (uint32)12), 255), 0))))
                }
              }
            }
          }
        }
      }
    }
  }
}

Inlining input_im
for (.__root, 0, 1) {
  produce output {
    let output.s0.y.loop_max = output.s0.y.max
    let output.s0.y.loop_min = output.s0.y.min
    let output.s0.y.loop_extent = ((output.s0.y.max + 1) - output.s0.y.min)
    let output.s0.x.loop_max = output.s0.x.max
    let output.s0.x.loop_min = output.s0.x.min
    let output.s0.x.loop_extent = ((output.s0.x.max + 1) - output.s0.x.min)
    let output.s0.__outermost.loop_extent = 1
    let output.s0.__outermost.loop_max = 0
    let output.s0.__outermost.loop_min = 0
    let output.s0.x.x.loop_extent = (((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64)
    let output.s0.x.x.loop_max = ((((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64) - 1)
    let output.s0.x.x.loop_min = 0
    let output.s0.x.xi.loop_extent = 64
    let output.s0.x.xi.loop_max = (64 - 1)
    let output.s0.x.xi.loop_min = 0
    let output.s0.y.y.loop_extent = (((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4)
    let output.s0.y.y.loop_max = ((((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4) - 1)
    let output.s0.y.y.loop_min = 0
    let output.s0.y.yi.loop_extent = 4
    let output.s0.y.yi.loop_max = (4 - 1)
    let output.s0.y.yi.loop_min = 0
    for<Hexagon> (output.s0.__outermost, output.s0.__outermost.loop_min, output.s0.__outermost.loop_extent) {
      for (output.s0.y.y, output.s0.y.y.loop_min, output.s0.y.y.loop_extent) {
        prefetch input()
        let output.s0.y.yi.base = ((output.s0.y.y*4) + output.s0.y.loop_min)
        realize rows([rows.x.min_realized, rows.x.extent_realized], [rows.y.min_realized, rows.y.extent_realized]) {
          produce rows {
            let rows.s0.y.loop_max = rows.s0.y.max
            let rows.s0.y.loop_min = rows.s0.y.min
            let rows.s0.y.loop_extent = ((rows.s0.y.max + 1) - rows.s0.y.min)
            let rows.s0.x.loop_max = rows.s0.x.max
            let rows.s0.x.loop_min = rows.s0.x.min
            let rows.s0.x.loop_extent = ((rows.s0.x.max + 1) - rows.s0.x.min)
            let rows.s0.__outermost.loop_extent = 1
            let rows.s0.__outermost.loop_max = 0
            let rows.s0.__outermost.loop_min = 0
            let rows.s0.x.x.loop_extent = (((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64)
            let rows.s0.x.x.loop_max = ((((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64) - 1)
            let rows.s0.x.x.loop_min = 0
            let rows.s0.x.xi.loop_extent = 64
            let rows.s0.x.xi.loop_max = (64 - 1)
            let rows.s0.x.xi.loop_min = 0
            let rows.s0.y.y.loop_extent = (((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4)
            let rows.s0.y.y.loop_max = ((((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4) - 1)
            let rows.s0.y.y.loop_min = 0
            let rows.s0.y.yi.loop_extent = 4
            let rows.s0.y.yi.loop_max = (4 - 1)
            let rows.s0.y.yi.loop_min = 0
            for (rows.s0.__outermost, rows.s0.__outermost.loop_min, rows.s0.__outermost.loop_extent) {
              for (rows.s0.y.y, rows.s0.y.y.loop_min, rows.s0.y.y.loop_extent) {
                let rows.s0.y.yi.base = ((rows.s0.y.y*4) + rows.s0.y.loop_min)
                for (rows.s0.x.x, rows.s0.x.x.loop_min, rows.s0.x.x.loop_extent) {
                  let rows.s0.x.xi.base = ((rows.s0.x.x*64) + rows.s0.x.loop_min)
                  unrolled (rows.s0.y.yi, rows.s0.y.yi.loop_min, rows.s0.y.yi.loop_extent) {
                    let rows.s0.y = (rows.s0.y.yi.base + rows.s0.y.yi)
                    vectorized (rows.s0.x.xi, rows.s0.x.xi.loop_min, rows.s0.x.xi.loop_extent) {
                      let rows.s0.x = (rows.s0.x.xi.base + rows.s0.x.xi)
                      rows((rows.s0.x.xi.base + rows.s0.x.xi), (rows.s0.y.yi.base + rows.s0.y.yi)) = (let t4 = (rows.s0.x.xi.base + rows.s0.x.xi) in (let t5 = (rows.s0.y.yi.base + rows.s0.y.yi) in ((((((int32(input(t4, (t5 - 3))) + (int32(input(t4, (t5 - 2)))*6)) + (int32(input(t4, (t5 - 1)))*15)) + (int32(input(t4, t5))*20)) + (int32(input(t4, (t5 + 1)))*15)) + (int32(input(t4, (t5 + 2)))*6)) + int32(input(t4, (t5 + 3))))))
                    }
                  }
                }
              }
            }
          }
          consume rows {
            for (output.s0.x.x, output.s0.x.x.loop_min, output.s0.x.x.loop_extent) {
              let output.s0.x.xi.base = ((output.s0.x.x*64) + output.s0.x.loop_min)
              unrolled (output.s0.y.yi, output.s0.y.yi.loop_min, output.s0.y.yi.loop_extent) {
                let output.s0.y = (output.s0.y.yi.base + output.s0.y.yi)
                vectorized (output.s0.x.xi, output.s0.x.xi.loop_min, output.s0.x.xi.loop_extent) {
                  let output.s0.x = (output.s0.x.xi.base + output.s0.x.xi)
                  output((output.s0.x.xi.base + output.s0.x.xi), (output.s0.y.yi.base + output.s0.y.yi)) = (let t0 = (output.s0.x.xi.base + output.s0.x.xi) in (let t1 = (output.s0.y.yi.base + output.s0.y.yi) in uint8(max(min(shift_right(((((((rows((t0 - 3), t1) + (rows((t0 - 2), t1)*6)) + (rows((t0 - 1), t1)*15)) + (rows(t0, t1)*20)) + (rows((t0 + 1), t1)*15)) + (rows((t0 + 2), t1)*6)) + rows((t0 + 3), t1)), (uint32)12), 255), 0))))
                }
              }
            }
          }
        }
      }
    }
  }
}

Lowering after creating initial loop nests:
produce output {
  let output.s0.y.loop_max = output.s0.y.max
  let output.s0.y.loop_min = output.s0.y.min
  let output.s0.y.loop_extent = ((output.s0.y.max + 1) - output.s0.y.min)
  let output.s0.x.loop_max = output.s0.x.max
  let output.s0.x.loop_min = output.s0.x.min
  let output.s0.x.loop_extent = ((output.s0.x.max + 1) - output.s0.x.min)
  let output.s0.x.x.loop_extent = (((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64)
  let output.s0.x.x.loop_max = ((((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64) - 1)
  let output.s0.x.x.loop_min = 0
  let output.s0.x.xi.loop_extent = 64
  let output.s0.x.xi.loop_max = (64 - 1)
  let output.s0.x.xi.loop_min = 0
  let output.s0.y.y.loop_extent = (((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4)
  let output.s0.y.y.loop_max = ((((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4) - 1)
  let output.s0.y.y.loop_min = 0
  let output.s0.y.yi.loop_extent = 4
  let output.s0.y.yi.loop_max = (4 - 1)
  let output.s0.y.yi.loop_min = 0
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, output.s0.y.y.loop_min, output.s0.y.y.loop_extent) {
      prefetch input()
      let output.s0.y.yi.base = ((output.s0.y.y*4) + output.s0.y.loop_min)
      realize rows([rows.x.min_realized, rows.x.extent_realized], [rows.y.min_realized, rows.y.extent_realized]) {
        produce rows {
          let rows.s0.y.loop_max = rows.s0.y.max
          let rows.s0.y.loop_min = rows.s0.y.min
          let rows.s0.y.loop_extent = ((rows.s0.y.max + 1) - rows.s0.y.min)
          let rows.s0.x.loop_max = rows.s0.x.max
          let rows.s0.x.loop_min = rows.s0.x.min
          let rows.s0.x.loop_extent = ((rows.s0.x.max + 1) - rows.s0.x.min)
          let rows.s0.x.x.loop_extent = (((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64)
          let rows.s0.x.x.loop_max = ((((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64) - 1)
          let rows.s0.x.x.loop_min = 0
          let rows.s0.x.xi.loop_extent = 64
          let rows.s0.x.xi.loop_max = (64 - 1)
          let rows.s0.x.xi.loop_min = 0
          let rows.s0.y.y.loop_extent = (((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4)
          let rows.s0.y.y.loop_max = ((((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4) - 1)
          let rows.s0.y.y.loop_min = 0
          let rows.s0.y.yi.loop_extent = 4
          let rows.s0.y.yi.loop_max = (4 - 1)
          let rows.s0.y.yi.loop_min = 0
          for (rows.s0.y.y, rows.s0.y.y.loop_min, rows.s0.y.y.loop_extent) {
            let rows.s0.y.yi.base = ((rows.s0.y.y*4) + rows.s0.y.loop_min)
            for (rows.s0.x.x, rows.s0.x.x.loop_min, rows.s0.x.x.loop_extent) {
              let rows.s0.x.xi.base = ((rows.s0.x.x*64) + rows.s0.x.loop_min)
              unrolled (rows.s0.y.yi, rows.s0.y.yi.loop_min, rows.s0.y.yi.loop_extent) {
                let rows.s0.y = (rows.s0.y.yi.base + rows.s0.y.yi)
                vectorized (rows.s0.x.xi, rows.s0.x.xi.loop_min, rows.s0.x.xi.loop_extent) {
                  let rows.s0.x = (rows.s0.x.xi.base + rows.s0.x.xi)
                  rows((rows.s0.x.xi.base + rows.s0.x.xi), (rows.s0.y.yi.base + rows.s0.y.yi)) = (let t4 = (rows.s0.x.xi.base + rows.s0.x.xi) in (let t5 = (rows.s0.y.yi.base + rows.s0.y.yi) in ((((((int32(input(t4, (t5 - 3))) + (int32(input(t4, (t5 - 2)))*6)) + (int32(input(t4, (t5 - 1)))*15)) + (int32(input(t4, t5))*20)) + (int32(input(t4, (t5 + 1)))*15)) + (int32(input(t4, (t5 + 2)))*6)) + int32(input(t4, (t5 + 3))))))
                }
              }
            }
          }
        }
        consume rows {
          for (output.s0.x.x, output.s0.x.x.loop_min, output.s0.x.x.loop_extent) {
            let output.s0.x.xi.base = ((output.s0.x.x*64) + output.s0.x.loop_min)
            unrolled (output.s0.y.yi, output.s0.y.yi.loop_min, output.s0.y.yi.loop_extent) {
              let output.s0.y = (output.s0.y.yi.base + output.s0.y.yi)
              vectorized (output.s0.x.xi, output.s0.x.xi.loop_min, output.s0.x.xi.loop_extent) {
                let output.s0.x = (output.s0.x.xi.base + output.s0.x.xi)
                output((output.s0.x.xi.base + output.s0.x.xi), (output.s0.y.yi.base + output.s0.y.yi)) = (let t0 = (output.s0.x.xi.base + output.s0.x.xi) in (let t1 = (output.s0.y.yi.base + output.s0.y.yi) in uint8(max(min(shift_right(((((((rows((t0 - 3), t1) + (rows((t0 - 2), t1)*6)) + (rows((t0 - 1), t1)*15)) + (rows(t0, t1)*20)) + (rows((t0 + 1), t1)*15)) + (rows((t0 + 2), t1)*6)) + rows((t0 + 3), t1)), (uint32)12), 255), 0))))
              }
            }
          }
        }
      }
    }
  }
}

Skipping injecting memoization...
Injecting tracing...
Lowering after injecting tracing:
produce output {
  let output.s0.y.loop_max = output.s0.y.max
  let output.s0.y.loop_min = output.s0.y.min
  let output.s0.y.loop_extent = ((output.s0.y.max + 1) - output.s0.y.min)
  let output.s0.x.loop_max = output.s0.x.max
  let output.s0.x.loop_min = output.s0.x.min
  let output.s0.x.loop_extent = ((output.s0.x.max + 1) - output.s0.x.min)
  let output.s0.x.x.loop_extent = (((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64)
  let output.s0.x.x.loop_max = ((((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64) - 1)
  let output.s0.x.x.loop_min = 0
  let output.s0.x.xi.loop_extent = 64
  let output.s0.x.xi.loop_max = (64 - 1)
  let output.s0.x.xi.loop_min = 0
  let output.s0.y.y.loop_extent = (((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4)
  let output.s0.y.y.loop_max = ((((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4) - 1)
  let output.s0.y.y.loop_min = 0
  let output.s0.y.yi.loop_extent = 4
  let output.s0.y.yi.loop_max = (4 - 1)
  let output.s0.y.yi.loop_min = 0
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, output.s0.y.y.loop_min, output.s0.y.y.loop_extent) {
      prefetch input()
      let output.s0.y.yi.base = ((output.s0.y.y*4) + output.s0.y.loop_min)
      realize rows([rows.x.min_realized, rows.x.extent_realized], [rows.y.min_realized, rows.y.extent_realized]) {
        produce rows {
          let rows.s0.y.loop_max = rows.s0.y.max
          let rows.s0.y.loop_min = rows.s0.y.min
          let rows.s0.y.loop_extent = ((rows.s0.y.max + 1) - rows.s0.y.min)
          let rows.s0.x.loop_max = rows.s0.x.max
          let rows.s0.x.loop_min = rows.s0.x.min
          let rows.s0.x.loop_extent = ((rows.s0.x.max + 1) - rows.s0.x.min)
          let rows.s0.x.x.loop_extent = (((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64)
          let rows.s0.x.x.loop_max = ((((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64) - 1)
          let rows.s0.x.x.loop_min = 0
          let rows.s0.x.xi.loop_extent = 64
          let rows.s0.x.xi.loop_max = (64 - 1)
          let rows.s0.x.xi.loop_min = 0
          let rows.s0.y.y.loop_extent = (((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4)
          let rows.s0.y.y.loop_max = ((((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4) - 1)
          let rows.s0.y.y.loop_min = 0
          let rows.s0.y.yi.loop_extent = 4
          let rows.s0.y.yi.loop_max = (4 - 1)
          let rows.s0.y.yi.loop_min = 0
          for (rows.s0.y.y, rows.s0.y.y.loop_min, rows.s0.y.y.loop_extent) {
            let rows.s0.y.yi.base = ((rows.s0.y.y*4) + rows.s0.y.loop_min)
            for (rows.s0.x.x, rows.s0.x.x.loop_min, rows.s0.x.x.loop_extent) {
              let rows.s0.x.xi.base = ((rows.s0.x.x*64) + rows.s0.x.loop_min)
              unrolled (rows.s0.y.yi, rows.s0.y.yi.loop_min, rows.s0.y.yi.loop_extent) {
                let rows.s0.y = (rows.s0.y.yi.base + rows.s0.y.yi)
                vectorized (rows.s0.x.xi, rows.s0.x.xi.loop_min, rows.s0.x.xi.loop_extent) {
                  let rows.s0.x = (rows.s0.x.xi.base + rows.s0.x.xi)
                  rows((rows.s0.x.xi.base + rows.s0.x.xi), (rows.s0.y.yi.base + rows.s0.y.yi)) = (let t4 = (rows.s0.x.xi.base + rows.s0.x.xi) in (let t5 = (rows.s0.y.yi.base + rows.s0.y.yi) in ((((((int32(input(t4, (t5 - 3))) + (int32(input(t4, (t5 - 2)))*6)) + (int32(input(t4, (t5 - 1)))*15)) + (int32(input(t4, t5))*20)) + (int32(input(t4, (t5 + 1)))*15)) + (int32(input(t4, (t5 + 2)))*6)) + int32(input(t4, (t5 + 3))))))
                }
              }
            }
          }
        }
        consume rows {
          for (output.s0.x.x, output.s0.x.x.loop_min, output.s0.x.x.loop_extent) {
            let output.s0.x.xi.base = ((output.s0.x.x*64) + output.s0.x.loop_min)
            unrolled (output.s0.y.yi, output.s0.y.yi.loop_min, output.s0.y.yi.loop_extent) {
              let output.s0.y = (output.s0.y.yi.base + output.s0.y.yi)
              vectorized (output.s0.x.xi, output.s0.x.xi.loop_min, output.s0.x.xi.loop_extent) {
                let output.s0.x = (output.s0.x.xi.base + output.s0.x.xi)
                output((output.s0.x.xi.base + output.s0.x.xi), (output.s0.y.yi.base + output.s0.y.yi)) = (let t0 = (output.s0.x.xi.base + output.s0.x.xi) in (let t1 = (output.s0.y.yi.base + output.s0.y.yi) in uint8(max(min(shift_right(((((((rows((t0 - 3), t1) + (rows((t0 - 2), t1)*6)) + (rows((t0 - 1), t1)*15)) + (rows(t0, t1)*20)) + (rows((t0 + 1), t1)*15)) + (rows((t0 + 2), t1)*6)) + rows((t0 + 3), t1)), (uint32)12), 255), 0))))
              }
            }
          }
        }
      }
    }
  }
}

Adding checks for parameters
Lowering after injecting parameter checks:
produce output {
  let output.s0.y.loop_max = output.s0.y.max
  let output.s0.y.loop_min = output.s0.y.min
  let output.s0.y.loop_extent = ((output.s0.y.max + 1) - output.s0.y.min)
  let output.s0.x.loop_max = output.s0.x.max
  let output.s0.x.loop_min = output.s0.x.min
  let output.s0.x.loop_extent = ((output.s0.x.max + 1) - output.s0.x.min)
  let output.s0.x.x.loop_extent = (((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64)
  let output.s0.x.x.loop_max = ((((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64) - 1)
  let output.s0.x.x.loop_min = 0
  let output.s0.x.xi.loop_extent = 64
  let output.s0.x.xi.loop_max = (64 - 1)
  let output.s0.x.xi.loop_min = 0
  let output.s0.y.y.loop_extent = (((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4)
  let output.s0.y.y.loop_max = ((((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4) - 1)
  let output.s0.y.y.loop_min = 0
  let output.s0.y.yi.loop_extent = 4
  let output.s0.y.yi.loop_max = (4 - 1)
  let output.s0.y.yi.loop_min = 0
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, output.s0.y.y.loop_min, output.s0.y.y.loop_extent) {
      prefetch input()
      let output.s0.y.yi.base = ((output.s0.y.y*4) + output.s0.y.loop_min)
      realize rows([rows.x.min_realized, rows.x.extent_realized], [rows.y.min_realized, rows.y.extent_realized]) {
        produce rows {
          let rows.s0.y.loop_max = rows.s0.y.max
          let rows.s0.y.loop_min = rows.s0.y.min
          let rows.s0.y.loop_extent = ((rows.s0.y.max + 1) - rows.s0.y.min)
          let rows.s0.x.loop_max = rows.s0.x.max
          let rows.s0.x.loop_min = rows.s0.x.min
          let rows.s0.x.loop_extent = ((rows.s0.x.max + 1) - rows.s0.x.min)
          let rows.s0.x.x.loop_extent = (((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64)
          let rows.s0.x.x.loop_max = ((((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64) - 1)
          let rows.s0.x.x.loop_min = 0
          let rows.s0.x.xi.loop_extent = 64
          let rows.s0.x.xi.loop_max = (64 - 1)
          let rows.s0.x.xi.loop_min = 0
          let rows.s0.y.y.loop_extent = (((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4)
          let rows.s0.y.y.loop_max = ((((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4) - 1)
          let rows.s0.y.y.loop_min = 0
          let rows.s0.y.yi.loop_extent = 4
          let rows.s0.y.yi.loop_max = (4 - 1)
          let rows.s0.y.yi.loop_min = 0
          for (rows.s0.y.y, rows.s0.y.y.loop_min, rows.s0.y.y.loop_extent) {
            let rows.s0.y.yi.base = ((rows.s0.y.y*4) + rows.s0.y.loop_min)
            for (rows.s0.x.x, rows.s0.x.x.loop_min, rows.s0.x.x.loop_extent) {
              let rows.s0.x.xi.base = ((rows.s0.x.x*64) + rows.s0.x.loop_min)
              unrolled (rows.s0.y.yi, rows.s0.y.yi.loop_min, rows.s0.y.yi.loop_extent) {
                let rows.s0.y = (rows.s0.y.yi.base + rows.s0.y.yi)
                vectorized (rows.s0.x.xi, rows.s0.x.xi.loop_min, rows.s0.x.xi.loop_extent) {
                  let rows.s0.x = (rows.s0.x.xi.base + rows.s0.x.xi)
                  rows((rows.s0.x.xi.base + rows.s0.x.xi), (rows.s0.y.yi.base + rows.s0.y.yi)) = (let t4 = (rows.s0.x.xi.base + rows.s0.x.xi) in (let t5 = (rows.s0.y.yi.base + rows.s0.y.yi) in ((((((int32(input(t4, (t5 - 3))) + (int32(input(t4, (t5 - 2)))*6)) + (int32(input(t4, (t5 - 1)))*15)) + (int32(input(t4, t5))*20)) + (int32(input(t4, (t5 + 1)))*15)) + (int32(input(t4, (t5 + 2)))*6)) + int32(input(t4, (t5 + 3))))))
                }
              }
            }
          }
        }
        consume rows {
          for (output.s0.x.x, output.s0.x.x.loop_min, output.s0.x.x.loop_extent) {
            let output.s0.x.xi.base = ((output.s0.x.x*64) + output.s0.x.loop_min)
            unrolled (output.s0.y.yi, output.s0.y.yi.loop_min, output.s0.y.yi.loop_extent) {
              let output.s0.y = (output.s0.y.yi.base + output.s0.y.yi)
              vectorized (output.s0.x.xi, output.s0.x.xi.loop_min, output.s0.x.xi.loop_extent) {
                let output.s0.x = (output.s0.x.xi.base + output.s0.x.xi)
                output((output.s0.x.xi.base + output.s0.x.xi), (output.s0.y.yi.base + output.s0.y.yi)) = (let t0 = (output.s0.x.xi.base + output.s0.x.xi) in (let t1 = (output.s0.y.yi.base + output.s0.y.yi) in uint8(max(min(shift_right(((((((rows((t0 - 3), t1) + (rows((t0 - 2), t1)*6)) + (rows((t0 - 1), t1)*15)) + (rows(t0, t1)*20)) + (rows((t0 + 1), t1)*15)) + (rows((t0 + 2), t1)*6)) + rows((t0 + 3), t1)), (uint32)12), 255), 0))))
              }
            }
          }
        }
      }
    }
  }
}

Computing bounds of each function's value
Bounds on value 0 for func input_im are: (uint8)0, (uint8)255
Bounds on value 0 for func input_32 are: 0, 255
Bounds on value 0 for func rows are: 0, 16320
Bounds on value 0 for func cols are: 0, 1044480
Bounds on value 0 for func output are: (uint8)0, (uint8)255
Adding checks for images
Injecting constraints for input.0
Injecting constraints for input.1
Injecting constraints for output.0
Injecting constraints for output.1
Lowering after injecting image checks:
let input.extent.0.required = (((((((rows.s0.x.max - rows.s0.x.min)/64)*64) + rows.s0.x.min) + 63) + 1) - rows.s0.x.min)
let input.min.0.required = rows.s0.x.min
let input.stride.0.required = 1
let input.extent.1.required = (((((((rows.s0.y.max - rows.s0.y.min)/4)*4) + rows.s0.y.min) + 6) + 1) - (rows.s0.y.min + -3))
let input.min.1.required = (rows.s0.y.min + -3)
let input.stride.1.required = (input.stride.0.required*input.extent.0.required)
let output.extent.0.required = (((((((output.s0.x.max - output.s0.x.min)/64)*64) + output.s0.x.min) + 63) + 1) - (output.s0.x.min + 0))
let output.min.0.required = (output.s0.x.min + 0)
let output.stride.0.required = 1
let output.extent.1.required = (((((((output.s0.y.max - output.s0.y.min)/4)*4) + output.s0.y.min) + 3) + 1) - (output.s0.y.min + 0))
let output.min.1.required = (output.s0.y.min + 0)
let output.stride.1.required = (output.stride.0.required*output.extent.0.required)
let input.stride.0.constrained = 1
let input.min.0.constrained = 0
let input.stride.1.constrained = ((input.stride.1/64)*64)
let input.min.1.constrained = 0
let output.stride.0.constrained = 1
let output.min.0.constrained = 0
let output.stride.1.constrained = ((output.stride.1/64)*64)
let output.min.1.constrained = 0
let input.stride.0.proposed = 1
let input.min.0.proposed = 0
let input.extent.0.proposed = input.extent.0.required
let input.stride.1.proposed = ((input.stride.1.required/64)*64)
let input.min.1.proposed = 0
let input.extent.1.proposed = input.extent.1.required
let output.stride.0.proposed = 1
let output.min.0.proposed = 0
let output.extent.0.proposed = output.extent.0.required
let output.stride.1.proposed = ((output.stride.1.required/64)*64)
let output.min.1.proposed = 0
let output.extent.1.proposed = output.extent.1.required
assert((input.stride.0 == input.stride.0.constrained), 0)
assert((input.min.0 == input.min.0.constrained), 0)
assert((input.stride.1 == input.stride.1.constrained), 0)
assert((input.min.1 == input.min.1.constrained), 0)
assert((output.stride.0 == output.stride.0.constrained), 0)
assert((output.min.0 == output.min.0.constrained), 0)
assert((output.stride.1 == output.stride.1.constrained), 0)
assert((output.min.1 == output.min.1.constrained), 0)
produce output {
  let output.s0.y.loop_max = output.s0.y.max
  let output.s0.y.loop_min = output.s0.y.min
  let output.s0.y.loop_extent = ((output.s0.y.max + 1) - output.s0.y.min)
  let output.s0.x.loop_max = output.s0.x.max
  let output.s0.x.loop_min = output.s0.x.min
  let output.s0.x.loop_extent = ((output.s0.x.max + 1) - output.s0.x.min)
  let output.s0.x.x.loop_extent = (((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64)
  let output.s0.x.x.loop_max = ((((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64) - 1)
  let output.s0.x.x.loop_min = 0
  let output.s0.x.xi.loop_extent = 64
  let output.s0.x.xi.loop_max = (64 - 1)
  let output.s0.x.xi.loop_min = 0
  let output.s0.y.y.loop_extent = (((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4)
  let output.s0.y.y.loop_max = ((((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4) - 1)
  let output.s0.y.y.loop_min = 0
  let output.s0.y.yi.loop_extent = 4
  let output.s0.y.yi.loop_max = (4 - 1)
  let output.s0.y.yi.loop_min = 0
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, output.s0.y.y.loop_min, output.s0.y.y.loop_extent) {
      prefetch input()
      let output.s0.y.yi.base = ((output.s0.y.y*4) + output.s0.y.loop_min)
      realize rows([rows.x.min_realized, rows.x.extent_realized], [rows.y.min_realized, rows.y.extent_realized]) {
        produce rows {
          let rows.s0.y.loop_max = rows.s0.y.max
          let rows.s0.y.loop_min = rows.s0.y.min
          let rows.s0.y.loop_extent = ((rows.s0.y.max + 1) - rows.s0.y.min)
          let rows.s0.x.loop_max = rows.s0.x.max
          let rows.s0.x.loop_min = rows.s0.x.min
          let rows.s0.x.loop_extent = ((rows.s0.x.max + 1) - rows.s0.x.min)
          let rows.s0.x.x.loop_extent = (((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64)
          let rows.s0.x.x.loop_max = ((((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64) - 1)
          let rows.s0.x.x.loop_min = 0
          let rows.s0.x.xi.loop_extent = 64
          let rows.s0.x.xi.loop_max = (64 - 1)
          let rows.s0.x.xi.loop_min = 0
          let rows.s0.y.y.loop_extent = (((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4)
          let rows.s0.y.y.loop_max = ((((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4) - 1)
          let rows.s0.y.y.loop_min = 0
          let rows.s0.y.yi.loop_extent = 4
          let rows.s0.y.yi.loop_max = (4 - 1)
          let rows.s0.y.yi.loop_min = 0
          for (rows.s0.y.y, rows.s0.y.y.loop_min, rows.s0.y.y.loop_extent) {
            let rows.s0.y.yi.base = ((rows.s0.y.y*4) + rows.s0.y.loop_min)
            for (rows.s0.x.x, rows.s0.x.x.loop_min, rows.s0.x.x.loop_extent) {
              let rows.s0.x.xi.base = ((rows.s0.x.x*64) + rows.s0.x.loop_min)
              unrolled (rows.s0.y.yi, rows.s0.y.yi.loop_min, rows.s0.y.yi.loop_extent) {
                let rows.s0.y = (rows.s0.y.yi.base + rows.s0.y.yi)
                vectorized (rows.s0.x.xi, rows.s0.x.xi.loop_min, rows.s0.x.xi.loop_extent) {
                  let rows.s0.x = (rows.s0.x.xi.base + rows.s0.x.xi)
                  rows((rows.s0.x.xi.base + rows.s0.x.xi), (rows.s0.y.yi.base + rows.s0.y.yi)) = (let t4 = (rows.s0.x.xi.base + rows.s0.x.xi) in (let t5 = (rows.s0.y.yi.base + rows.s0.y.yi) in ((((((int32(input(t4, (t5 - 3))) + (int32(input(t4, (t5 - 2)))*6)) + (int32(input(t4, (t5 - 1)))*15)) + (int32(input(t4, t5))*20)) + (int32(input(t4, (t5 + 1)))*15)) + (int32(input(t4, (t5 + 2)))*6)) + int32(input(t4, (t5 + 3))))))
                }
              }
            }
          }
        }
        consume rows {
          for (output.s0.x.x, output.s0.x.x.loop_min, output.s0.x.x.loop_extent) {
            let output.s0.x.xi.base = ((output.s0.x.x*64) + output.s0.x.loop_min)
            unrolled (output.s0.y.yi, output.s0.y.yi.loop_min, output.s0.y.yi.loop_extent) {
              let output.s0.y = (output.s0.y.yi.base + output.s0.y.yi)
              vectorized (output.s0.x.xi, output.s0.x.xi.loop_min, output.s0.x.xi.loop_extent) {
                let output.s0.x = (output.s0.x.xi.base + output.s0.x.xi)
                output((output.s0.x.xi.base + output.s0.x.xi), (output.s0.y.yi.base + output.s0.y.yi)) = (let t0 = (output.s0.x.xi.base + output.s0.x.xi) in (let t1 = (output.s0.y.yi.base + output.s0.y.yi) in uint8(max(min(shift_right(((((((rows((t0 - 3), t1) + (rows((t0 - 2), t1)*6)) + (rows((t0 - 1), t1)*15)) + (rows(t0, t1)*20)) + (rows((t0 + 1), t1)*15)) + (rows((t0 + 2), t1)*6)) + rows((t0 + 3), t1)), (uint32)12), 255), 0))))
              }
            }
          }
        }
      }
    }
  }
}

Performing computation bounds inference...
Lowering after computation bounds inference:
let output.s0.y.max = ((0 + output.extent.1) - 1)
let output.s0.y.min = 0
let output.s0.x.max = ((0 + output.extent.0) - 1)
let output.s0.x.min = 0
let rows.s0.y.max = output.s0.y.max
let rows.s0.y.min = output.s0.y.min
let rows.s0.x.max = (output.s0.x.max + 3)
let rows.s0.x.min = (output.s0.x.min + -3)
let input.extent.0.required = (((((((rows.s0.x.max - rows.s0.x.min)/64)*64) + rows.s0.x.min) + 63) + 1) - rows.s0.x.min)
let input.min.0.required = rows.s0.x.min
let input.stride.0.required = 1
let input.extent.1.required = (((((((rows.s0.y.max - rows.s0.y.min)/4)*4) + rows.s0.y.min) + 6) + 1) - (rows.s0.y.min + -3))
let input.min.1.required = (rows.s0.y.min + -3)
let input.stride.1.required = (input.stride.0.required*input.extent.0.required)
let output.extent.0.required = (((((((output.s0.x.max - output.s0.x.min)/64)*64) + output.s0.x.min) + 63) + 1) - (output.s0.x.min + 0))
let output.min.0.required = (output.s0.x.min + 0)
let output.stride.0.required = 1
let output.extent.1.required = (((((((output.s0.y.max - output.s0.y.min)/4)*4) + output.s0.y.min) + 3) + 1) - (output.s0.y.min + 0))
let output.min.1.required = (output.s0.y.min + 0)
let output.stride.1.required = (output.stride.0.required*output.extent.0.required)
let input.stride.0.constrained = 1
let input.min.0.constrained = 0
let input.stride.1.constrained = ((input.stride.1/64)*64)
let input.min.1.constrained = 0
let output.stride.0.constrained = 1
let output.min.0.constrained = 0
let output.stride.1.constrained = ((output.stride.1/64)*64)
let output.min.1.constrained = 0
let input.stride.0.proposed = 1
let input.min.0.proposed = 0
let input.extent.0.proposed = input.extent.0.required
let input.stride.1.proposed = ((input.stride.1.required/64)*64)
let input.min.1.proposed = 0
let input.extent.1.proposed = input.extent.1.required
let output.stride.0.proposed = 1
let output.min.0.proposed = 0
let output.extent.0.proposed = output.extent.0.required
let output.stride.1.proposed = ((output.stride.1.required/64)*64)
let output.min.1.proposed = 0
let output.extent.1.proposed = output.extent.1.required
assert((input.stride.0 == input.stride.0.constrained), 0)
assert((input.min.0 == input.min.0.constrained), 0)
assert((input.stride.1 == input.stride.1.constrained), 0)
assert((input.min.1 == input.min.1.constrained), 0)
assert((output.stride.0 == output.stride.0.constrained), 0)
assert((output.min.0 == output.min.0.constrained), 0)
assert((output.stride.1 == output.stride.1.constrained), 0)
assert((output.min.1 == output.min.1.constrained), 0)
produce output {
  let output.s0.y.loop_max = output.s0.y.max
  let output.s0.y.loop_min = output.s0.y.min
  let output.s0.y.loop_extent = ((output.s0.y.max + 1) - output.s0.y.min)
  let output.s0.x.loop_max = output.s0.x.max
  let output.s0.x.loop_min = output.s0.x.min
  let output.s0.x.loop_extent = ((output.s0.x.max + 1) - output.s0.x.min)
  let output.s0.x.x.loop_extent = (((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64)
  let output.s0.x.x.loop_max = ((((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64) - 1)
  let output.s0.x.x.loop_min = 0
  let output.s0.x.xi.loop_extent = 64
  let output.s0.x.xi.loop_max = (64 - 1)
  let output.s0.x.xi.loop_min = 0
  let output.s0.y.y.loop_extent = (((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4)
  let output.s0.y.y.loop_max = ((((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4) - 1)
  let output.s0.y.y.loop_min = 0
  let output.s0.y.yi.loop_extent = 4
  let output.s0.y.yi.loop_max = (4 - 1)
  let output.s0.y.yi.loop_min = 0
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, output.s0.y.y.loop_min, output.s0.y.y.loop_extent) {
      let output.s0.y.min = (((output.s0.y.y*4) + output.s0.y.loop_min) + output.s0.y.yi.loop_min)
      let output.s0.y.max = (((output.s0.y.y*4) + output.s0.y.loop_min) + ((output.s0.y.yi.loop_extent + output.s0.y.yi.loop_min) - 1))
      let output.s0.x.min = (((output.s0.x.x.loop_min*64) + output.s0.x.loop_min) + output.s0.x.xi.loop_min)
      let output.s0.x.max = (((((output.s0.x.x.loop_extent + output.s0.x.x.loop_min)*64) + output.s0.x.loop_min) + -64) + ((output.s0.x.xi.loop_extent + output.s0.x.xi.loop_min) - 1))
      let rows.s0.y.max = output.s0.y.max
      let rows.s0.y.min = output.s0.y.min
      let rows.s0.x.max = (output.s0.x.max + 3)
      let rows.s0.x.min = (output.s0.x.min + -3)
      prefetch input()
      let output.s0.y.yi.base = ((output.s0.y.y*4) + output.s0.y.loop_min)
      realize rows([rows.x.min_realized, rows.x.extent_realized], [rows.y.min_realized, rows.y.extent_realized]) {
        produce rows {
          let rows.s0.y.loop_max = rows.s0.y.max
          let rows.s0.y.loop_min = rows.s0.y.min
          let rows.s0.y.loop_extent = ((rows.s0.y.max + 1) - rows.s0.y.min)
          let rows.s0.x.loop_max = rows.s0.x.max
          let rows.s0.x.loop_min = rows.s0.x.min
          let rows.s0.x.loop_extent = ((rows.s0.x.max + 1) - rows.s0.x.min)
          let rows.s0.x.x.loop_extent = (((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64)
          let rows.s0.x.x.loop_max = ((((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64) - 1)
          let rows.s0.x.x.loop_min = 0
          let rows.s0.x.xi.loop_extent = 64
          let rows.s0.x.xi.loop_max = (64 - 1)
          let rows.s0.x.xi.loop_min = 0
          let rows.s0.y.y.loop_extent = (((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4)
          let rows.s0.y.y.loop_max = ((((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4) - 1)
          let rows.s0.y.y.loop_min = 0
          let rows.s0.y.yi.loop_extent = 4
          let rows.s0.y.yi.loop_max = (4 - 1)
          let rows.s0.y.yi.loop_min = 0
          for (rows.s0.y.y, rows.s0.y.y.loop_min, rows.s0.y.y.loop_extent) {
            let rows.s0.y.yi.base = ((rows.s0.y.y*4) + rows.s0.y.loop_min)
            for (rows.s0.x.x, rows.s0.x.x.loop_min, rows.s0.x.x.loop_extent) {
              let rows.s0.x.xi.base = ((rows.s0.x.x*64) + rows.s0.x.loop_min)
              unrolled (rows.s0.y.yi, rows.s0.y.yi.loop_min, rows.s0.y.yi.loop_extent) {
                let rows.s0.y = (rows.s0.y.yi.base + rows.s0.y.yi)
                vectorized (rows.s0.x.xi, rows.s0.x.xi.loop_min, rows.s0.x.xi.loop_extent) {
                  let rows.s0.x = (rows.s0.x.xi.base + rows.s0.x.xi)
                  rows((rows.s0.x.xi.base + rows.s0.x.xi), (rows.s0.y.yi.base + rows.s0.y.yi)) = (let t4 = (rows.s0.x.xi.base + rows.s0.x.xi) in (let t5 = (rows.s0.y.yi.base + rows.s0.y.yi) in ((((((int32(input(t4, (t5 - 3))) + (int32(input(t4, (t5 - 2)))*6)) + (int32(input(t4, (t5 - 1)))*15)) + (int32(input(t4, t5))*20)) + (int32(input(t4, (t5 + 1)))*15)) + (int32(input(t4, (t5 + 2)))*6)) + int32(input(t4, (t5 + 3))))))
                }
              }
            }
          }
        }
        consume rows {
          for (output.s0.x.x, output.s0.x.x.loop_min, output.s0.x.x.loop_extent) {
            let output.s0.x.xi.base = ((output.s0.x.x*64) + output.s0.x.loop_min)
            unrolled (output.s0.y.yi, output.s0.y.yi.loop_min, output.s0.y.yi.loop_extent) {
              let output.s0.y = (output.s0.y.yi.base + output.s0.y.yi)
              vectorized (output.s0.x.xi, output.s0.x.xi.loop_min, output.s0.x.xi.loop_extent) {
                let output.s0.x = (output.s0.x.xi.base + output.s0.x.xi)
                output((output.s0.x.xi.base + output.s0.x.xi), (output.s0.y.yi.base + output.s0.y.yi)) = (let t0 = (output.s0.x.xi.base + output.s0.x.xi) in (let t1 = (output.s0.y.yi.base + output.s0.y.yi) in uint8(max(min(shift_right(((((((rows((t0 - 3), t1) + (rows((t0 - 2), t1)*6)) + (rows((t0 - 1), t1)*15)) + (rows(t0, t1)*20)) + (rows((t0 + 1), t1)*15)) + (rows((t0 + 2), t1)*6)) + rows((t0 + 3), t1)), (uint32)12), 255), 0))))
              }
            }
          }
        }
      }
    }
  }
}

Removing extern loops...
Lowering after removing extern loops:
let output.s0.y.max = ((0 + output.extent.1) - 1)
let output.s0.y.min = 0
let output.s0.x.max = ((0 + output.extent.0) - 1)
let output.s0.x.min = 0
let rows.s0.y.max = output.s0.y.max
let rows.s0.y.min = output.s0.y.min
let rows.s0.x.max = (output.s0.x.max + 3)
let rows.s0.x.min = (output.s0.x.min + -3)
let input.extent.0.required = (((((((rows.s0.x.max - rows.s0.x.min)/64)*64) + rows.s0.x.min) + 63) + 1) - rows.s0.x.min)
let input.min.0.required = rows.s0.x.min
let input.stride.0.required = 1
let input.extent.1.required = (((((((rows.s0.y.max - rows.s0.y.min)/4)*4) + rows.s0.y.min) + 6) + 1) - (rows.s0.y.min + -3))
let input.min.1.required = (rows.s0.y.min + -3)
let input.stride.1.required = (input.stride.0.required*input.extent.0.required)
let output.extent.0.required = (((((((output.s0.x.max - output.s0.x.min)/64)*64) + output.s0.x.min) + 63) + 1) - (output.s0.x.min + 0))
let output.min.0.required = (output.s0.x.min + 0)
let output.stride.0.required = 1
let output.extent.1.required = (((((((output.s0.y.max - output.s0.y.min)/4)*4) + output.s0.y.min) + 3) + 1) - (output.s0.y.min + 0))
let output.min.1.required = (output.s0.y.min + 0)
let output.stride.1.required = (output.stride.0.required*output.extent.0.required)
let input.stride.0.constrained = 1
let input.min.0.constrained = 0
let input.stride.1.constrained = ((input.stride.1/64)*64)
let input.min.1.constrained = 0
let output.stride.0.constrained = 1
let output.min.0.constrained = 0
let output.stride.1.constrained = ((output.stride.1/64)*64)
let output.min.1.constrained = 0
let input.stride.0.proposed = 1
let input.min.0.proposed = 0
let input.extent.0.proposed = input.extent.0.required
let input.stride.1.proposed = ((input.stride.1.required/64)*64)
let input.min.1.proposed = 0
let input.extent.1.proposed = input.extent.1.required
let output.stride.0.proposed = 1
let output.min.0.proposed = 0
let output.extent.0.proposed = output.extent.0.required
let output.stride.1.proposed = ((output.stride.1.required/64)*64)
let output.min.1.proposed = 0
let output.extent.1.proposed = output.extent.1.required
assert((input.stride.0 == input.stride.0.constrained), 0)
assert((input.min.0 == input.min.0.constrained), 0)
assert((input.stride.1 == input.stride.1.constrained), 0)
assert((input.min.1 == input.min.1.constrained), 0)
assert((output.stride.0 == output.stride.0.constrained), 0)
assert((output.min.0 == output.min.0.constrained), 0)
assert((output.stride.1 == output.stride.1.constrained), 0)
assert((output.min.1 == output.min.1.constrained), 0)
produce output {
  let output.s0.y.loop_max = output.s0.y.max
  let output.s0.y.loop_min = output.s0.y.min
  let output.s0.y.loop_extent = ((output.s0.y.max + 1) - output.s0.y.min)
  let output.s0.x.loop_max = output.s0.x.max
  let output.s0.x.loop_min = output.s0.x.min
  let output.s0.x.loop_extent = ((output.s0.x.max + 1) - output.s0.x.min)
  let output.s0.x.x.loop_extent = (((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64)
  let output.s0.x.x.loop_max = ((((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64) - 1)
  let output.s0.x.x.loop_min = 0
  let output.s0.x.xi.loop_extent = 64
  let output.s0.x.xi.loop_max = (64 - 1)
  let output.s0.x.xi.loop_min = 0
  let output.s0.y.y.loop_extent = (((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4)
  let output.s0.y.y.loop_max = ((((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4) - 1)
  let output.s0.y.y.loop_min = 0
  let output.s0.y.yi.loop_extent = 4
  let output.s0.y.yi.loop_max = (4 - 1)
  let output.s0.y.yi.loop_min = 0
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, output.s0.y.y.loop_min, output.s0.y.y.loop_extent) {
      let output.s0.y.min = (((output.s0.y.y*4) + output.s0.y.loop_min) + output.s0.y.yi.loop_min)
      let output.s0.y.max = (((output.s0.y.y*4) + output.s0.y.loop_min) + ((output.s0.y.yi.loop_extent + output.s0.y.yi.loop_min) - 1))
      let output.s0.x.min = (((output.s0.x.x.loop_min*64) + output.s0.x.loop_min) + output.s0.x.xi.loop_min)
      let output.s0.x.max = (((((output.s0.x.x.loop_extent + output.s0.x.x.loop_min)*64) + output.s0.x.loop_min) + -64) + ((output.s0.x.xi.loop_extent + output.s0.x.xi.loop_min) - 1))
      let rows.s0.y.max = output.s0.y.max
      let rows.s0.y.min = output.s0.y.min
      let rows.s0.x.max = (output.s0.x.max + 3)
      let rows.s0.x.min = (output.s0.x.min + -3)
      prefetch input()
      let output.s0.y.yi.base = ((output.s0.y.y*4) + output.s0.y.loop_min)
      realize rows([rows.x.min_realized, rows.x.extent_realized], [rows.y.min_realized, rows.y.extent_realized]) {
        produce rows {
          let rows.s0.y.loop_max = rows.s0.y.max
          let rows.s0.y.loop_min = rows.s0.y.min
          let rows.s0.y.loop_extent = ((rows.s0.y.max + 1) - rows.s0.y.min)
          let rows.s0.x.loop_max = rows.s0.x.max
          let rows.s0.x.loop_min = rows.s0.x.min
          let rows.s0.x.loop_extent = ((rows.s0.x.max + 1) - rows.s0.x.min)
          let rows.s0.x.x.loop_extent = (((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64)
          let rows.s0.x.x.loop_max = ((((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64) - 1)
          let rows.s0.x.x.loop_min = 0
          let rows.s0.x.xi.loop_extent = 64
          let rows.s0.x.xi.loop_max = (64 - 1)
          let rows.s0.x.xi.loop_min = 0
          let rows.s0.y.y.loop_extent = (((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4)
          let rows.s0.y.y.loop_max = ((((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4) - 1)
          let rows.s0.y.y.loop_min = 0
          let rows.s0.y.yi.loop_extent = 4
          let rows.s0.y.yi.loop_max = (4 - 1)
          let rows.s0.y.yi.loop_min = 0
          for (rows.s0.y.y, rows.s0.y.y.loop_min, rows.s0.y.y.loop_extent) {
            let rows.s0.y.yi.base = ((rows.s0.y.y*4) + rows.s0.y.loop_min)
            for (rows.s0.x.x, rows.s0.x.x.loop_min, rows.s0.x.x.loop_extent) {
              let rows.s0.x.xi.base = ((rows.s0.x.x*64) + rows.s0.x.loop_min)
              unrolled (rows.s0.y.yi, rows.s0.y.yi.loop_min, rows.s0.y.yi.loop_extent) {
                let rows.s0.y = (rows.s0.y.yi.base + rows.s0.y.yi)
                vectorized (rows.s0.x.xi, rows.s0.x.xi.loop_min, rows.s0.x.xi.loop_extent) {
                  let rows.s0.x = (rows.s0.x.xi.base + rows.s0.x.xi)
                  rows((rows.s0.x.xi.base + rows.s0.x.xi), (rows.s0.y.yi.base + rows.s0.y.yi)) = (let t4 = (rows.s0.x.xi.base + rows.s0.x.xi) in (let t5 = (rows.s0.y.yi.base + rows.s0.y.yi) in ((((((int32(input(t4, (t5 - 3))) + (int32(input(t4, (t5 - 2)))*6)) + (int32(input(t4, (t5 - 1)))*15)) + (int32(input(t4, t5))*20)) + (int32(input(t4, (t5 + 1)))*15)) + (int32(input(t4, (t5 + 2)))*6)) + int32(input(t4, (t5 + 3))))))
                }
              }
            }
          }
        }
        consume rows {
          for (output.s0.x.x, output.s0.x.x.loop_min, output.s0.x.x.loop_extent) {
            let output.s0.x.xi.base = ((output.s0.x.x*64) + output.s0.x.loop_min)
            unrolled (output.s0.y.yi, output.s0.y.yi.loop_min, output.s0.y.yi.loop_extent) {
              let output.s0.y = (output.s0.y.yi.base + output.s0.y.yi)
              vectorized (output.s0.x.xi, output.s0.x.xi.loop_min, output.s0.x.xi.loop_extent) {
                let output.s0.x = (output.s0.x.xi.base + output.s0.x.xi)
                output((output.s0.x.xi.base + output.s0.x.xi), (output.s0.y.yi.base + output.s0.y.yi)) = (let t0 = (output.s0.x.xi.base + output.s0.x.xi) in (let t1 = (output.s0.y.yi.base + output.s0.y.yi) in uint8(max(min(shift_right(((((((rows((t0 - 3), t1) + (rows((t0 - 2), t1)*6)) + (rows((t0 - 1), t1)*15)) + (rows(t0, t1)*20)) + (rows((t0 + 1), t1)*15)) + (rows((t0 + 2), t1)*6)) + rows((t0 + 3), t1)), (uint32)12), 255), 0))))
              }
            }
          }
        }
      }
    }
  }
}

Performing sliding window optimization...
Lowering after sliding window:
let output.s0.y.max = ((0 + output.extent.1) - 1)
let output.s0.y.min = 0
let output.s0.x.max = ((0 + output.extent.0) - 1)
let output.s0.x.min = 0
let rows.s0.y.max = output.s0.y.max
let rows.s0.y.min = output.s0.y.min
let rows.s0.x.max = (output.s0.x.max + 3)
let rows.s0.x.min = (output.s0.x.min + -3)
let input.extent.0.required = (((((((rows.s0.x.max - rows.s0.x.min)/64)*64) + rows.s0.x.min) + 63) + 1) - rows.s0.x.min)
let input.min.0.required = rows.s0.x.min
let input.stride.0.required = 1
let input.extent.1.required = (((((((rows.s0.y.max - rows.s0.y.min)/4)*4) + rows.s0.y.min) + 6) + 1) - (rows.s0.y.min + -3))
let input.min.1.required = (rows.s0.y.min + -3)
let input.stride.1.required = (input.stride.0.required*input.extent.0.required)
let output.extent.0.required = (((((((output.s0.x.max - output.s0.x.min)/64)*64) + output.s0.x.min) + 63) + 1) - (output.s0.x.min + 0))
let output.min.0.required = (output.s0.x.min + 0)
let output.stride.0.required = 1
let output.extent.1.required = (((((((output.s0.y.max - output.s0.y.min)/4)*4) + output.s0.y.min) + 3) + 1) - (output.s0.y.min + 0))
let output.min.1.required = (output.s0.y.min + 0)
let output.stride.1.required = (output.stride.0.required*output.extent.0.required)
let input.stride.0.constrained = 1
let input.min.0.constrained = 0
let input.stride.1.constrained = ((input.stride.1/64)*64)
let input.min.1.constrained = 0
let output.stride.0.constrained = 1
let output.min.0.constrained = 0
let output.stride.1.constrained = ((output.stride.1/64)*64)
let output.min.1.constrained = 0
let input.stride.0.proposed = 1
let input.min.0.proposed = 0
let input.extent.0.proposed = input.extent.0.required
let input.stride.1.proposed = ((input.stride.1.required/64)*64)
let input.min.1.proposed = 0
let input.extent.1.proposed = input.extent.1.required
let output.stride.0.proposed = 1
let output.min.0.proposed = 0
let output.extent.0.proposed = output.extent.0.required
let output.stride.1.proposed = ((output.stride.1.required/64)*64)
let output.min.1.proposed = 0
let output.extent.1.proposed = output.extent.1.required
assert((input.stride.0 == input.stride.0.constrained), 0)
assert((input.min.0 == input.min.0.constrained), 0)
assert((input.stride.1 == input.stride.1.constrained), 0)
assert((input.min.1 == input.min.1.constrained), 0)
assert((output.stride.0 == output.stride.0.constrained), 0)
assert((output.min.0 == output.min.0.constrained), 0)
assert((output.stride.1 == output.stride.1.constrained), 0)
assert((output.min.1 == output.min.1.constrained), 0)
produce output {
  let output.s0.y.loop_max = output.s0.y.max
  let output.s0.y.loop_min = output.s0.y.min
  let output.s0.y.loop_extent = ((output.s0.y.max + 1) - output.s0.y.min)
  let output.s0.x.loop_max = output.s0.x.max
  let output.s0.x.loop_min = output.s0.x.min
  let output.s0.x.loop_extent = ((output.s0.x.max + 1) - output.s0.x.min)
  let output.s0.x.x.loop_extent = (((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64)
  let output.s0.x.x.loop_max = ((((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64) - 1)
  let output.s0.x.x.loop_min = 0
  let output.s0.x.xi.loop_extent = 64
  let output.s0.x.xi.loop_max = (64 - 1)
  let output.s0.x.xi.loop_min = 0
  let output.s0.y.y.loop_extent = (((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4)
  let output.s0.y.y.loop_max = ((((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4) - 1)
  let output.s0.y.y.loop_min = 0
  let output.s0.y.yi.loop_extent = 4
  let output.s0.y.yi.loop_max = (4 - 1)
  let output.s0.y.yi.loop_min = 0
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, output.s0.y.y.loop_min, output.s0.y.y.loop_extent) {
      let output.s0.y.min = (((output.s0.y.y*4) + output.s0.y.loop_min) + output.s0.y.yi.loop_min)
      let output.s0.y.max = (((output.s0.y.y*4) + output.s0.y.loop_min) + ((output.s0.y.yi.loop_extent + output.s0.y.yi.loop_min) - 1))
      let output.s0.x.min = (((output.s0.x.x.loop_min*64) + output.s0.x.loop_min) + output.s0.x.xi.loop_min)
      let output.s0.x.max = (((((output.s0.x.x.loop_extent + output.s0.x.x.loop_min)*64) + output.s0.x.loop_min) + -64) + ((output.s0.x.xi.loop_extent + output.s0.x.xi.loop_min) - 1))
      let rows.s0.y.max = output.s0.y.max
      let rows.s0.y.min = output.s0.y.min
      let rows.s0.x.max = (output.s0.x.max + 3)
      let rows.s0.x.min = (output.s0.x.min + -3)
      prefetch input()
      let output.s0.y.yi.base = ((output.s0.y.y*4) + output.s0.y.loop_min)
      realize rows([rows.x.min_realized, rows.x.extent_realized], [rows.y.min_realized, rows.y.extent_realized]) {
        produce rows {
          let rows.s0.y.loop_max = rows.s0.y.max
          let rows.s0.y.loop_min = rows.s0.y.min
          let rows.s0.y.loop_extent = ((rows.s0.y.max + 1) - rows.s0.y.min)
          let rows.s0.x.loop_max = rows.s0.x.max
          let rows.s0.x.loop_min = rows.s0.x.min
          let rows.s0.x.loop_extent = ((rows.s0.x.max + 1) - rows.s0.x.min)
          let rows.s0.x.x.loop_extent = (((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64)
          let rows.s0.x.x.loop_max = ((((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64) - 1)
          let rows.s0.x.x.loop_min = 0
          let rows.s0.x.xi.loop_extent = 64
          let rows.s0.x.xi.loop_max = (64 - 1)
          let rows.s0.x.xi.loop_min = 0
          let rows.s0.y.y.loop_extent = (((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4)
          let rows.s0.y.y.loop_max = ((((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4) - 1)
          let rows.s0.y.y.loop_min = 0
          let rows.s0.y.yi.loop_extent = 4
          let rows.s0.y.yi.loop_max = (4 - 1)
          let rows.s0.y.yi.loop_min = 0
          for (rows.s0.y.y, rows.s0.y.y.loop_min, rows.s0.y.y.loop_extent) {
            let rows.s0.y.yi.base = ((rows.s0.y.y*4) + rows.s0.y.loop_min)
            for (rows.s0.x.x, rows.s0.x.x.loop_min, rows.s0.x.x.loop_extent) {
              let rows.s0.x.xi.base = ((rows.s0.x.x*64) + rows.s0.x.loop_min)
              unrolled (rows.s0.y.yi, rows.s0.y.yi.loop_min, rows.s0.y.yi.loop_extent) {
                let rows.s0.y = (rows.s0.y.yi.base + rows.s0.y.yi)
                vectorized (rows.s0.x.xi, rows.s0.x.xi.loop_min, rows.s0.x.xi.loop_extent) {
                  let rows.s0.x = (rows.s0.x.xi.base + rows.s0.x.xi)
                  rows((rows.s0.x.xi.base + rows.s0.x.xi), (rows.s0.y.yi.base + rows.s0.y.yi)) = (let t4 = (rows.s0.x.xi.base + rows.s0.x.xi) in (let t5 = (rows.s0.y.yi.base + rows.s0.y.yi) in ((((((int32(input(t4, (t5 - 3))) + (int32(input(t4, (t5 - 2)))*6)) + (int32(input(t4, (t5 - 1)))*15)) + (int32(input(t4, t5))*20)) + (int32(input(t4, (t5 + 1)))*15)) + (int32(input(t4, (t5 + 2)))*6)) + int32(input(t4, (t5 + 3))))))
                }
              }
            }
          }
        }
        consume rows {
          for (output.s0.x.x, output.s0.x.x.loop_min, output.s0.x.x.loop_extent) {
            let output.s0.x.xi.base = ((output.s0.x.x*64) + output.s0.x.loop_min)
            unrolled (output.s0.y.yi, output.s0.y.yi.loop_min, output.s0.y.yi.loop_extent) {
              let output.s0.y = (output.s0.y.yi.base + output.s0.y.yi)
              vectorized (output.s0.x.xi, output.s0.x.xi.loop_min, output.s0.x.xi.loop_extent) {
                let output.s0.x = (output.s0.x.xi.base + output.s0.x.xi)
                output((output.s0.x.xi.base + output.s0.x.xi), (output.s0.y.yi.base + output.s0.y.yi)) = (let t0 = (output.s0.x.xi.base + output.s0.x.xi) in (let t1 = (output.s0.y.yi.base + output.s0.y.yi) in uint8(max(min(shift_right(((((((rows((t0 - 3), t1) + (rows((t0 - 2), t1)*6)) + (rows((t0 - 1), t1)*15)) + (rows(t0, t1)*20)) + (rows((t0 + 1), t1)*15)) + (rows((t0 + 2), t1)*6)) + rows((t0 + 3), t1)), (uint32)12), 255), 0))))
              }
            }
          }
        }
      }
    }
  }
}

Simplifying correlated differences...
Lowering after simplifying correlated differences:
let output.s0.y.max = ((0 + output.extent.1) - 1)
let output.s0.y.min = 0
let output.s0.x.max = ((0 + output.extent.0) - 1)
let output.s0.x.min = 0
let rows.s0.y.max = output.s0.y.max
let rows.s0.y.min = output.s0.y.min
let rows.s0.x.max = (output.s0.x.max + 3)
let rows.s0.x.min = (output.s0.x.min + -3)
let input.extent.0.required = (((((((rows.s0.x.max - rows.s0.x.min)/64)*64) + rows.s0.x.min) + 63) + 1) - rows.s0.x.min)
let input.min.0.required = rows.s0.x.min
let input.stride.0.required = 1
let input.extent.1.required = (((((((rows.s0.y.max - rows.s0.y.min)/4)*4) + rows.s0.y.min) + 6) + 1) - (rows.s0.y.min + -3))
let input.min.1.required = (rows.s0.y.min + -3)
let input.stride.1.required = (input.stride.0.required*input.extent.0.required)
let output.extent.0.required = (((((((output.s0.x.max - output.s0.x.min)/64)*64) + output.s0.x.min) + 63) + 1) - (output.s0.x.min + 0))
let output.min.0.required = (output.s0.x.min + 0)
let output.stride.0.required = 1
let output.extent.1.required = (((((((output.s0.y.max - output.s0.y.min)/4)*4) + output.s0.y.min) + 3) + 1) - (output.s0.y.min + 0))
let output.min.1.required = (output.s0.y.min + 0)
let output.stride.1.required = (output.stride.0.required*output.extent.0.required)
let input.stride.0.constrained = 1
let input.min.0.constrained = 0
let input.stride.1.constrained = ((input.stride.1/64)*64)
let input.min.1.constrained = 0
let output.stride.0.constrained = 1
let output.min.0.constrained = 0
let output.stride.1.constrained = ((output.stride.1/64)*64)
let output.min.1.constrained = 0
let input.stride.0.proposed = 1
let input.min.0.proposed = 0
let input.extent.0.proposed = input.extent.0.required
let input.stride.1.proposed = ((input.stride.1.required/64)*64)
let input.min.1.proposed = 0
let input.extent.1.proposed = input.extent.1.required
let output.stride.0.proposed = 1
let output.min.0.proposed = 0
let output.extent.0.proposed = output.extent.0.required
let output.stride.1.proposed = ((output.stride.1.required/64)*64)
let output.min.1.proposed = 0
let output.extent.1.proposed = output.extent.1.required
assert((input.stride.0 == input.stride.0.constrained), 0)
assert((input.min.0 == input.min.0.constrained), 0)
assert((input.stride.1 == input.stride.1.constrained), 0)
assert((input.min.1 == input.min.1.constrained), 0)
assert((output.stride.0 == output.stride.0.constrained), 0)
assert((output.min.0 == output.min.0.constrained), 0)
assert((output.stride.1 == output.stride.1.constrained), 0)
assert((output.min.1 == output.min.1.constrained), 0)
produce output {
  let output.s0.y.loop_max = output.s0.y.max
  let output.s0.y.loop_min = output.s0.y.min
  let output.s0.y.loop_extent = ((output.s0.y.max + 1) - output.s0.y.min)
  let output.s0.x.loop_max = output.s0.x.max
  let output.s0.x.loop_min = output.s0.x.min
  let output.s0.x.loop_extent = ((output.s0.x.max + 1) - output.s0.x.min)
  let output.s0.x.x.loop_extent = (((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64)
  let output.s0.x.x.loop_max = ((((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64) - 1)
  let output.s0.x.x.loop_min = 0
  let output.s0.x.xi.loop_extent = 64
  let output.s0.x.xi.loop_max = (64 - 1)
  let output.s0.x.xi.loop_min = 0
  let output.s0.y.y.loop_extent = (((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4)
  let output.s0.y.y.loop_max = ((((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4) - 1)
  let output.s0.y.y.loop_min = 0
  let output.s0.y.yi.loop_extent = 4
  let output.s0.y.yi.loop_max = (4 - 1)
  let output.s0.y.yi.loop_min = 0
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, output.s0.y.y.loop_min, output.s0.y.y.loop_extent) {
      let output.s0.y.min = (((output.s0.y.y*4) + output.s0.y.loop_min) + output.s0.y.yi.loop_min)
      let output.s0.y.max = (((output.s0.y.y*4) + output.s0.y.loop_min) + ((output.s0.y.yi.loop_extent + output.s0.y.yi.loop_min) - 1))
      let output.s0.x.min = (((output.s0.x.x.loop_min*64) + output.s0.x.loop_min) + output.s0.x.xi.loop_min)
      let output.s0.x.max = (((((output.s0.x.x.loop_extent + output.s0.x.x.loop_min)*64) + output.s0.x.loop_min) + -64) + ((output.s0.x.xi.loop_extent + output.s0.x.xi.loop_min) - 1))
      let rows.s0.y.max = output.s0.y.max
      let rows.s0.y.min = output.s0.y.min
      let rows.s0.x.max = (output.s0.x.max + 3)
      let rows.s0.x.min = (output.s0.x.min + -3)
      prefetch input()
      let output.s0.y.yi.base = ((output.s0.y.y*4) + output.s0.y.loop_min)
      realize rows([rows.x.min_realized, rows.x.extent_realized], [rows.y.min_realized, rows.y.extent_realized]) {
        produce rows {
          let rows.s0.y.loop_max = rows.s0.y.max
          let rows.s0.y.loop_min = rows.s0.y.min
          let rows.s0.y.loop_extent = ((rows.s0.y.max + 1) - rows.s0.y.min)
          let rows.s0.x.loop_max = rows.s0.x.max
          let rows.s0.x.loop_min = rows.s0.x.min
          let rows.s0.x.loop_extent = ((rows.s0.x.max + 1) - rows.s0.x.min)
          let rows.s0.x.x.loop_extent = (((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64)
          let rows.s0.x.x.loop_max = ((((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64) - 1)
          let rows.s0.x.x.loop_min = 0
          let rows.s0.x.xi.loop_extent = 64
          let rows.s0.x.xi.loop_max = (64 - 1)
          let rows.s0.x.xi.loop_min = 0
          let rows.s0.y.y.loop_extent = (((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4)
          let rows.s0.y.y.loop_max = ((((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4) - 1)
          let rows.s0.y.y.loop_min = 0
          let rows.s0.y.yi.loop_extent = 4
          let rows.s0.y.yi.loop_max = (4 - 1)
          let rows.s0.y.yi.loop_min = 0
          for (rows.s0.y.y, rows.s0.y.y.loop_min, rows.s0.y.y.loop_extent) {
            let rows.s0.y.yi.base = ((rows.s0.y.y*4) + rows.s0.y.loop_min)
            for (rows.s0.x.x, rows.s0.x.x.loop_min, rows.s0.x.x.loop_extent) {
              let rows.s0.x.xi.base = ((rows.s0.x.x*64) + rows.s0.x.loop_min)
              unrolled (rows.s0.y.yi, rows.s0.y.yi.loop_min, rows.s0.y.yi.loop_extent) {
                let rows.s0.y = (rows.s0.y.yi.base + rows.s0.y.yi)
                vectorized (rows.s0.x.xi, rows.s0.x.xi.loop_min, rows.s0.x.xi.loop_extent) {
                  let rows.s0.x = (rows.s0.x.xi.base + rows.s0.x.xi)
                  rows((rows.s0.x.xi.base + rows.s0.x.xi), (rows.s0.y.yi.base + rows.s0.y.yi)) = (let t4 = (rows.s0.x.xi.base + rows.s0.x.xi) in (let t5 = (rows.s0.y.yi.base + rows.s0.y.yi) in ((((((int32(input(t4, (t5 - 3))) + (int32(input(t4, (t5 - 2)))*6)) + (int32(input(t4, (t5 - 1)))*15)) + (int32(input(t4, t5))*20)) + (int32(input(t4, (t5 + 1)))*15)) + (int32(input(t4, (t5 + 2)))*6)) + int32(input(t4, (t5 + 3))))))
                }
              }
            }
          }
        }
        consume rows {
          for (output.s0.x.x, output.s0.x.x.loop_min, output.s0.x.x.loop_extent) {
            let output.s0.x.xi.base = ((output.s0.x.x*64) + output.s0.x.loop_min)
            unrolled (output.s0.y.yi, output.s0.y.yi.loop_min, output.s0.y.yi.loop_extent) {
              let output.s0.y = (output.s0.y.yi.base + output.s0.y.yi)
              vectorized (output.s0.x.xi, output.s0.x.xi.loop_min, output.s0.x.xi.loop_extent) {
                let output.s0.x = (output.s0.x.xi.base + output.s0.x.xi)
                output((output.s0.x.xi.base + output.s0.x.xi), (output.s0.y.yi.base + output.s0.y.yi)) = (let t0 = (output.s0.x.xi.base + output.s0.x.xi) in (let t1 = (output.s0.y.yi.base + output.s0.y.yi) in uint8(max(min(shift_right(((((((rows((t0 - 3), t1) + (rows((t0 - 2), t1)*6)) + (rows((t0 - 1), t1)*15)) + (rows(t0, t1)*20)) + (rows((t0 + 1), t1)*15)) + (rows((t0 + 2), t1)*6)) + rows((t0 + 3), t1)), (uint32)12), 255), 0))))
              }
            }
          }
        }
      }
    }
  }
}

Performing allocation bounds inference...
Lowering after allocation bounds inference:
let output.s0.y.max = ((0 + output.extent.1) - 1)
let output.s0.y.min = 0
let output.s0.x.max = ((0 + output.extent.0) - 1)
let output.s0.x.min = 0
let rows.s0.y.max = output.s0.y.max
let rows.s0.y.min = output.s0.y.min
let rows.s0.x.max = (output.s0.x.max + 3)
let rows.s0.x.min = (output.s0.x.min + -3)
let input.extent.0.required = (((((((rows.s0.x.max - rows.s0.x.min)/64)*64) + rows.s0.x.min) + 63) + 1) - rows.s0.x.min)
let input.min.0.required = rows.s0.x.min
let input.stride.0.required = 1
let input.extent.1.required = (((((((rows.s0.y.max - rows.s0.y.min)/4)*4) + rows.s0.y.min) + 6) + 1) - (rows.s0.y.min + -3))
let input.min.1.required = (rows.s0.y.min + -3)
let input.stride.1.required = (input.stride.0.required*input.extent.0.required)
let output.extent.0.required = (((((((output.s0.x.max - output.s0.x.min)/64)*64) + output.s0.x.min) + 63) + 1) - (output.s0.x.min + 0))
let output.min.0.required = (output.s0.x.min + 0)
let output.stride.0.required = 1
let output.extent.1.required = (((((((output.s0.y.max - output.s0.y.min)/4)*4) + output.s0.y.min) + 3) + 1) - (output.s0.y.min + 0))
let output.min.1.required = (output.s0.y.min + 0)
let output.stride.1.required = (output.stride.0.required*output.extent.0.required)
let input.stride.0.constrained = 1
let input.min.0.constrained = 0
let input.stride.1.constrained = ((input.stride.1/64)*64)
let input.min.1.constrained = 0
let output.stride.0.constrained = 1
let output.min.0.constrained = 0
let output.stride.1.constrained = ((output.stride.1/64)*64)
let output.min.1.constrained = 0
let input.stride.0.proposed = 1
let input.min.0.proposed = 0
let input.extent.0.proposed = input.extent.0.required
let input.stride.1.proposed = ((input.stride.1.required/64)*64)
let input.min.1.proposed = 0
let input.extent.1.proposed = input.extent.1.required
let output.stride.0.proposed = 1
let output.min.0.proposed = 0
let output.extent.0.proposed = output.extent.0.required
let output.stride.1.proposed = ((output.stride.1.required/64)*64)
let output.min.1.proposed = 0
let output.extent.1.proposed = output.extent.1.required
assert((input.stride.0 == input.stride.0.constrained), 0)
assert((input.min.0 == input.min.0.constrained), 0)
assert((input.stride.1 == input.stride.1.constrained), 0)
assert((input.min.1 == input.min.1.constrained), 0)
assert((output.stride.0 == output.stride.0.constrained), 0)
assert((output.min.0 == output.min.0.constrained), 0)
assert((output.stride.1 == output.stride.1.constrained), 0)
assert((output.min.1 == output.min.1.constrained), 0)
produce output {
  let output.s0.y.loop_max = output.s0.y.max
  let output.s0.y.loop_min = output.s0.y.min
  let output.s0.y.loop_extent = ((output.s0.y.max + 1) - output.s0.y.min)
  let output.s0.x.loop_max = output.s0.x.max
  let output.s0.x.loop_min = output.s0.x.min
  let output.s0.x.loop_extent = ((output.s0.x.max + 1) - output.s0.x.min)
  let output.s0.x.x.loop_extent = (((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64)
  let output.s0.x.x.loop_max = ((((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64) - 1)
  let output.s0.x.x.loop_min = 0
  let output.s0.x.xi.loop_extent = 64
  let output.s0.x.xi.loop_max = (64 - 1)
  let output.s0.x.xi.loop_min = 0
  let output.s0.y.y.loop_extent = (((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4)
  let output.s0.y.y.loop_max = ((((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4) - 1)
  let output.s0.y.y.loop_min = 0
  let output.s0.y.yi.loop_extent = 4
  let output.s0.y.yi.loop_max = (4 - 1)
  let output.s0.y.yi.loop_min = 0
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, output.s0.y.y.loop_min, output.s0.y.y.loop_extent) {
      let output.s0.y.min = (((output.s0.y.y*4) + output.s0.y.loop_min) + output.s0.y.yi.loop_min)
      let output.s0.y.max = (((output.s0.y.y*4) + output.s0.y.loop_min) + ((output.s0.y.yi.loop_extent + output.s0.y.yi.loop_min) - 1))
      let output.s0.x.min = (((output.s0.x.x.loop_min*64) + output.s0.x.loop_min) + output.s0.x.xi.loop_min)
      let output.s0.x.max = (((((output.s0.x.x.loop_extent + output.s0.x.x.loop_min)*64) + output.s0.x.loop_min) + -64) + ((output.s0.x.xi.loop_extent + output.s0.x.xi.loop_min) - 1))
      let rows.s0.y.max = output.s0.y.max
      let rows.s0.y.min = output.s0.y.min
      let rows.s0.x.max = (output.s0.x.max + 3)
      let rows.s0.x.min = (output.s0.x.min + -3)
      prefetch input()
      let output.s0.y.yi.base = ((output.s0.y.y*4) + output.s0.y.loop_min)
      let rows.y.max_realized = (max(((output.s0.y.yi.loop_extent + output.s0.y.yi.loop_min) + output.s0.y.yi.base), (((((rows.s0.y.max - rows.s0.y.min)/4)*4) + rows.s0.y.min) + 4)) + -1)
      let rows.y.min_realized = min((output.s0.y.yi.base + output.s0.y.yi.loop_min), rows.s0.y.min)
      let rows.y.extent_realized = (max(((output.s0.y.yi.loop_extent + output.s0.y.yi.loop_min) + output.s0.y.yi.base), (((((rows.s0.y.max - rows.s0.y.min)/4)*4) + rows.s0.y.min) + 4)) - min((output.s0.y.yi.base + output.s0.y.yi.loop_min), rows.s0.y.min))
      let rows.x.max_realized = (max(((((output.s0.x.x.loop_extent + output.s0.x.x.loop_min)*64) + output.s0.x.loop_min) + (output.s0.x.xi.loop_extent + output.s0.x.xi.loop_min)), (((((rows.s0.x.max - rows.s0.x.min)/64)*64) + rows.s0.x.min) + 125)) + -62)
      let rows.x.min_realized = min(((((output.s0.x.x.loop_min*64) + output.s0.x.loop_min) + output.s0.x.xi.loop_min) + -3), rows.s0.x.min)
      let rows.x.extent_realized = ((max(((((output.s0.x.x.loop_extent + output.s0.x.x.loop_min)*64) + output.s0.x.loop_min) + (output.s0.x.xi.loop_extent + output.s0.x.xi.loop_min)), (((((rows.s0.x.max - rows.s0.x.min)/64)*64) + rows.s0.x.min) + 125)) - min(((((output.s0.x.x.loop_min*64) + output.s0.x.loop_min) + output.s0.x.xi.loop_min) + -3), rows.s0.x.min)) + -61)
      realize rows([rows.x.min_realized, rows.x.extent_realized], [rows.y.min_realized, rows.y.extent_realized]) {
        produce rows {
          let rows.s0.y.loop_max = rows.s0.y.max
          let rows.s0.y.loop_min = rows.s0.y.min
          let rows.s0.y.loop_extent = ((rows.s0.y.max + 1) - rows.s0.y.min)
          let rows.s0.x.loop_max = rows.s0.x.max
          let rows.s0.x.loop_min = rows.s0.x.min
          let rows.s0.x.loop_extent = ((rows.s0.x.max + 1) - rows.s0.x.min)
          let rows.s0.x.x.loop_extent = (((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64)
          let rows.s0.x.x.loop_max = ((((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64) - 1)
          let rows.s0.x.x.loop_min = 0
          let rows.s0.x.xi.loop_extent = 64
          let rows.s0.x.xi.loop_max = (64 - 1)
          let rows.s0.x.xi.loop_min = 0
          let rows.s0.y.y.loop_extent = (((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4)
          let rows.s0.y.y.loop_max = ((((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4) - 1)
          let rows.s0.y.y.loop_min = 0
          let rows.s0.y.yi.loop_extent = 4
          let rows.s0.y.yi.loop_max = (4 - 1)
          let rows.s0.y.yi.loop_min = 0
          for (rows.s0.y.y, rows.s0.y.y.loop_min, rows.s0.y.y.loop_extent) {
            let rows.s0.y.yi.base = ((rows.s0.y.y*4) + rows.s0.y.loop_min)
            for (rows.s0.x.x, rows.s0.x.x.loop_min, rows.s0.x.x.loop_extent) {
              let rows.s0.x.xi.base = ((rows.s0.x.x*64) + rows.s0.x.loop_min)
              unrolled (rows.s0.y.yi, rows.s0.y.yi.loop_min, rows.s0.y.yi.loop_extent) {
                let rows.s0.y = (rows.s0.y.yi.base + rows.s0.y.yi)
                vectorized (rows.s0.x.xi, rows.s0.x.xi.loop_min, rows.s0.x.xi.loop_extent) {
                  let rows.s0.x = (rows.s0.x.xi.base + rows.s0.x.xi)
                  rows((rows.s0.x.xi.base + rows.s0.x.xi), (rows.s0.y.yi.base + rows.s0.y.yi)) = (let t4 = (rows.s0.x.xi.base + rows.s0.x.xi) in (let t5 = (rows.s0.y.yi.base + rows.s0.y.yi) in ((((((int32(input(t4, (t5 - 3))) + (int32(input(t4, (t5 - 2)))*6)) + (int32(input(t4, (t5 - 1)))*15)) + (int32(input(t4, t5))*20)) + (int32(input(t4, (t5 + 1)))*15)) + (int32(input(t4, (t5 + 2)))*6)) + int32(input(t4, (t5 + 3))))))
                }
              }
            }
          }
        }
        consume rows {
          for (output.s0.x.x, output.s0.x.x.loop_min, output.s0.x.x.loop_extent) {
            let output.s0.x.xi.base = ((output.s0.x.x*64) + output.s0.x.loop_min)
            unrolled (output.s0.y.yi, output.s0.y.yi.loop_min, output.s0.y.yi.loop_extent) {
              let output.s0.y = (output.s0.y.yi.base + output.s0.y.yi)
              vectorized (output.s0.x.xi, output.s0.x.xi.loop_min, output.s0.x.xi.loop_extent) {
                let output.s0.x = (output.s0.x.xi.base + output.s0.x.xi)
                output((output.s0.x.xi.base + output.s0.x.xi), (output.s0.y.yi.base + output.s0.y.yi)) = (let t0 = (output.s0.x.xi.base + output.s0.x.xi) in (let t1 = (output.s0.y.yi.base + output.s0.y.yi) in uint8(max(min(shift_right(((((((rows((t0 - 3), t1) + (rows((t0 - 2), t1)*6)) + (rows((t0 - 1), t1)*15)) + (rows(t0, t1)*20)) + (rows((t0 + 1), t1)*15)) + (rows((t0 + 2), t1)*6)) + rows((t0 + 3), t1)), (uint32)12), 255), 0))))
              }
            }
          }
        }
      }
    }
  }
}

Removing code that depends on undef values...
Lowering after removing code that depends on undef values:
let output.s0.y.max = ((0 + output.extent.1) - 1)
let output.s0.y.min = 0
let output.s0.x.max = ((0 + output.extent.0) - 1)
let output.s0.x.min = 0
let rows.s0.y.max = output.s0.y.max
let rows.s0.y.min = output.s0.y.min
let rows.s0.x.max = (output.s0.x.max + 3)
let rows.s0.x.min = (output.s0.x.min + -3)
let input.extent.0.required = (((((((rows.s0.x.max - rows.s0.x.min)/64)*64) + rows.s0.x.min) + 63) + 1) - rows.s0.x.min)
let input.min.0.required = rows.s0.x.min
let input.stride.0.required = 1
let input.extent.1.required = (((((((rows.s0.y.max - rows.s0.y.min)/4)*4) + rows.s0.y.min) + 6) + 1) - (rows.s0.y.min + -3))
let input.min.1.required = (rows.s0.y.min + -3)
let input.stride.1.required = (input.stride.0.required*input.extent.0.required)
let output.extent.0.required = (((((((output.s0.x.max - output.s0.x.min)/64)*64) + output.s0.x.min) + 63) + 1) - (output.s0.x.min + 0))
let output.min.0.required = (output.s0.x.min + 0)
let output.stride.0.required = 1
let output.extent.1.required = (((((((output.s0.y.max - output.s0.y.min)/4)*4) + output.s0.y.min) + 3) + 1) - (output.s0.y.min + 0))
let output.min.1.required = (output.s0.y.min + 0)
let output.stride.1.required = (output.stride.0.required*output.extent.0.required)
let input.stride.0.constrained = 1
let input.min.0.constrained = 0
let input.stride.1.constrained = ((input.stride.1/64)*64)
let input.min.1.constrained = 0
let output.stride.0.constrained = 1
let output.min.0.constrained = 0
let output.stride.1.constrained = ((output.stride.1/64)*64)
let output.min.1.constrained = 0
let input.stride.0.proposed = 1
let input.min.0.proposed = 0
let input.extent.0.proposed = input.extent.0.required
let input.stride.1.proposed = ((input.stride.1.required/64)*64)
let input.min.1.proposed = 0
let input.extent.1.proposed = input.extent.1.required
let output.stride.0.proposed = 1
let output.min.0.proposed = 0
let output.extent.0.proposed = output.extent.0.required
let output.stride.1.proposed = ((output.stride.1.required/64)*64)
let output.min.1.proposed = 0
let output.extent.1.proposed = output.extent.1.required
assert((input.stride.0 == input.stride.0.constrained), 0)
assert((input.min.0 == input.min.0.constrained), 0)
assert((input.stride.1 == input.stride.1.constrained), 0)
assert((input.min.1 == input.min.1.constrained), 0)
assert((output.stride.0 == output.stride.0.constrained), 0)
assert((output.min.0 == output.min.0.constrained), 0)
assert((output.stride.1 == output.stride.1.constrained), 0)
assert((output.min.1 == output.min.1.constrained), 0)
produce output {
  let output.s0.y.loop_max = output.s0.y.max
  let output.s0.y.loop_min = output.s0.y.min
  let output.s0.y.loop_extent = ((output.s0.y.max + 1) - output.s0.y.min)
  let output.s0.x.loop_max = output.s0.x.max
  let output.s0.x.loop_min = output.s0.x.min
  let output.s0.x.loop_extent = ((output.s0.x.max + 1) - output.s0.x.min)
  let output.s0.x.x.loop_extent = (((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64)
  let output.s0.x.x.loop_max = ((((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64) - 1)
  let output.s0.x.x.loop_min = 0
  let output.s0.x.xi.loop_extent = 64
  let output.s0.x.xi.loop_max = (64 - 1)
  let output.s0.x.xi.loop_min = 0
  let output.s0.y.y.loop_extent = (((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4)
  let output.s0.y.y.loop_max = ((((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4) - 1)
  let output.s0.y.y.loop_min = 0
  let output.s0.y.yi.loop_extent = 4
  let output.s0.y.yi.loop_max = (4 - 1)
  let output.s0.y.yi.loop_min = 0
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, output.s0.y.y.loop_min, output.s0.y.y.loop_extent) {
      let output.s0.y.min = (((output.s0.y.y*4) + output.s0.y.loop_min) + output.s0.y.yi.loop_min)
      let output.s0.y.max = (((output.s0.y.y*4) + output.s0.y.loop_min) + ((output.s0.y.yi.loop_extent + output.s0.y.yi.loop_min) - 1))
      let output.s0.x.min = (((output.s0.x.x.loop_min*64) + output.s0.x.loop_min) + output.s0.x.xi.loop_min)
      let output.s0.x.max = (((((output.s0.x.x.loop_extent + output.s0.x.x.loop_min)*64) + output.s0.x.loop_min) + -64) + ((output.s0.x.xi.loop_extent + output.s0.x.xi.loop_min) - 1))
      let rows.s0.y.max = output.s0.y.max
      let rows.s0.y.min = output.s0.y.min
      let rows.s0.x.max = (output.s0.x.max + 3)
      let rows.s0.x.min = (output.s0.x.min + -3)
      prefetch input()
      let output.s0.y.yi.base = ((output.s0.y.y*4) + output.s0.y.loop_min)
      let rows.y.max_realized = (max(((output.s0.y.yi.loop_extent + output.s0.y.yi.loop_min) + output.s0.y.yi.base), (((((rows.s0.y.max - rows.s0.y.min)/4)*4) + rows.s0.y.min) + 4)) + -1)
      let rows.y.min_realized = min((output.s0.y.yi.base + output.s0.y.yi.loop_min), rows.s0.y.min)
      let rows.y.extent_realized = (max(((output.s0.y.yi.loop_extent + output.s0.y.yi.loop_min) + output.s0.y.yi.base), (((((rows.s0.y.max - rows.s0.y.min)/4)*4) + rows.s0.y.min) + 4)) - min((output.s0.y.yi.base + output.s0.y.yi.loop_min), rows.s0.y.min))
      let rows.x.max_realized = (max(((((output.s0.x.x.loop_extent + output.s0.x.x.loop_min)*64) + output.s0.x.loop_min) + (output.s0.x.xi.loop_extent + output.s0.x.xi.loop_min)), (((((rows.s0.x.max - rows.s0.x.min)/64)*64) + rows.s0.x.min) + 125)) + -62)
      let rows.x.min_realized = min(((((output.s0.x.x.loop_min*64) + output.s0.x.loop_min) + output.s0.x.xi.loop_min) + -3), rows.s0.x.min)
      let rows.x.extent_realized = ((max(((((output.s0.x.x.loop_extent + output.s0.x.x.loop_min)*64) + output.s0.x.loop_min) + (output.s0.x.xi.loop_extent + output.s0.x.xi.loop_min)), (((((rows.s0.x.max - rows.s0.x.min)/64)*64) + rows.s0.x.min) + 125)) - min(((((output.s0.x.x.loop_min*64) + output.s0.x.loop_min) + output.s0.x.xi.loop_min) + -3), rows.s0.x.min)) + -61)
      realize rows([rows.x.min_realized, rows.x.extent_realized], [rows.y.min_realized, rows.y.extent_realized]) {
        produce rows {
          let rows.s0.y.loop_max = rows.s0.y.max
          let rows.s0.y.loop_min = rows.s0.y.min
          let rows.s0.y.loop_extent = ((rows.s0.y.max + 1) - rows.s0.y.min)
          let rows.s0.x.loop_max = rows.s0.x.max
          let rows.s0.x.loop_min = rows.s0.x.min
          let rows.s0.x.loop_extent = ((rows.s0.x.max + 1) - rows.s0.x.min)
          let rows.s0.x.x.loop_extent = (((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64)
          let rows.s0.x.x.loop_max = ((((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64) - 1)
          let rows.s0.x.x.loop_min = 0
          let rows.s0.x.xi.loop_extent = 64
          let rows.s0.x.xi.loop_max = (64 - 1)
          let rows.s0.x.xi.loop_min = 0
          let rows.s0.y.y.loop_extent = (((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4)
          let rows.s0.y.y.loop_max = ((((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4) - 1)
          let rows.s0.y.y.loop_min = 0
          let rows.s0.y.yi.loop_extent = 4
          let rows.s0.y.yi.loop_max = (4 - 1)
          let rows.s0.y.yi.loop_min = 0
          for (rows.s0.y.y, rows.s0.y.y.loop_min, rows.s0.y.y.loop_extent) {
            let rows.s0.y.yi.base = ((rows.s0.y.y*4) + rows.s0.y.loop_min)
            for (rows.s0.x.x, rows.s0.x.x.loop_min, rows.s0.x.x.loop_extent) {
              let rows.s0.x.xi.base = ((rows.s0.x.x*64) + rows.s0.x.loop_min)
              unrolled (rows.s0.y.yi, rows.s0.y.yi.loop_min, rows.s0.y.yi.loop_extent) {
                let rows.s0.y = (rows.s0.y.yi.base + rows.s0.y.yi)
                vectorized (rows.s0.x.xi, rows.s0.x.xi.loop_min, rows.s0.x.xi.loop_extent) {
                  let rows.s0.x = (rows.s0.x.xi.base + rows.s0.x.xi)
                  rows((rows.s0.x.xi.base + rows.s0.x.xi), (rows.s0.y.yi.base + rows.s0.y.yi)) = (let t4 = (rows.s0.x.xi.base + rows.s0.x.xi) in (let t5 = (rows.s0.y.yi.base + rows.s0.y.yi) in ((((((int32(input(t4, (t5 - 3))) + (int32(input(t4, (t5 - 2)))*6)) + (int32(input(t4, (t5 - 1)))*15)) + (int32(input(t4, t5))*20)) + (int32(input(t4, (t5 + 1)))*15)) + (int32(input(t4, (t5 + 2)))*6)) + int32(input(t4, (t5 + 3))))))
                }
              }
            }
          }
        }
        consume rows {
          for (output.s0.x.x, output.s0.x.x.loop_min, output.s0.x.x.loop_extent) {
            let output.s0.x.xi.base = ((output.s0.x.x*64) + output.s0.x.loop_min)
            unrolled (output.s0.y.yi, output.s0.y.yi.loop_min, output.s0.y.yi.loop_extent) {
              let output.s0.y = (output.s0.y.yi.base + output.s0.y.yi)
              vectorized (output.s0.x.xi, output.s0.x.xi.loop_min, output.s0.x.xi.loop_extent) {
                let output.s0.x = (output.s0.x.xi.base + output.s0.x.xi)
                output((output.s0.x.xi.base + output.s0.x.xi), (output.s0.y.yi.base + output.s0.y.yi)) = (let t0 = (output.s0.x.xi.base + output.s0.x.xi) in (let t1 = (output.s0.y.yi.base + output.s0.y.yi) in uint8(max(min(shift_right(((((((rows((t0 - 3), t1) + (rows((t0 - 2), t1)*6)) + (rows((t0 - 1), t1)*15)) + (rows(t0, t1)*20)) + (rows((t0 + 1), t1)*15)) + (rows((t0 + 2), t1)*6)) + rows((t0 + 3), t1)), (uint32)12), 255), 0))))
              }
            }
          }
        }
      }
    }
  }
}


Uniquifying variable names...
Lowering after uniquifying variable names:
let output.s0.y.max = ((0 + output.extent.1) - 1)
let output.s0.y.min = 0
let output.s0.x.max = ((0 + output.extent.0) - 1)
let output.s0.x.min = 0
let rows.s0.y.max = output.s0.y.max
let rows.s0.y.min = output.s0.y.min
let rows.s0.x.max = (output.s0.x.max + 3)
let rows.s0.x.min = (output.s0.x.min + -3)
let input.extent.0.required = (((((((rows.s0.x.max - rows.s0.x.min)/64)*64) + rows.s0.x.min) + 63) + 1) - rows.s0.x.min)
let input.min.0.required = rows.s0.x.min
let input.stride.0.required = 1
let input.extent.1.required = (((((((rows.s0.y.max - rows.s0.y.min)/4)*4) + rows.s0.y.min) + 6) + 1) - (rows.s0.y.min + -3))
let input.min.1.required = (rows.s0.y.min + -3)
let input.stride.1.required = (input.stride.0.required*input.extent.0.required)
let output.extent.0.required = (((((((output.s0.x.max - output.s0.x.min)/64)*64) + output.s0.x.min) + 63) + 1) - (output.s0.x.min + 0))
let output.min.0.required = (output.s0.x.min + 0)
let output.stride.0.required = 1
let output.extent.1.required = (((((((output.s0.y.max - output.s0.y.min)/4)*4) + output.s0.y.min) + 3) + 1) - (output.s0.y.min + 0))
let output.min.1.required = (output.s0.y.min + 0)
let output.stride.1.required = (output.stride.0.required*output.extent.0.required)
let input.stride.0.constrained = 1
let input.min.0.constrained = 0
let input.stride.1.constrained = ((input.stride.1/64)*64)
let input.min.1.constrained = 0
let output.stride.0.constrained = 1
let output.min.0.constrained = 0
let output.stride.1.constrained = ((output.stride.1/64)*64)
let output.min.1.constrained = 0
let input.stride.0.proposed = 1
let input.min.0.proposed = 0
let input.extent.0.proposed = input.extent.0.required
let input.stride.1.proposed = ((input.stride.1.required/64)*64)
let input.min.1.proposed = 0
let input.extent.1.proposed = input.extent.1.required
let output.stride.0.proposed = 1
let output.min.0.proposed = 0
let output.extent.0.proposed = output.extent.0.required
let output.stride.1.proposed = ((output.stride.1.required/64)*64)
let output.min.1.proposed = 0
let output.extent.1.proposed = output.extent.1.required
assert((input.stride.0 == input.stride.0.constrained), 0)
assert((input.min.0 == input.min.0.constrained), 0)
assert((input.stride.1 == input.stride.1.constrained), 0)
assert((input.min.1 == input.min.1.constrained), 0)
assert((output.stride.0 == output.stride.0.constrained), 0)
assert((output.min.0 == output.min.0.constrained), 0)
assert((output.stride.1 == output.stride.1.constrained), 0)
assert((output.min.1 == output.min.1.constrained), 0)
produce output {
  let output.s0.y.loop_max = output.s0.y.max
  let output.s0.y.loop_min = output.s0.y.min
  let output.s0.y.loop_extent = ((output.s0.y.max + 1) - output.s0.y.min)
  let output.s0.x.loop_max = output.s0.x.max
  let output.s0.x.loop_min = output.s0.x.min
  let output.s0.x.loop_extent = ((output.s0.x.max + 1) - output.s0.x.min)
  let output.s0.x.x.loop_extent = (((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64)
  let output.s0.x.x.loop_max = ((((output.s0.x.loop_max - output.s0.x.loop_min) + 64)/64) - 1)
  let output.s0.x.x.loop_min = 0
  let output.s0.x.xi.loop_extent = 64
  let output.s0.x.xi.loop_max = (64 - 1)
  let output.s0.x.xi.loop_min = 0
  let output.s0.y.y.loop_extent = (((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4)
  let output.s0.y.y.loop_max = ((((output.s0.y.loop_max - output.s0.y.loop_min) + 4)/4) - 1)
  let output.s0.y.y.loop_min = 0
  let output.s0.y.yi.loop_extent = 4
  let output.s0.y.yi.loop_max = (4 - 1)
  let output.s0.y.yi.loop_min = 0
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, output.s0.y.y.loop_min, output.s0.y.y.loop_extent) {
      let output.s0.y.min_1 = (((output.s0.y.y*4) + output.s0.y.loop_min) + output.s0.y.yi.loop_min)
      let output.s0.y.max_1 = (((output.s0.y.y*4) + output.s0.y.loop_min) + ((output.s0.y.yi.loop_extent + output.s0.y.yi.loop_min) - 1))
      let output.s0.x.min_1 = (((output.s0.x.x.loop_min*64) + output.s0.x.loop_min) + output.s0.x.xi.loop_min)
      let output.s0.x.max_1 = (((((output.s0.x.x.loop_extent + output.s0.x.x.loop_min)*64) + output.s0.x.loop_min) + -64) + ((output.s0.x.xi.loop_extent + output.s0.x.xi.loop_min) - 1))
      let rows.s0.y.max_1 = output.s0.y.max_1
      let rows.s0.y.min_1 = output.s0.y.min_1
      let rows.s0.x.max_1 = (output.s0.x.max_1 + 3)
      let rows.s0.x.min_1 = (output.s0.x.min_1 + -3)
      prefetch input()
      let output.s0.y.yi.base = ((output.s0.y.y*4) + output.s0.y.loop_min)
      let rows.y.max_realized = (max(((output.s0.y.yi.loop_extent + output.s0.y.yi.loop_min) + output.s0.y.yi.base), (((((rows.s0.y.max_1 - rows.s0.y.min_1)/4)*4) + rows.s0.y.min_1) + 4)) + -1)
      let rows.y.min_realized = min((output.s0.y.yi.base + output.s0.y.yi.loop_min), rows.s0.y.min_1)
      let rows.y.extent_realized = (max(((output.s0.y.yi.loop_extent + output.s0.y.yi.loop_min) + output.s0.y.yi.base), (((((rows.s0.y.max_1 - rows.s0.y.min_1)/4)*4) + rows.s0.y.min_1) + 4)) - min((output.s0.y.yi.base + output.s0.y.yi.loop_min), rows.s0.y.min_1))
      let rows.x.max_realized = (max(((((output.s0.x.x.loop_extent + output.s0.x.x.loop_min)*64) + output.s0.x.loop_min) + (output.s0.x.xi.loop_extent + output.s0.x.xi.loop_min)), (((((rows.s0.x.max_1 - rows.s0.x.min_1)/64)*64) + rows.s0.x.min_1) + 125)) + -62)
      let rows.x.min_realized = min(((((output.s0.x.x.loop_min*64) + output.s0.x.loop_min) + output.s0.x.xi.loop_min) + -3), rows.s0.x.min_1)
      let rows.x.extent_realized = ((max(((((output.s0.x.x.loop_extent + output.s0.x.x.loop_min)*64) + output.s0.x.loop_min) + (output.s0.x.xi.loop_extent + output.s0.x.xi.loop_min)), (((((rows.s0.x.max_1 - rows.s0.x.min_1)/64)*64) + rows.s0.x.min_1) + 125)) - min(((((output.s0.x.x.loop_min*64) + output.s0.x.loop_min) + output.s0.x.xi.loop_min) + -3), rows.s0.x.min_1)) + -61)
      realize rows([rows.x.min_realized, rows.x.extent_realized], [rows.y.min_realized, rows.y.extent_realized]) {
        produce rows {
          let rows.s0.y.loop_max = rows.s0.y.max_1
          let rows.s0.y.loop_min = rows.s0.y.min_1
          let rows.s0.y.loop_extent = ((rows.s0.y.max_1 + 1) - rows.s0.y.min_1)
          let rows.s0.x.loop_max = rows.s0.x.max_1
          let rows.s0.x.loop_min = rows.s0.x.min_1
          let rows.s0.x.loop_extent = ((rows.s0.x.max_1 + 1) - rows.s0.x.min_1)
          let rows.s0.x.x.loop_extent = (((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64)
          let rows.s0.x.x.loop_max = ((((rows.s0.x.loop_max - rows.s0.x.loop_min) + 64)/64) - 1)
          let rows.s0.x.x.loop_min = 0
          let rows.s0.x.xi.loop_extent = 64
          let rows.s0.x.xi.loop_max = (64 - 1)
          let rows.s0.x.xi.loop_min = 0
          let rows.s0.y.y.loop_extent = (((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4)
          let rows.s0.y.y.loop_max = ((((rows.s0.y.loop_max - rows.s0.y.loop_min) + 4)/4) - 1)
          let rows.s0.y.y.loop_min = 0
          let rows.s0.y.yi.loop_extent = 4
          let rows.s0.y.yi.loop_max = (4 - 1)
          let rows.s0.y.yi.loop_min = 0
          for (rows.s0.y.y, rows.s0.y.y.loop_min, rows.s0.y.y.loop_extent) {
            let rows.s0.y.yi.base = ((rows.s0.y.y*4) + rows.s0.y.loop_min)
            for (rows.s0.x.x, rows.s0.x.x.loop_min, rows.s0.x.x.loop_extent) {
              let rows.s0.x.xi.base = ((rows.s0.x.x*64) + rows.s0.x.loop_min)
              unrolled (rows.s0.y.yi, rows.s0.y.yi.loop_min, rows.s0.y.yi.loop_extent) {
                let rows.s0.y = (rows.s0.y.yi.base + rows.s0.y.yi)
                vectorized (rows.s0.x.xi, rows.s0.x.xi.loop_min, rows.s0.x.xi.loop_extent) {
                  let rows.s0.x = (rows.s0.x.xi.base + rows.s0.x.xi)
                  rows((rows.s0.x.xi.base + rows.s0.x.xi), (rows.s0.y.yi.base + rows.s0.y.yi)) = (let t4 = (rows.s0.x.xi.base + rows.s0.x.xi) in (let t5 = (rows.s0.y.yi.base + rows.s0.y.yi) in ((((((int32(input(t4, (t5 - 3))) + (int32(input(t4, (t5 - 2)))*6)) + (int32(input(t4, (t5 - 1)))*15)) + (int32(input(t4, t5))*20)) + (int32(input(t4, (t5 + 1)))*15)) + (int32(input(t4, (t5 + 2)))*6)) + int32(input(t4, (t5 + 3))))))
                }
              }
            }
          }
        }
        consume rows {
          for (output.s0.x.x, output.s0.x.x.loop_min, output.s0.x.x.loop_extent) {
            let output.s0.x.xi.base = ((output.s0.x.x*64) + output.s0.x.loop_min)
            unrolled (output.s0.y.yi, output.s0.y.yi.loop_min, output.s0.y.yi.loop_extent) {
              let output.s0.y = (output.s0.y.yi.base + output.s0.y.yi)
              vectorized (output.s0.x.xi, output.s0.x.xi.loop_min, output.s0.x.xi.loop_extent) {
                let output.s0.x = (output.s0.x.xi.base + output.s0.x.xi)
                output((output.s0.x.xi.base + output.s0.x.xi), (output.s0.y.yi.base + output.s0.y.yi)) = (let t0 = (output.s0.x.xi.base + output.s0.x.xi) in (let t1 = (output.s0.y.yi.base + output.s0.y.yi) in uint8(max(min(shift_right(((((((rows((t0 - 3), t1) + (rows((t0 - 2), t1)*6)) + (rows((t0 - 1), t1)*15)) + (rows(t0, t1)*20)) + (rows((t0 + 1), t1)*15)) + (rows((t0 + 2), t1)*6)) + rows((t0 + 3), t1)), (uint32)12), 255), 0))))
              }
            }
          }
        }
      }
    }
  }
}


Simplifying...
Lowering after first simplification:
let output.s0.y.max = (output.extent.1 + -1)
let output.s0.y.min = 0
let output.s0.x.max = (output.extent.0 + -1)
let output.s0.x.min = 0
let rows.s0.y.max = (output.extent.1 + -1)
let rows.s0.y.min = 0
let rows.s0.x.max = (output.extent.0 + 2)
let rows.s0.x.min = -3
let input.extent.0.required = ((((output.extent.0 + 5)/64)*64) + 64)
let input.min.0.required = -3
let input.stride.0.required = 1
let input.extent.1.required = ((((output.extent.1 + -1)/4)*4) + 10)
let input.min.1.required = -3
let input.stride.1.required = ((((output.extent.0 + 5)/64)*64) + 64)
let output.extent.0.required = ((((output.extent.0 + -1)/64)*64) + 64)
let output.min.0.required = 0
let output.stride.0.required = 1
let output.extent.1.required = ((((output.extent.1 + -1)/4)*4) + 4)
let output.min.1.required = 0
let output.stride.1.required = ((((output.extent.0 + -1)/64)*64) + 64)
let input.stride.0.constrained = 1
let input.min.0.constrained = 0
let input.stride.1.constrained = ((input.stride.1/64)*64)
let input.min.1.constrained = 0
let output.stride.0.constrained = 1
let output.min.0.constrained = 0
let output.stride.1.constrained = ((output.stride.1/64)*64)
let output.min.1.constrained = 0
let input.stride.0.proposed = 1
let input.min.0.proposed = 0
let input.extent.0.proposed = ((((output.extent.0 + 5)/64)*64) + 64)
let input.stride.1.proposed = (((output.extent.0 + 69)/64)*64)
let input.min.1.proposed = 0
let input.extent.1.proposed = ((((output.extent.1 + -1)/4)*4) + 10)
let output.stride.0.proposed = 1
let output.min.0.proposed = 0
let output.extent.0.proposed = ((((output.extent.0 + -1)/64)*64) + 64)
let output.stride.1.proposed = (((output.extent.0 + 63)/64)*64)
let output.min.1.proposed = 0
let output.extent.1.proposed = ((((output.extent.1 + -1)/4)*4) + 4)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  let output.s0.y.loop_max = (output.extent.1 + -1)
  let output.s0.y.loop_min = 0
  let output.s0.y.loop_extent = output.extent.1
  let output.s0.x.loop_max = (output.extent.0 + -1)
  let output.s0.x.loop_min = 0
  let output.s0.x.loop_extent = output.extent.0
  let output.s0.x.x.loop_extent = ((output.extent.0 + 63)/64)
  let output.s0.x.x.loop_max = ((output.extent.0 + -1)/64)
  let output.s0.x.x.loop_min = 0
  let output.s0.x.xi.loop_extent = 64
  let output.s0.x.xi.loop_max = 63
  let output.s0.x.xi.loop_min = 0
  let output.s0.y.y.loop_extent = ((output.extent.1 + 3)/4)
  let output.s0.y.y.loop_max = ((output.extent.1 + -1)/4)
  let output.s0.y.y.loop_min = 0
  let output.s0.y.yi.loop_extent = 4
  let output.s0.y.yi.loop_max = 3
  let output.s0.y.yi.loop_min = 0
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      let output.s0.y.min_1 = (output.s0.y.y*4)
      let output.s0.y.max_1 = ((output.s0.y.y*4) + 3)
      let output.s0.x.min_1 = 0
      let output.s0.x.max_1 = ((((output.extent.0 + 63)/64)*64) + -1)
      let rows.s0.y.max_1 = ((output.s0.y.y*4) + 3)
      let rows.s0.y.min_1 = (output.s0.y.y*4)
      let rows.s0.x.max_1 = ((((output.extent.0 + 63)/64)*64) + 2)
      let rows.s0.x.min_1 = -3
      prefetch input()
      let output.s0.y.yi.base = (output.s0.y.y*4)
      let rows.y.max_realized = ((output.s0.y.y*4) + 3)
      let rows.y.min_realized = (output.s0.y.y*4)
      let rows.y.extent_realized = 4
      let rows.x.max_realized = ((((output.extent.0 + 63)/64)*64) + 60)
      let rows.x.min_realized = -3
      let rows.x.extent_realized = ((((output.extent.0 + 63)/64)*64) + 64)
      realize rows([-3, ((((output.extent.0 + 63)/64)*64) + 64)], [(output.s0.y.y*4), 4]) {
        produce rows {
          let rows.s0.y.loop_max = ((output.s0.y.y*4) + 3)
          let rows.s0.y.loop_min = (output.s0.y.y*4)
          let rows.s0.y.loop_extent = 4
          let rows.s0.x.loop_max = ((((output.extent.0 + 63)/64)*64) + 2)
          let rows.s0.x.loop_min = -3
          let rows.s0.x.loop_extent = ((((output.extent.0 + 63)/64)*64) + 6)
          let rows.s0.x.x.loop_extent = ((output.extent.0 + 127)/64)
          let rows.s0.x.x.loop_max = ((output.extent.0 + 63)/64)
          let rows.s0.x.x.loop_min = 0
          let rows.s0.x.xi.loop_extent = 64
          let rows.s0.x.xi.loop_max = 63
          let rows.s0.x.xi.loop_min = 0
          let rows.s0.y.y.loop_extent = 1
          let rows.s0.y.y.loop_max = 0
          let rows.s0.y.y.loop_min = 0
          let rows.s0.y.yi.loop_extent = 4
          let rows.s0.y.yi.loop_max = 3
          let rows.s0.y.yi.loop_min = 0
          let rows.s0.y.y = 0
          let rows.s0.y.yi.base = (output.s0.y.y*4)
          for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
            let rows.s0.x.xi.base = ((rows.s0.x.x*64) + -3)
            unrolled (rows.s0.y.yi, 0, 4) {
              let rows.s0.y = ((output.s0.y.y*4) + rows.s0.y.yi)
              vectorized (rows.s0.x.xi, 0, 64) {
                let rows.s0.x = (((rows.s0.x.x*64) + rows.s0.x.xi) + -3)
                rows((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), ((output.s0.y.y*4) + rows.s0.y.yi)) = (let t4 = (((rows.s0.x.x*64) + rows.s0.x.xi) + -3) in (let t5 = ((output.s0.y.y*4) + rows.s0.y.yi) in (((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + 2)))*6) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + 1)))*15) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), ((output.s0.y.y*4) + rows.s0.y.yi)))*20) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + -1)))*15) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + -2)))*6) + int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + -3)))))))) + int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + 3))))))
              }
            }
          }
        }
        consume rows {
          for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
            let output.s0.x.xi.base = (output.s0.x.x*64)
            unrolled (output.s0.y.yi, 0, 4) {
              let output.s0.y = ((output.s0.y.y*4) + output.s0.y.yi)
              vectorized (output.s0.x.xi, 0, 64) {
                let output.s0.x = ((output.s0.x.x*64) + output.s0.x.xi)
                output(((output.s0.x.x*64) + output.s0.x.xi), ((output.s0.y.y*4) + output.s0.y.yi)) = (let t0 = ((output.s0.x.x*64) + output.s0.x.xi) in (let t1 = ((output.s0.y.y*4) + output.s0.y.yi) in uint8(max(min(((rows((((output.s0.x.x*64) + output.s0.x.xi) + 3), ((output.s0.y.y*4) + output.s0.y.yi)) + ((rows((((output.s0.x.x*64) + output.s0.x.xi) + 2), ((output.s0.y.y*4) + output.s0.y.yi))*6) + ((rows((((output.s0.x.x*64) + output.s0.x.xi) + 1), ((output.s0.y.y*4) + output.s0.y.yi))*15) + ((rows(((output.s0.x.x*64) + output.s0.x.xi), ((output.s0.y.y*4) + output.s0.y.yi))*20) + ((rows((((output.s0.x.x*64) + output.s0.x.xi) + -1), ((output.s0.y.y*4) + output.s0.y.yi))*15) + (rows((((output.s0.x.x*64) + output.s0.x.xi) + -3), ((output.s0.y.y*4) + output.s0.y.yi)) + (rows((((output.s0.x.x*64) + output.s0.x.xi) + -2), ((output.s0.y.y*4) + output.s0.y.yi))*6)))))))/4096), 255), 0))))
              }
            }
          }
        }
      }
    }
  }
}


Performing storage folding optimization...
Lowering after storage folding:
let output.s0.y.max = (output.extent.1 + -1)
let output.s0.y.min = 0
let output.s0.x.max = (output.extent.0 + -1)
let output.s0.x.min = 0
let rows.s0.y.max = (output.extent.1 + -1)
let rows.s0.y.min = 0
let rows.s0.x.max = (output.extent.0 + 2)
let rows.s0.x.min = -3
let input.extent.0.required = ((((output.extent.0 + 5)/64)*64) + 64)
let input.min.0.required = -3
let input.stride.0.required = 1
let input.extent.1.required = ((((output.extent.1 + -1)/4)*4) + 10)
let input.min.1.required = -3
let input.stride.1.required = ((((output.extent.0 + 5)/64)*64) + 64)
let output.extent.0.required = ((((output.extent.0 + -1)/64)*64) + 64)
let output.min.0.required = 0
let output.stride.0.required = 1
let output.extent.1.required = ((((output.extent.1 + -1)/4)*4) + 4)
let output.min.1.required = 0
let output.stride.1.required = ((((output.extent.0 + -1)/64)*64) + 64)
let input.stride.0.constrained = 1
let input.min.0.constrained = 0
let input.stride.1.constrained = ((input.stride.1/64)*64)
let input.min.1.constrained = 0
let output.stride.0.constrained = 1
let output.min.0.constrained = 0
let output.stride.1.constrained = ((output.stride.1/64)*64)
let output.min.1.constrained = 0
let input.stride.0.proposed = 1
let input.min.0.proposed = 0
let input.extent.0.proposed = ((((output.extent.0 + 5)/64)*64) + 64)
let input.stride.1.proposed = (((output.extent.0 + 69)/64)*64)
let input.min.1.proposed = 0
let input.extent.1.proposed = ((((output.extent.1 + -1)/4)*4) + 10)
let output.stride.0.proposed = 1
let output.min.0.proposed = 0
let output.extent.0.proposed = ((((output.extent.0 + -1)/64)*64) + 64)
let output.stride.1.proposed = (((output.extent.0 + 63)/64)*64)
let output.min.1.proposed = 0
let output.extent.1.proposed = ((((output.extent.1 + -1)/4)*4) + 4)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  let output.s0.y.loop_max = (output.extent.1 + -1)
  let output.s0.y.loop_min = 0
  let output.s0.y.loop_extent = output.extent.1
  let output.s0.x.loop_max = (output.extent.0 + -1)
  let output.s0.x.loop_min = 0
  let output.s0.x.loop_extent = output.extent.0
  let output.s0.x.x.loop_extent = ((output.extent.0 + 63)/64)
  let output.s0.x.x.loop_max = ((output.extent.0 + -1)/64)
  let output.s0.x.x.loop_min = 0
  let output.s0.x.xi.loop_extent = 64
  let output.s0.x.xi.loop_max = 63
  let output.s0.x.xi.loop_min = 0
  let output.s0.y.y.loop_extent = ((output.extent.1 + 3)/4)
  let output.s0.y.y.loop_max = ((output.extent.1 + -1)/4)
  let output.s0.y.y.loop_min = 0
  let output.s0.y.yi.loop_extent = 4
  let output.s0.y.yi.loop_max = 3
  let output.s0.y.yi.loop_min = 0
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      let output.s0.y.min_1 = (output.s0.y.y*4)
      let output.s0.y.max_1 = ((output.s0.y.y*4) + 3)
      let output.s0.x.min_1 = 0
      let output.s0.x.max_1 = ((((output.extent.0 + 63)/64)*64) + -1)
      let rows.s0.y.max_1 = ((output.s0.y.y*4) + 3)
      let rows.s0.y.min_1 = (output.s0.y.y*4)
      let rows.s0.x.max_1 = ((((output.extent.0 + 63)/64)*64) + 2)
      let rows.s0.x.min_1 = -3
      prefetch input()
      let output.s0.y.yi.base = (output.s0.y.y*4)
      let rows.y.max_realized = ((output.s0.y.y*4) + 3)
      let rows.y.min_realized = (output.s0.y.y*4)
      let rows.y.extent_realized = 4
      let rows.x.max_realized = ((((output.extent.0 + 63)/64)*64) + 60)
      let rows.x.min_realized = -3
      let rows.x.extent_realized = ((((output.extent.0 + 63)/64)*64) + 64)
      realize rows([-3, ((((output.extent.0 + 63)/64)*64) + 64)], [(output.s0.y.y*4), 4]) {
        produce rows {
          let rows.s0.y.loop_max = ((output.s0.y.y*4) + 3)
          let rows.s0.y.loop_min = (output.s0.y.y*4)
          let rows.s0.y.loop_extent = 4
          let rows.s0.x.loop_max = ((((output.extent.0 + 63)/64)*64) + 2)
          let rows.s0.x.loop_min = -3
          let rows.s0.x.loop_extent = ((((output.extent.0 + 63)/64)*64) + 6)
          let rows.s0.x.x.loop_extent = ((output.extent.0 + 127)/64)
          let rows.s0.x.x.loop_max = ((output.extent.0 + 63)/64)
          let rows.s0.x.x.loop_min = 0
          let rows.s0.x.xi.loop_extent = 64
          let rows.s0.x.xi.loop_max = 63
          let rows.s0.x.xi.loop_min = 0
          let rows.s0.y.y.loop_extent = 1
          let rows.s0.y.y.loop_max = 0
          let rows.s0.y.y.loop_min = 0
          let rows.s0.y.yi.loop_extent = 4
          let rows.s0.y.yi.loop_max = 3
          let rows.s0.y.yi.loop_min = 0
          let rows.s0.y.y = 0
          let rows.s0.y.yi.base = (output.s0.y.y*4)
          for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
            let rows.s0.x.xi.base = ((rows.s0.x.x*64) + -3)
            unrolled (rows.s0.y.yi, 0, 4) {
              let rows.s0.y = ((output.s0.y.y*4) + rows.s0.y.yi)
              vectorized (rows.s0.x.xi, 0, 64) {
                let rows.s0.x = (((rows.s0.x.x*64) + rows.s0.x.xi) + -3)
                rows((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), ((output.s0.y.y*4) + rows.s0.y.yi)) = (let t4 = (((rows.s0.x.x*64) + rows.s0.x.xi) + -3) in (let t5 = ((output.s0.y.y*4) + rows.s0.y.yi) in (((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + 2)))*6) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + 1)))*15) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), ((output.s0.y.y*4) + rows.s0.y.yi)))*20) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + -1)))*15) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + -2)))*6) + int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + -3)))))))) + int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + 3))))))
              }
            }
          }
        }
        consume rows {
          for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
            let output.s0.x.xi.base = (output.s0.x.x*64)
            unrolled (output.s0.y.yi, 0, 4) {
              let output.s0.y = ((output.s0.y.y*4) + output.s0.y.yi)
              vectorized (output.s0.x.xi, 0, 64) {
                let output.s0.x = ((output.s0.x.x*64) + output.s0.x.xi)
                output(((output.s0.x.x*64) + output.s0.x.xi), ((output.s0.y.y*4) + output.s0.y.yi)) = (let t0 = ((output.s0.x.x*64) + output.s0.x.xi) in (let t1 = ((output.s0.y.y*4) + output.s0.y.yi) in uint8(max(min(((rows((((output.s0.x.x*64) + output.s0.x.xi) + 3), ((output.s0.y.y*4) + output.s0.y.yi)) + ((rows((((output.s0.x.x*64) + output.s0.x.xi) + 2), ((output.s0.y.y*4) + output.s0.y.yi))*6) + ((rows((((output.s0.x.x*64) + output.s0.x.xi) + 1), ((output.s0.y.y*4) + output.s0.y.yi))*15) + ((rows(((output.s0.x.x*64) + output.s0.x.xi), ((output.s0.y.y*4) + output.s0.y.yi))*20) + ((rows((((output.s0.x.x*64) + output.s0.x.xi) + -1), ((output.s0.y.y*4) + output.s0.y.yi))*15) + (rows((((output.s0.x.x*64) + output.s0.x.xi) + -3), ((output.s0.y.y*4) + output.s0.y.yi)) + (rows((((output.s0.x.x*64) + output.s0.x.xi) + -2), ((output.s0.y.y*4) + output.s0.y.yi))*6)))))))/4096), 255), 0))))
              }
            }
          }
        }
      }
    }
  }
}

Injecting debug_to_file calls...
Lowering after injecting debug_to_file calls:
let output.s0.y.max = (output.extent.1 + -1)
let output.s0.y.min = 0
let output.s0.x.max = (output.extent.0 + -1)
let output.s0.x.min = 0
let rows.s0.y.max = (output.extent.1 + -1)
let rows.s0.y.min = 0
let rows.s0.x.max = (output.extent.0 + 2)
let rows.s0.x.min = -3
let input.extent.0.required = ((((output.extent.0 + 5)/64)*64) + 64)
let input.min.0.required = -3
let input.stride.0.required = 1
let input.extent.1.required = ((((output.extent.1 + -1)/4)*4) + 10)
let input.min.1.required = -3
let input.stride.1.required = ((((output.extent.0 + 5)/64)*64) + 64)
let output.extent.0.required = ((((output.extent.0 + -1)/64)*64) + 64)
let output.min.0.required = 0
let output.stride.0.required = 1
let output.extent.1.required = ((((output.extent.1 + -1)/4)*4) + 4)
let output.min.1.required = 0
let output.stride.1.required = ((((output.extent.0 + -1)/64)*64) + 64)
let input.stride.0.constrained = 1
let input.min.0.constrained = 0
let input.stride.1.constrained = ((input.stride.1/64)*64)
let input.min.1.constrained = 0
let output.stride.0.constrained = 1
let output.min.0.constrained = 0
let output.stride.1.constrained = ((output.stride.1/64)*64)
let output.min.1.constrained = 0
let input.stride.0.proposed = 1
let input.min.0.proposed = 0
let input.extent.0.proposed = ((((output.extent.0 + 5)/64)*64) + 64)
let input.stride.1.proposed = (((output.extent.0 + 69)/64)*64)
let input.min.1.proposed = 0
let input.extent.1.proposed = ((((output.extent.1 + -1)/4)*4) + 10)
let output.stride.0.proposed = 1
let output.min.0.proposed = 0
let output.extent.0.proposed = ((((output.extent.0 + -1)/64)*64) + 64)
let output.stride.1.proposed = (((output.extent.0 + 63)/64)*64)
let output.min.1.proposed = 0
let output.extent.1.proposed = ((((output.extent.1 + -1)/4)*4) + 4)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  let output.s0.y.loop_max = (output.extent.1 + -1)
  let output.s0.y.loop_min = 0
  let output.s0.y.loop_extent = output.extent.1
  let output.s0.x.loop_max = (output.extent.0 + -1)
  let output.s0.x.loop_min = 0
  let output.s0.x.loop_extent = output.extent.0
  let output.s0.x.x.loop_extent = ((output.extent.0 + 63)/64)
  let output.s0.x.x.loop_max = ((output.extent.0 + -1)/64)
  let output.s0.x.x.loop_min = 0
  let output.s0.x.xi.loop_extent = 64
  let output.s0.x.xi.loop_max = 63
  let output.s0.x.xi.loop_min = 0
  let output.s0.y.y.loop_extent = ((output.extent.1 + 3)/4)
  let output.s0.y.y.loop_max = ((output.extent.1 + -1)/4)
  let output.s0.y.y.loop_min = 0
  let output.s0.y.yi.loop_extent = 4
  let output.s0.y.yi.loop_max = 3
  let output.s0.y.yi.loop_min = 0
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      let output.s0.y.min_1 = (output.s0.y.y*4)
      let output.s0.y.max_1 = ((output.s0.y.y*4) + 3)
      let output.s0.x.min_1 = 0
      let output.s0.x.max_1 = ((((output.extent.0 + 63)/64)*64) + -1)
      let rows.s0.y.max_1 = ((output.s0.y.y*4) + 3)
      let rows.s0.y.min_1 = (output.s0.y.y*4)
      let rows.s0.x.max_1 = ((((output.extent.0 + 63)/64)*64) + 2)
      let rows.s0.x.min_1 = -3
      prefetch input()
      let output.s0.y.yi.base = (output.s0.y.y*4)
      let rows.y.max_realized = ((output.s0.y.y*4) + 3)
      let rows.y.min_realized = (output.s0.y.y*4)
      let rows.y.extent_realized = 4
      let rows.x.max_realized = ((((output.extent.0 + 63)/64)*64) + 60)
      let rows.x.min_realized = -3
      let rows.x.extent_realized = ((((output.extent.0 + 63)/64)*64) + 64)
      realize rows([-3, ((((output.extent.0 + 63)/64)*64) + 64)], [(output.s0.y.y*4), 4]) {
        produce rows {
          let rows.s0.y.loop_max = ((output.s0.y.y*4) + 3)
          let rows.s0.y.loop_min = (output.s0.y.y*4)
          let rows.s0.y.loop_extent = 4
          let rows.s0.x.loop_max = ((((output.extent.0 + 63)/64)*64) + 2)
          let rows.s0.x.loop_min = -3
          let rows.s0.x.loop_extent = ((((output.extent.0 + 63)/64)*64) + 6)
          let rows.s0.x.x.loop_extent = ((output.extent.0 + 127)/64)
          let rows.s0.x.x.loop_max = ((output.extent.0 + 63)/64)
          let rows.s0.x.x.loop_min = 0
          let rows.s0.x.xi.loop_extent = 64
          let rows.s0.x.xi.loop_max = 63
          let rows.s0.x.xi.loop_min = 0
          let rows.s0.y.y.loop_extent = 1
          let rows.s0.y.y.loop_max = 0
          let rows.s0.y.y.loop_min = 0
          let rows.s0.y.yi.loop_extent = 4
          let rows.s0.y.yi.loop_max = 3
          let rows.s0.y.yi.loop_min = 0
          let rows.s0.y.y = 0
          let rows.s0.y.yi.base = (output.s0.y.y*4)
          for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
            let rows.s0.x.xi.base = ((rows.s0.x.x*64) + -3)
            unrolled (rows.s0.y.yi, 0, 4) {
              let rows.s0.y = ((output.s0.y.y*4) + rows.s0.y.yi)
              vectorized (rows.s0.x.xi, 0, 64) {
                let rows.s0.x = (((rows.s0.x.x*64) + rows.s0.x.xi) + -3)
                rows((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), ((output.s0.y.y*4) + rows.s0.y.yi)) = (let t4 = (((rows.s0.x.x*64) + rows.s0.x.xi) + -3) in (let t5 = ((output.s0.y.y*4) + rows.s0.y.yi) in (((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + 2)))*6) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + 1)))*15) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), ((output.s0.y.y*4) + rows.s0.y.yi)))*20) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + -1)))*15) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + -2)))*6) + int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + -3)))))))) + int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + 3))))))
              }
            }
          }
        }
        consume rows {
          for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
            let output.s0.x.xi.base = (output.s0.x.x*64)
            unrolled (output.s0.y.yi, 0, 4) {
              let output.s0.y = ((output.s0.y.y*4) + output.s0.y.yi)
              vectorized (output.s0.x.xi, 0, 64) {
                let output.s0.x = ((output.s0.x.x*64) + output.s0.x.xi)
                output(((output.s0.x.x*64) + output.s0.x.xi), ((output.s0.y.y*4) + output.s0.y.yi)) = (let t0 = ((output.s0.x.x*64) + output.s0.x.xi) in (let t1 = ((output.s0.y.y*4) + output.s0.y.yi) in uint8(max(min(((rows((((output.s0.x.x*64) + output.s0.x.xi) + 3), ((output.s0.y.y*4) + output.s0.y.yi)) + ((rows((((output.s0.x.x*64) + output.s0.x.xi) + 2), ((output.s0.y.y*4) + output.s0.y.yi))*6) + ((rows((((output.s0.x.x*64) + output.s0.x.xi) + 1), ((output.s0.y.y*4) + output.s0.y.yi))*15) + ((rows(((output.s0.x.x*64) + output.s0.x.xi), ((output.s0.y.y*4) + output.s0.y.yi))*20) + ((rows((((output.s0.x.x*64) + output.s0.x.xi) + -1), ((output.s0.y.y*4) + output.s0.y.yi))*15) + (rows((((output.s0.x.x*64) + output.s0.x.xi) + -3), ((output.s0.y.y*4) + output.s0.y.yi)) + (rows((((output.s0.x.x*64) + output.s0.x.xi) + -2), ((output.s0.y.y*4) + output.s0.y.yi))*6)))))))/4096), 255), 0))))
              }
            }
          }
        }
      }
    }
  }
}

Injecting prefetches...
Lowering after injecting prefetches:
let output.s0.y.max = (output.extent.1 + -1)
let output.s0.y.min = 0
let output.s0.x.max = (output.extent.0 + -1)
let output.s0.x.min = 0
let rows.s0.y.max = (output.extent.1 + -1)
let rows.s0.y.min = 0
let rows.s0.x.max = (output.extent.0 + 2)
let rows.s0.x.min = -3
let input.extent.0.required = ((((output.extent.0 + 5)/64)*64) + 64)
let input.min.0.required = -3
let input.stride.0.required = 1
let input.extent.1.required = ((((output.extent.1 + -1)/4)*4) + 10)
let input.min.1.required = -3
let input.stride.1.required = ((((output.extent.0 + 5)/64)*64) + 64)
let output.extent.0.required = ((((output.extent.0 + -1)/64)*64) + 64)
let output.min.0.required = 0
let output.stride.0.required = 1
let output.extent.1.required = ((((output.extent.1 + -1)/4)*4) + 4)
let output.min.1.required = 0
let output.stride.1.required = ((((output.extent.0 + -1)/64)*64) + 64)
let input.stride.0.constrained = 1
let input.min.0.constrained = 0
let input.stride.1.constrained = ((input.stride.1/64)*64)
let input.min.1.constrained = 0
let output.stride.0.constrained = 1
let output.min.0.constrained = 0
let output.stride.1.constrained = ((output.stride.1/64)*64)
let output.min.1.constrained = 0
let input.stride.0.proposed = 1
let input.min.0.proposed = 0
let input.extent.0.proposed = ((((output.extent.0 + 5)/64)*64) + 64)
let input.stride.1.proposed = (((output.extent.0 + 69)/64)*64)
let input.min.1.proposed = 0
let input.extent.1.proposed = ((((output.extent.1 + -1)/4)*4) + 10)
let output.stride.0.proposed = 1
let output.min.0.proposed = 0
let output.extent.0.proposed = ((((output.extent.0 + -1)/64)*64) + 64)
let output.stride.1.proposed = (((output.extent.0 + 63)/64)*64)
let output.min.1.proposed = 0
let output.extent.1.proposed = ((((output.extent.1 + -1)/4)*4) + 4)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  let output.s0.y.loop_max = (output.extent.1 + -1)
  let output.s0.y.loop_min = 0
  let output.s0.y.loop_extent = output.extent.1
  let output.s0.x.loop_max = (output.extent.0 + -1)
  let output.s0.x.loop_min = 0
  let output.s0.x.loop_extent = output.extent.0
  let output.s0.x.x.loop_extent = ((output.extent.0 + 63)/64)
  let output.s0.x.x.loop_max = ((output.extent.0 + -1)/64)
  let output.s0.x.x.loop_min = 0
  let output.s0.x.xi.loop_extent = 64
  let output.s0.x.xi.loop_max = 63
  let output.s0.x.xi.loop_min = 0
  let output.s0.y.y.loop_extent = ((output.extent.1 + 3)/4)
  let output.s0.y.y.loop_max = ((output.extent.1 + -1)/4)
  let output.s0.y.y.loop_min = 0
  let output.s0.y.yi.loop_extent = 4
  let output.s0.y.yi.loop_max = 3
  let output.s0.y.yi.loop_min = 0
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      let output.s0.y.min_1 = (output.s0.y.y*4)
      let output.s0.y.max_1 = ((output.s0.y.y*4) + 3)
      let output.s0.x.min_1 = 0
      let output.s0.x.max_1 = ((((output.extent.0 + 63)/64)*64) + -1)
      let rows.s0.y.max_1 = ((output.s0.y.y*4) + 3)
      let rows.s0.y.min_1 = (output.s0.y.y*4)
      let rows.s0.x.max_1 = ((((output.extent.0 + 63)/64)*64) + 2)
      let rows.s0.x.min_1 = -3
      prefetch input([-3, ((((output.extent.0 + 63)/64)*64) + 64)], [((output.s0.y.y*4) + 5), 10])
      let output.s0.y.yi.base = (output.s0.y.y*4)
      let rows.y.max_realized = ((output.s0.y.y*4) + 3)
      let rows.y.min_realized = (output.s0.y.y*4)
      let rows.y.extent_realized = 4
      let rows.x.max_realized = ((((output.extent.0 + 63)/64)*64) + 60)
      let rows.x.min_realized = -3
      let rows.x.extent_realized = ((((output.extent.0 + 63)/64)*64) + 64)
      realize rows([-3, ((((output.extent.0 + 63)/64)*64) + 64)], [(output.s0.y.y*4), 4]) {
        produce rows {
          let rows.s0.y.loop_max = ((output.s0.y.y*4) + 3)
          let rows.s0.y.loop_min = (output.s0.y.y*4)
          let rows.s0.y.loop_extent = 4
          let rows.s0.x.loop_max = ((((output.extent.0 + 63)/64)*64) + 2)
          let rows.s0.x.loop_min = -3
          let rows.s0.x.loop_extent = ((((output.extent.0 + 63)/64)*64) + 6)
          let rows.s0.x.x.loop_extent = ((output.extent.0 + 127)/64)
          let rows.s0.x.x.loop_max = ((output.extent.0 + 63)/64)
          let rows.s0.x.x.loop_min = 0
          let rows.s0.x.xi.loop_extent = 64
          let rows.s0.x.xi.loop_max = 63
          let rows.s0.x.xi.loop_min = 0
          let rows.s0.y.y.loop_extent = 1
          let rows.s0.y.y.loop_max = 0
          let rows.s0.y.y.loop_min = 0
          let rows.s0.y.yi.loop_extent = 4
          let rows.s0.y.yi.loop_max = 3
          let rows.s0.y.yi.loop_min = 0
          let rows.s0.y.y = 0
          let rows.s0.y.yi.base = (output.s0.y.y*4)
          for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
            let rows.s0.x.xi.base = ((rows.s0.x.x*64) + -3)
            unrolled (rows.s0.y.yi, 0, 4) {
              let rows.s0.y = ((output.s0.y.y*4) + rows.s0.y.yi)
              vectorized (rows.s0.x.xi, 0, 64) {
                let rows.s0.x = (((rows.s0.x.x*64) + rows.s0.x.xi) + -3)
                rows((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), ((output.s0.y.y*4) + rows.s0.y.yi)) = (let t4 = (((rows.s0.x.x*64) + rows.s0.x.xi) + -3) in (let t5 = ((output.s0.y.y*4) + rows.s0.y.yi) in (((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + 2)))*6) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + 1)))*15) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), ((output.s0.y.y*4) + rows.s0.y.yi)))*20) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + -1)))*15) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + -2)))*6) + int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + -3)))))))) + int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + 3))))))
              }
            }
          }
        }
        consume rows {
          for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
            let output.s0.x.xi.base = (output.s0.x.x*64)
            unrolled (output.s0.y.yi, 0, 4) {
              let output.s0.y = ((output.s0.y.y*4) + output.s0.y.yi)
              vectorized (output.s0.x.xi, 0, 64) {
                let output.s0.x = ((output.s0.x.x*64) + output.s0.x.xi)
                output(((output.s0.x.x*64) + output.s0.x.xi), ((output.s0.y.y*4) + output.s0.y.yi)) = (let t0 = ((output.s0.x.x*64) + output.s0.x.xi) in (let t1 = ((output.s0.y.y*4) + output.s0.y.yi) in uint8(max(min(((rows((((output.s0.x.x*64) + output.s0.x.xi) + 3), ((output.s0.y.y*4) + output.s0.y.yi)) + ((rows((((output.s0.x.x*64) + output.s0.x.xi) + 2), ((output.s0.y.y*4) + output.s0.y.yi))*6) + ((rows((((output.s0.x.x*64) + output.s0.x.xi) + 1), ((output.s0.y.y*4) + output.s0.y.yi))*15) + ((rows(((output.s0.x.x*64) + output.s0.x.xi), ((output.s0.y.y*4) + output.s0.y.yi))*20) + ((rows((((output.s0.x.x*64) + output.s0.x.xi) + -1), ((output.s0.y.y*4) + output.s0.y.yi))*15) + (rows((((output.s0.x.x*64) + output.s0.x.xi) + -3), ((output.s0.y.y*4) + output.s0.y.yi)) + (rows((((output.s0.x.x*64) + output.s0.x.xi) + -2), ((output.s0.y.y*4) + output.s0.y.yi))*6)))))))/4096), 255), 0))))
              }
            }
          }
        }
      }
    }
  }
}


Dynamically skipping stages...
skip_stages checking cols
skip_stages checking rows
skip_stages checking input_32
skip_stages checking input_im
Lowering after dynamically skipping stages:
let output.s0.y.max = (output.extent.1 + -1)
let output.s0.y.min = 0
let output.s0.x.max = (output.extent.0 + -1)
let output.s0.x.min = 0
let rows.s0.y.max = (output.extent.1 + -1)
let rows.s0.y.min = 0
let rows.s0.x.max = (output.extent.0 + 2)
let rows.s0.x.min = -3
let input.extent.0.required = ((((output.extent.0 + 5)/64)*64) + 64)
let input.min.0.required = -3
let input.stride.0.required = 1
let input.extent.1.required = ((((output.extent.1 + -1)/4)*4) + 10)
let input.min.1.required = -3
let input.stride.1.required = ((((output.extent.0 + 5)/64)*64) + 64)
let output.extent.0.required = ((((output.extent.0 + -1)/64)*64) + 64)
let output.min.0.required = 0
let output.stride.0.required = 1
let output.extent.1.required = ((((output.extent.1 + -1)/4)*4) + 4)
let output.min.1.required = 0
let output.stride.1.required = ((((output.extent.0 + -1)/64)*64) + 64)
let input.stride.0.constrained = 1
let input.min.0.constrained = 0
let input.stride.1.constrained = ((input.stride.1/64)*64)
let input.min.1.constrained = 0
let output.stride.0.constrained = 1
let output.min.0.constrained = 0
let output.stride.1.constrained = ((output.stride.1/64)*64)
let output.min.1.constrained = 0
let input.stride.0.proposed = 1
let input.min.0.proposed = 0
let input.extent.0.proposed = ((((output.extent.0 + 5)/64)*64) + 64)
let input.stride.1.proposed = (((output.extent.0 + 69)/64)*64)
let input.min.1.proposed = 0
let input.extent.1.proposed = ((((output.extent.1 + -1)/4)*4) + 10)
let output.stride.0.proposed = 1
let output.min.0.proposed = 0
let output.extent.0.proposed = ((((output.extent.0 + -1)/64)*64) + 64)
let output.stride.1.proposed = (((output.extent.0 + 63)/64)*64)
let output.min.1.proposed = 0
let output.extent.1.proposed = ((((output.extent.1 + -1)/4)*4) + 4)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  let output.s0.y.loop_max = (output.extent.1 + -1)
  let output.s0.y.loop_min = 0
  let output.s0.y.loop_extent = output.extent.1
  let output.s0.x.loop_max = (output.extent.0 + -1)
  let output.s0.x.loop_min = 0
  let output.s0.x.loop_extent = output.extent.0
  let output.s0.x.x.loop_extent = ((output.extent.0 + 63)/64)
  let output.s0.x.x.loop_max = ((output.extent.0 + -1)/64)
  let output.s0.x.x.loop_min = 0
  let output.s0.x.xi.loop_extent = 64
  let output.s0.x.xi.loop_max = 63
  let output.s0.x.xi.loop_min = 0
  let output.s0.y.y.loop_extent = ((output.extent.1 + 3)/4)
  let output.s0.y.y.loop_max = ((output.extent.1 + -1)/4)
  let output.s0.y.y.loop_min = 0
  let output.s0.y.yi.loop_extent = 4
  let output.s0.y.yi.loop_max = 3
  let output.s0.y.yi.loop_min = 0
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      let output.s0.y.min_1 = (output.s0.y.y*4)
      let output.s0.y.max_1 = ((output.s0.y.y*4) + 3)
      let output.s0.x.min_1 = 0
      let output.s0.x.max_1 = ((((output.extent.0 + 63)/64)*64) + -1)
      let rows.s0.y.max_1 = ((output.s0.y.y*4) + 3)
      let rows.s0.y.min_1 = (output.s0.y.y*4)
      let rows.s0.x.max_1 = ((((output.extent.0 + 63)/64)*64) + 2)
      let rows.s0.x.min_1 = -3
      prefetch input([-3, ((((output.extent.0 + 63)/64)*64) + 64)], [((output.s0.y.y*4) + 5), 10])
      let output.s0.y.yi.base = (output.s0.y.y*4)
      let rows.y.max_realized = ((output.s0.y.y*4) + 3)
      let rows.y.min_realized = (output.s0.y.y*4)
      let rows.y.extent_realized = 4
      let rows.x.max_realized = ((((output.extent.0 + 63)/64)*64) + 60)
      let rows.x.min_realized = -3
      let rows.x.extent_realized = ((((output.extent.0 + 63)/64)*64) + 64)
      realize rows([-3, ((((output.extent.0 + 63)/64)*64) + 64)], [(output.s0.y.y*4), 4]) {
        produce rows {
          let rows.s0.y.loop_max = ((output.s0.y.y*4) + 3)
          let rows.s0.y.loop_min = (output.s0.y.y*4)
          let rows.s0.y.loop_extent = 4
          let rows.s0.x.loop_max = ((((output.extent.0 + 63)/64)*64) + 2)
          let rows.s0.x.loop_min = -3
          let rows.s0.x.loop_extent = ((((output.extent.0 + 63)/64)*64) + 6)
          let rows.s0.x.x.loop_extent = ((output.extent.0 + 127)/64)
          let rows.s0.x.x.loop_max = ((output.extent.0 + 63)/64)
          let rows.s0.x.x.loop_min = 0
          let rows.s0.x.xi.loop_extent = 64
          let rows.s0.x.xi.loop_max = 63
          let rows.s0.x.xi.loop_min = 0
          let rows.s0.y.y.loop_extent = 1
          let rows.s0.y.y.loop_max = 0
          let rows.s0.y.y.loop_min = 0
          let rows.s0.y.yi.loop_extent = 4
          let rows.s0.y.yi.loop_max = 3
          let rows.s0.y.yi.loop_min = 0
          let rows.s0.y.y = 0
          let rows.s0.y.yi.base = (output.s0.y.y*4)
          for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
            let rows.s0.x.xi.base = ((rows.s0.x.x*64) + -3)
            unrolled (rows.s0.y.yi, 0, 4) {
              let rows.s0.y = ((output.s0.y.y*4) + rows.s0.y.yi)
              vectorized (rows.s0.x.xi, 0, 64) {
                let rows.s0.x = (((rows.s0.x.x*64) + rows.s0.x.xi) + -3)
                rows((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), ((output.s0.y.y*4) + rows.s0.y.yi)) = (let t4 = (((rows.s0.x.x*64) + rows.s0.x.xi) + -3) in (let t5 = ((output.s0.y.y*4) + rows.s0.y.yi) in (((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + 2)))*6) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + 1)))*15) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), ((output.s0.y.y*4) + rows.s0.y.yi)))*20) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + -1)))*15) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + -2)))*6) + int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + -3)))))))) + int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + 3))))))
              }
            }
          }
        }
        consume rows {
          for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
            let output.s0.x.xi.base = (output.s0.x.x*64)
            unrolled (output.s0.y.yi, 0, 4) {
              let output.s0.y = ((output.s0.y.y*4) + output.s0.y.yi)
              vectorized (output.s0.x.xi, 0, 64) {
                let output.s0.x = ((output.s0.x.x*64) + output.s0.x.xi)
                output(((output.s0.x.x*64) + output.s0.x.xi), ((output.s0.y.y*4) + output.s0.y.yi)) = (let t0 = ((output.s0.x.x*64) + output.s0.x.xi) in (let t1 = ((output.s0.y.y*4) + output.s0.y.yi) in uint8(max(min(((rows((((output.s0.x.x*64) + output.s0.x.xi) + 3), ((output.s0.y.y*4) + output.s0.y.yi)) + ((rows((((output.s0.x.x*64) + output.s0.x.xi) + 2), ((output.s0.y.y*4) + output.s0.y.yi))*6) + ((rows((((output.s0.x.x*64) + output.s0.x.xi) + 1), ((output.s0.y.y*4) + output.s0.y.yi))*15) + ((rows(((output.s0.x.x*64) + output.s0.x.xi), ((output.s0.y.y*4) + output.s0.y.yi))*20) + ((rows((((output.s0.x.x*64) + output.s0.x.xi) + -1), ((output.s0.y.y*4) + output.s0.y.yi))*15) + (rows((((output.s0.x.x*64) + output.s0.x.xi) + -3), ((output.s0.y.y*4) + output.s0.y.yi)) + (rows((((output.s0.x.x*64) + output.s0.x.xi) + -2), ((output.s0.y.y*4) + output.s0.y.yi))*6)))))))/4096), 255), 0))))
              }
            }
          }
        }
      }
    }
  }
}


Forking asynchronous producers...
Lowering after forking asynchronous producers:
let output.s0.y.max = (output.extent.1 + -1)
let output.s0.y.min = 0
let output.s0.x.max = (output.extent.0 + -1)
let output.s0.x.min = 0
let rows.s0.y.max = (output.extent.1 + -1)
let rows.s0.y.min = 0
let rows.s0.x.max = (output.extent.0 + 2)
let rows.s0.x.min = -3
let input.extent.0.required = ((((output.extent.0 + 5)/64)*64) + 64)
let input.min.0.required = -3
let input.stride.0.required = 1
let input.extent.1.required = ((((output.extent.1 + -1)/4)*4) + 10)
let input.min.1.required = -3
let input.stride.1.required = ((((output.extent.0 + 5)/64)*64) + 64)
let output.extent.0.required = ((((output.extent.0 + -1)/64)*64) + 64)
let output.min.0.required = 0
let output.stride.0.required = 1
let output.extent.1.required = ((((output.extent.1 + -1)/4)*4) + 4)
let output.min.1.required = 0
let output.stride.1.required = ((((output.extent.0 + -1)/64)*64) + 64)
let input.stride.0.constrained = 1
let input.min.0.constrained = 0
let input.stride.1.constrained = ((input.stride.1/64)*64)
let input.min.1.constrained = 0
let output.stride.0.constrained = 1
let output.min.0.constrained = 0
let output.stride.1.constrained = ((output.stride.1/64)*64)
let output.min.1.constrained = 0
let input.stride.0.proposed = 1
let input.min.0.proposed = 0
let input.extent.0.proposed = ((((output.extent.0 + 5)/64)*64) + 64)
let input.stride.1.proposed = (((output.extent.0 + 69)/64)*64)
let input.min.1.proposed = 0
let input.extent.1.proposed = ((((output.extent.1 + -1)/4)*4) + 10)
let output.stride.0.proposed = 1
let output.min.0.proposed = 0
let output.extent.0.proposed = ((((output.extent.0 + -1)/64)*64) + 64)
let output.stride.1.proposed = (((output.extent.0 + 63)/64)*64)
let output.min.1.proposed = 0
let output.extent.1.proposed = ((((output.extent.1 + -1)/4)*4) + 4)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
let output.s0.y.loop_max = (output.extent.1 + -1)
let output.s0.y.loop_min = 0
let output.s0.y.loop_extent = output.extent.1
let output.s0.x.loop_max = (output.extent.0 + -1)
let output.s0.x.loop_min = 0
let output.s0.x.loop_extent = output.extent.0
let output.s0.x.x.loop_extent = ((output.extent.0 + 63)/64)
let output.s0.x.x.loop_max = ((output.extent.0 + -1)/64)
let output.s0.x.x.loop_min = 0
let output.s0.x.xi.loop_extent = 64
let output.s0.x.xi.loop_max = 63
let output.s0.x.xi.loop_min = 0
let output.s0.y.y.loop_extent = ((output.extent.1 + 3)/4)
let output.s0.y.y.loop_max = ((output.extent.1 + -1)/4)
let output.s0.y.y.loop_min = 0
let output.s0.y.yi.loop_extent = 4
let output.s0.y.yi.loop_max = 3
let output.s0.y.yi.loop_min = 0
produce output {
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      let output.s0.y.min_1 = (output.s0.y.y*4)
      let output.s0.y.max_1 = ((output.s0.y.y*4) + 3)
      let output.s0.x.min_1 = 0
      let output.s0.x.max_1 = ((((output.extent.0 + 63)/64)*64) + -1)
      let rows.s0.y.max_1 = ((output.s0.y.y*4) + 3)
      let rows.s0.y.min_1 = (output.s0.y.y*4)
      let rows.s0.x.max_1 = ((((output.extent.0 + 63)/64)*64) + 2)
      let rows.s0.x.min_1 = -3
      prefetch input([-3, ((((output.extent.0 + 63)/64)*64) + 64)], [((output.s0.y.y*4) + 5), 10])
      let output.s0.y.yi.base = (output.s0.y.y*4)
      let rows.y.max_realized = ((output.s0.y.y*4) + 3)
      let rows.y.min_realized = (output.s0.y.y*4)
      let rows.y.extent_realized = 4
      let rows.x.max_realized = ((((output.extent.0 + 63)/64)*64) + 60)
      let rows.x.min_realized = -3
      let rows.x.extent_realized = ((((output.extent.0 + 63)/64)*64) + 64)
      realize rows([-3, ((((output.extent.0 + 63)/64)*64) + 64)], [(output.s0.y.y*4), 4]) {
        let rows.s0.y.loop_max = ((output.s0.y.y*4) + 3)
        let rows.s0.y.loop_min = (output.s0.y.y*4)
        let rows.s0.y.loop_extent = 4
        let rows.s0.x.loop_max = ((((output.extent.0 + 63)/64)*64) + 2)
        let rows.s0.x.loop_min = -3
        let rows.s0.x.loop_extent = ((((output.extent.0 + 63)/64)*64) + 6)
        let rows.s0.x.x.loop_extent = ((output.extent.0 + 127)/64)
        let rows.s0.x.x.loop_max = ((output.extent.0 + 63)/64)
        let rows.s0.x.x.loop_min = 0
        let rows.s0.x.xi.loop_extent = 64
        let rows.s0.x.xi.loop_max = 63
        let rows.s0.x.xi.loop_min = 0
        let rows.s0.y.y.loop_extent = 1
        let rows.s0.y.y.loop_max = 0
        let rows.s0.y.y.loop_min = 0
        let rows.s0.y.yi.loop_extent = 4
        let rows.s0.y.yi.loop_max = 3
        let rows.s0.y.yi.loop_min = 0
        let rows.s0.y.y = 0
        let rows.s0.y.yi.base = (output.s0.y.y*4)
        produce rows {
          for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
            let rows.s0.x.xi.base = ((rows.s0.x.x*64) + -3)
            unrolled (rows.s0.y.yi, 0, 4) {
              let rows.s0.y = ((output.s0.y.y*4) + rows.s0.y.yi)
              vectorized (rows.s0.x.xi, 0, 64) {
                let rows.s0.x = (((rows.s0.x.x*64) + rows.s0.x.xi) + -3)
                rows((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), ((output.s0.y.y*4) + rows.s0.y.yi)) = (let t4 = (((rows.s0.x.x*64) + rows.s0.x.xi) + -3) in (let t5 = ((output.s0.y.y*4) + rows.s0.y.yi) in (((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + 2)))*6) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + 1)))*15) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), ((output.s0.y.y*4) + rows.s0.y.yi)))*20) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + -1)))*15) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + -2)))*6) + int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + -3)))))))) + int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + 3))))))
              }
            }
          }
        }
        consume rows {
          for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
            let output.s0.x.xi.base = (output.s0.x.x*64)
            unrolled (output.s0.y.yi, 0, 4) {
              let output.s0.y = ((output.s0.y.y*4) + output.s0.y.yi)
              vectorized (output.s0.x.xi, 0, 64) {
                let output.s0.x = ((output.s0.x.x*64) + output.s0.x.xi)
                output(((output.s0.x.x*64) + output.s0.x.xi), ((output.s0.y.y*4) + output.s0.y.yi)) = (let t0 = ((output.s0.x.x*64) + output.s0.x.xi) in (let t1 = ((output.s0.y.y*4) + output.s0.y.yi) in uint8(max(min(((rows((((output.s0.x.x*64) + output.s0.x.xi) + 3), ((output.s0.y.y*4) + output.s0.y.yi)) + ((rows((((output.s0.x.x*64) + output.s0.x.xi) + 2), ((output.s0.y.y*4) + output.s0.y.yi))*6) + ((rows((((output.s0.x.x*64) + output.s0.x.xi) + 1), ((output.s0.y.y*4) + output.s0.y.yi))*15) + ((rows(((output.s0.x.x*64) + output.s0.x.xi), ((output.s0.y.y*4) + output.s0.y.yi))*20) + ((rows((((output.s0.x.x*64) + output.s0.x.xi) + -1), ((output.s0.y.y*4) + output.s0.y.yi))*15) + (rows((((output.s0.x.x*64) + output.s0.x.xi) + -3), ((output.s0.y.y*4) + output.s0.y.yi)) + (rows((((output.s0.x.x*64) + output.s0.x.xi) + -2), ((output.s0.y.y*4) + output.s0.y.yi))*6)))))))/4096), 255), 0))))
              }
            }
          }
        }
      }
    }
  }
}

Destructuring tuple-valued realizations...
Lowering after destructuring tuple-valued realizations:
let output.s0.y.max = (output.extent.1 + -1)
let output.s0.y.min = 0
let output.s0.x.max = (output.extent.0 + -1)
let output.s0.x.min = 0
let rows.s0.y.max = (output.extent.1 + -1)
let rows.s0.y.min = 0
let rows.s0.x.max = (output.extent.0 + 2)
let rows.s0.x.min = -3
let input.extent.0.required = ((((output.extent.0 + 5)/64)*64) + 64)
let input.min.0.required = -3
let input.stride.0.required = 1
let input.extent.1.required = ((((output.extent.1 + -1)/4)*4) + 10)
let input.min.1.required = -3
let input.stride.1.required = ((((output.extent.0 + 5)/64)*64) + 64)
let output.extent.0.required = ((((output.extent.0 + -1)/64)*64) + 64)
let output.min.0.required = 0
let output.stride.0.required = 1
let output.extent.1.required = ((((output.extent.1 + -1)/4)*4) + 4)
let output.min.1.required = 0
let output.stride.1.required = ((((output.extent.0 + -1)/64)*64) + 64)
let input.stride.0.constrained = 1
let input.min.0.constrained = 0
let input.stride.1.constrained = ((input.stride.1/64)*64)
let input.min.1.constrained = 0
let output.stride.0.constrained = 1
let output.min.0.constrained = 0
let output.stride.1.constrained = ((output.stride.1/64)*64)
let output.min.1.constrained = 0
let input.stride.0.proposed = 1
let input.min.0.proposed = 0
let input.extent.0.proposed = ((((output.extent.0 + 5)/64)*64) + 64)
let input.stride.1.proposed = (((output.extent.0 + 69)/64)*64)
let input.min.1.proposed = 0
let input.extent.1.proposed = ((((output.extent.1 + -1)/4)*4) + 10)
let output.stride.0.proposed = 1
let output.min.0.proposed = 0
let output.extent.0.proposed = ((((output.extent.0 + -1)/64)*64) + 64)
let output.stride.1.proposed = (((output.extent.0 + 63)/64)*64)
let output.min.1.proposed = 0
let output.extent.1.proposed = ((((output.extent.1 + -1)/4)*4) + 4)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
let output.s0.y.loop_max = (output.extent.1 + -1)
let output.s0.y.loop_min = 0
let output.s0.y.loop_extent = output.extent.1
let output.s0.x.loop_max = (output.extent.0 + -1)
let output.s0.x.loop_min = 0
let output.s0.x.loop_extent = output.extent.0
let output.s0.x.x.loop_extent = ((output.extent.0 + 63)/64)
let output.s0.x.x.loop_max = ((output.extent.0 + -1)/64)
let output.s0.x.x.loop_min = 0
let output.s0.x.xi.loop_extent = 64
let output.s0.x.xi.loop_max = 63
let output.s0.x.xi.loop_min = 0
let output.s0.y.y.loop_extent = ((output.extent.1 + 3)/4)
let output.s0.y.y.loop_max = ((output.extent.1 + -1)/4)
let output.s0.y.y.loop_min = 0
let output.s0.y.yi.loop_extent = 4
let output.s0.y.yi.loop_max = 3
let output.s0.y.yi.loop_min = 0
produce output {
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      let output.s0.y.min_1 = (output.s0.y.y*4)
      let output.s0.y.max_1 = ((output.s0.y.y*4) + 3)
      let output.s0.x.min_1 = 0
      let output.s0.x.max_1 = ((((output.extent.0 + 63)/64)*64) + -1)
      let rows.s0.y.max_1 = ((output.s0.y.y*4) + 3)
      let rows.s0.y.min_1 = (output.s0.y.y*4)
      let rows.s0.x.max_1 = ((((output.extent.0 + 63)/64)*64) + 2)
      let rows.s0.x.min_1 = -3
      prefetch input([-3, ((((output.extent.0 + 63)/64)*64) + 64)], [((output.s0.y.y*4) + 5), 10])
      let output.s0.y.yi.base = (output.s0.y.y*4)
      let rows.y.max_realized = ((output.s0.y.y*4) + 3)
      let rows.y.min_realized = (output.s0.y.y*4)
      let rows.y.extent_realized = 4
      let rows.x.max_realized = ((((output.extent.0 + 63)/64)*64) + 60)
      let rows.x.min_realized = -3
      let rows.x.extent_realized = ((((output.extent.0 + 63)/64)*64) + 64)
      realize rows([-3, ((((output.extent.0 + 63)/64)*64) + 64)], [(output.s0.y.y*4), 4]) {
        let rows.s0.y.loop_max = ((output.s0.y.y*4) + 3)
        let rows.s0.y.loop_min = (output.s0.y.y*4)
        let rows.s0.y.loop_extent = 4
        let rows.s0.x.loop_max = ((((output.extent.0 + 63)/64)*64) + 2)
        let rows.s0.x.loop_min = -3
        let rows.s0.x.loop_extent = ((((output.extent.0 + 63)/64)*64) + 6)
        let rows.s0.x.x.loop_extent = ((output.extent.0 + 127)/64)
        let rows.s0.x.x.loop_max = ((output.extent.0 + 63)/64)
        let rows.s0.x.x.loop_min = 0
        let rows.s0.x.xi.loop_extent = 64
        let rows.s0.x.xi.loop_max = 63
        let rows.s0.x.xi.loop_min = 0
        let rows.s0.y.y.loop_extent = 1
        let rows.s0.y.y.loop_max = 0
        let rows.s0.y.y.loop_min = 0
        let rows.s0.y.yi.loop_extent = 4
        let rows.s0.y.yi.loop_max = 3
        let rows.s0.y.yi.loop_min = 0
        let rows.s0.y.y = 0
        let rows.s0.y.yi.base = (output.s0.y.y*4)
        produce rows {
          for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
            let rows.s0.x.xi.base = ((rows.s0.x.x*64) + -3)
            unrolled (rows.s0.y.yi, 0, 4) {
              let rows.s0.y = ((output.s0.y.y*4) + rows.s0.y.yi)
              vectorized (rows.s0.x.xi, 0, 64) {
                let rows.s0.x = (((rows.s0.x.x*64) + rows.s0.x.xi) + -3)
                rows((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), ((output.s0.y.y*4) + rows.s0.y.yi)) = (let t4 = (((rows.s0.x.x*64) + rows.s0.x.xi) + -3) in (let t5 = ((output.s0.y.y*4) + rows.s0.y.yi) in (((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + 2)))*6) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + 1)))*15) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), ((output.s0.y.y*4) + rows.s0.y.yi)))*20) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + -1)))*15) + ((int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + -2)))*6) + int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + -3)))))))) + int32(input((((rows.s0.x.x*64) + rows.s0.x.xi) + -3), (((output.s0.y.y*4) + rows.s0.y.yi) + 3))))))
              }
            }
          }
        }
        consume rows {
          for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
            let output.s0.x.xi.base = (output.s0.x.x*64)
            unrolled (output.s0.y.yi, 0, 4) {
              let output.s0.y = ((output.s0.y.y*4) + output.s0.y.yi)
              vectorized (output.s0.x.xi, 0, 64) {
                let output.s0.x = ((output.s0.x.x*64) + output.s0.x.xi)
                output(((output.s0.x.x*64) + output.s0.x.xi), ((output.s0.y.y*4) + output.s0.y.yi)) = (let t0 = ((output.s0.x.x*64) + output.s0.x.xi) in (let t1 = ((output.s0.y.y*4) + output.s0.y.yi) in uint8(max(min(((rows((((output.s0.x.x*64) + output.s0.x.xi) + 3), ((output.s0.y.y*4) + output.s0.y.yi)) + ((rows((((output.s0.x.x*64) + output.s0.x.xi) + 2), ((output.s0.y.y*4) + output.s0.y.yi))*6) + ((rows((((output.s0.x.x*64) + output.s0.x.xi) + 1), ((output.s0.y.y*4) + output.s0.y.yi))*15) + ((rows(((output.s0.x.x*64) + output.s0.x.xi), ((output.s0.y.y*4) + output.s0.y.yi))*20) + ((rows((((output.s0.x.x*64) + output.s0.x.xi) + -1), ((output.s0.y.y*4) + output.s0.y.yi))*15) + (rows((((output.s0.x.x*64) + output.s0.x.xi) + -3), ((output.s0.y.y*4) + output.s0.y.yi)) + (rows((((output.s0.x.x*64) + output.s0.x.xi) + -2), ((output.s0.y.y*4) + output.s0.y.yi))*6)))))))/4096), 255), 0))))
              }
            }
          }
        }
      }
    }
  }
}


Performing storage flattening...
Lowering after storage flattening:
let output.s0.y.max = (output.extent.1 + -1)
let output.s0.y.min = 0
let output.s0.x.max = (output.extent.0 + -1)
let output.s0.x.min = 0
let rows.s0.y.max = (output.extent.1 + -1)
let rows.s0.y.min = 0
let rows.s0.x.max = (output.extent.0 + 2)
let rows.s0.x.min = -3
let input.extent.0.required = ((((output.extent.0 + 5)/64)*64) + 64)
let input.min.0.required = -3
let input.stride.0.required = 1
let input.extent.1.required = ((((output.extent.1 + -1)/4)*4) + 10)
let input.min.1.required = -3
let input.stride.1.required = ((((output.extent.0 + 5)/64)*64) + 64)
let output.extent.0.required = ((((output.extent.0 + -1)/64)*64) + 64)
let output.min.0.required = 0
let output.stride.0.required = 1
let output.extent.1.required = ((((output.extent.1 + -1)/4)*4) + 4)
let output.min.1.required = 0
let output.stride.1.required = ((((output.extent.0 + -1)/64)*64) + 64)
let input.stride.0.constrained = 1
let input.min.0.constrained = 0
let input.stride.1.constrained = ((input.stride.1/64)*64)
let input.min.1.constrained = 0
let output.stride.0.constrained = 1
let output.min.0.constrained = 0
let output.stride.1.constrained = ((output.stride.1/64)*64)
let output.min.1.constrained = 0
let input.stride.0.proposed = 1
let input.min.0.proposed = 0
let input.extent.0.proposed = ((((output.extent.0 + 5)/64)*64) + 64)
let input.stride.1.proposed = (((output.extent.0 + 69)/64)*64)
let input.min.1.proposed = 0
let input.extent.1.proposed = ((((output.extent.1 + -1)/4)*4) + 10)
let output.stride.0.proposed = 1
let output.min.0.proposed = 0
let output.extent.0.proposed = ((((output.extent.0 + -1)/64)*64) + 64)
let output.stride.1.proposed = (((output.extent.0 + 63)/64)*64)
let output.min.1.proposed = 0
let output.extent.1.proposed = ((((output.extent.1 + -1)/4)*4) + 4)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
let output.s0.y.loop_max = (output.extent.1 + -1)
let output.s0.y.loop_min = 0
let output.s0.y.loop_extent = output.extent.1
let output.s0.x.loop_max = (output.extent.0 + -1)
let output.s0.x.loop_min = 0
let output.s0.x.loop_extent = output.extent.0
let output.s0.x.x.loop_extent = ((output.extent.0 + 63)/64)
let output.s0.x.x.loop_max = ((output.extent.0 + -1)/64)
let output.s0.x.x.loop_min = 0
let output.s0.x.xi.loop_extent = 64
let output.s0.x.xi.loop_max = 63
let output.s0.x.xi.loop_min = 0
let output.s0.y.y.loop_extent = ((output.extent.1 + 3)/4)
let output.s0.y.y.loop_max = ((output.extent.1 + -1)/4)
let output.s0.y.y.loop_min = 0
let output.s0.y.yi.loop_extent = 4
let output.s0.y.yi.loop_max = 3
let output.s0.y.yi.loop_min = 0
produce output {
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      let output.s0.y.min_1 = (output.s0.y.y*4)
      let output.s0.y.max_1 = ((output.s0.y.y*4) + 3)
      let output.s0.x.min_1 = 0
      let output.s0.x.max_1 = ((((output.extent.0 + 63)/64)*64) + -1)
      let rows.s0.y.max_1 = ((output.s0.y.y*4) + 3)
      let rows.s0.y.min_1 = (output.s0.y.y*4)
      let rows.s0.x.max_1 = ((((output.extent.0 + 63)/64)*64) + 2)
      let rows.s0.x.min_1 = -3
      prefetch(input, ((((0 + (-3*input.stride.0)) + ((output.s0.y.y*4)*input.stride.1)) - ((0 + (input.min.0*input.stride.0)) + (input.min.1*input.stride.1))) + (0 + (input.stride.1*5))), ((((output.extent.0 + 63)/64)*64) + 64), input.stride.0, 10, input.stride.1)
      let output.s0.y.yi.base = (output.s0.y.y*4)
      let rows.y.max_realized = ((output.s0.y.y*4) + 3)
      let rows.y.min_realized = (output.s0.y.y*4)
      let rows.y.extent_realized = 4
      let rows.x.max_realized = ((((output.extent.0 + 63)/64)*64) + 60)
      let rows.x.min_realized = -3
      let rows.x.extent_realized = ((((output.extent.0 + 63)/64)*64) + 64)
      let rows.extent.0 = ((((output.extent.0 + 63)/64)*64) + 64)
      let rows.min.0 = -3
      let rows.extent.1 = 4
      let rows.min.1 = (output.s0.y.y*4)
      let rows.stride.0 = 1
      let rows.stride.1 = (rows.stride.0*((((output.extent.0 + 63)/64)*64) + 64))
      allocate rows[int32 * ((((output.extent.0 + 63)/64)*64) + 64) * 4]
      let rows.buffer = (let t8 = make_struct((halide_dimension_t *), rows.min.0, rows.extent.0, rows.stride.0, 0, rows.min.1, rows.extent.1, rows.stride.1, 0) in _halide_buffer_init(alloca(size_of_halide_buffer_t()), t8, rows, (uint64)0, reinterpret((halide_device_interface_t *), (uint64)0), 0, 32, 2, t8, (uint64)0))
      let rows.s0.y.loop_max = ((output.s0.y.y*4) + 3)
      let rows.s0.y.loop_min = (output.s0.y.y*4)
      let rows.s0.y.loop_extent = 4
      let rows.s0.x.loop_max = ((((output.extent.0 + 63)/64)*64) + 2)
      let rows.s0.x.loop_min = -3
      let rows.s0.x.loop_extent = ((((output.extent.0 + 63)/64)*64) + 6)
      let rows.s0.x.x.loop_extent = ((output.extent.0 + 127)/64)
      let rows.s0.x.x.loop_max = ((output.extent.0 + 63)/64)
      let rows.s0.x.x.loop_min = 0
      let rows.s0.x.xi.loop_extent = 64
      let rows.s0.x.xi.loop_max = 63
      let rows.s0.x.xi.loop_min = 0
      let rows.s0.y.y.loop_extent = 1
      let rows.s0.y.y.loop_max = 0
      let rows.s0.y.y.loop_min = 0
      let rows.s0.y.yi.loop_extent = 4
      let rows.s0.y.yi.loop_max = 3
      let rows.s0.y.yi.loop_min = 0
      let rows.s0.y.y = 0
      let rows.s0.y.yi.base = (output.s0.y.y*4)
      produce rows {
        for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
          let rows.s0.x.xi.base = ((rows.s0.x.x*64) + -3)
          unrolled (rows.s0.y.yi, 0, 4) {
            let rows.s0.y = ((output.s0.y.y*4) + rows.s0.y.yi)
            vectorized (rows.s0.x.xi, 0, 64) {
              let rows.s0.x = (((rows.s0.x.x*64) + rows.s0.x.xi) + -3)
              rows[(((0 + ((((rows.s0.x.x*64) + rows.s0.x.xi) - rows.min.0)*rows.stride.0)) + ((((output.s0.y.y*4) + rows.s0.y.yi) - rows.min.1)*rows.stride.1)) + (0 + (rows.stride.0*-3)))] = (let t4 = (((rows.s0.x.x*64) + rows.s0.x.xi) + -3) in (let t5 = ((output.s0.y.y*4) + rows.s0.y.yi) in (((int32(input[((((0 + (((rows.s0.x.x*64) + rows.s0.x.xi)*input.stride.0)) + (((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1)) - ((0 + (input.min.0*input.stride.0)) + (input.min.1*input.stride.1))) + ((0 + (input.stride.0*-3)) + (input.stride.1*2)))])*6) + ((int32(input[((((0 + (((rows.s0.x.x*64) + rows.s0.x.xi)*input.stride.0)) + (((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1)) - ((0 + (input.min.0*input.stride.0)) + (input.min.1*input.stride.1))) + ((0 + (input.stride.0*-3)) + (input.stride.1*1)))])*15) + ((int32(input[((((0 + (((rows.s0.x.x*64) + rows.s0.x.xi)*input.stride.0)) + (((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1)) - ((0 + (input.min.0*input.stride.0)) + (input.min.1*input.stride.1))) + (0 + (input.stride.0*-3)))])*20) + ((int32(input[((((0 + (((rows.s0.x.x*64) + rows.s0.x.xi)*input.stride.0)) + (((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1)) - ((0 + (input.min.0*input.stride.0)) + (input.min.1*input.stride.1))) + ((0 + (input.stride.0*-3)) + (input.stride.1*-1)))])*15) + ((int32(input[((((0 + (((rows.s0.x.x*64) + rows.s0.x.xi)*input.stride.0)) + (((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1)) - ((0 + (input.min.0*input.stride.0)) + (input.min.1*input.stride.1))) + ((0 + (input.stride.0*-3)) + (input.stride.1*-2)))])*6) + int32(input[((((0 + (((rows.s0.x.x*64) + rows.s0.x.xi)*input.stride.0)) + (((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1)) - ((0 + (input.min.0*input.stride.0)) + (input.min.1*input.stride.1))) + ((0 + (input.stride.0*-3)) + (input.stride.1*-3)))])))))) + int32(input[((((0 + (((rows.s0.x.x*64) + rows.s0.x.xi)*input.stride.0)) + (((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1)) - ((0 + (input.min.0*input.stride.0)) + (input.min.1*input.stride.1))) + ((0 + (input.stride.0*-3)) + (input.stride.1*3)))]))))
            }
          }
        }
      }
      consume rows {
        for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
          let output.s0.x.xi.base = (output.s0.x.x*64)
          unrolled (output.s0.y.yi, 0, 4) {
            let output.s0.y = ((output.s0.y.y*4) + output.s0.y.yi)
            vectorized (output.s0.x.xi, 0, 64) {
              let output.s0.x = ((output.s0.x.x*64) + output.s0.x.xi)
              output[(((0 + (((output.s0.x.x*64) + output.s0.x.xi)*output.stride.0)) + (((output.s0.y.y*4) + output.s0.y.yi)*output.stride.1)) - ((0 + (output.min.0*output.stride.0)) + (output.min.1*output.stride.1)))] = (let t0 = ((output.s0.x.x*64) + output.s0.x.xi) in (let t1 = ((output.s0.y.y*4) + output.s0.y.yi) in uint8(max(min(((rows[(((0 + ((((output.s0.x.x*64) + output.s0.x.xi) - rows.min.0)*rows.stride.0)) + ((((output.s0.y.y*4) + output.s0.y.yi) - rows.min.1)*rows.stride.1)) + (0 + (rows.stride.0*3)))] + ((rows[(((0 + ((((output.s0.x.x*64) + output.s0.x.xi) - rows.min.0)*rows.stride.0)) + ((((output.s0.y.y*4) + output.s0.y.yi) - rows.min.1)*rows.stride.1)) + (0 + (rows.stride.0*2)))]*6) + ((rows[(((0 + ((((output.s0.x.x*64) + output.s0.x.xi) - rows.min.0)*rows.stride.0)) + ((((output.s0.y.y*4) + output.s0.y.yi) - rows.min.1)*rows.stride.1)) + (0 + (rows.stride.0*1)))]*15) + ((rows[((0 + ((((output.s0.x.x*64) + output.s0.x.xi) - rows.min.0)*rows.stride.0)) + ((((output.s0.y.y*4) + output.s0.y.yi) - rows.min.1)*rows.stride.1))]*20) + ((rows[(((0 + ((((output.s0.x.x*64) + output.s0.x.xi) - rows.min.0)*rows.stride.0)) + ((((output.s0.y.y*4) + output.s0.y.yi) - rows.min.1)*rows.stride.1)) + (0 + (rows.stride.0*-1)))]*15) + (rows[(((0 + ((((output.s0.x.x*64) + output.s0.x.xi) - rows.min.0)*rows.stride.0)) + ((((output.s0.y.y*4) + output.s0.y.yi) - rows.min.1)*rows.stride.1)) + (0 + (rows.stride.0*-3)))] + (rows[(((0 + ((((output.s0.x.x*64) + output.s0.x.xi) - rows.min.0)*rows.stride.0)) + ((((output.s0.y.y*4) + output.s0.y.yi) - rows.min.1)*rows.stride.1)) + (0 + (rows.stride.0*-2)))]*6)))))))/4096), 255), 0))))
            }
          }
        }
      }
    }
  }
}


Unpacking buffer arguments...
Lowering after unpacking buffer arguments...
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input"))
let input = _halide_buffer_get_host(input.buffer)
let input.min.0 = _halide_buffer_get_min(input.buffer, 0)
let input.stride.0 = _halide_buffer_get_stride(input.buffer, 0)
let input.min.1 = _halide_buffer_get_min(input.buffer, 1)
let input.stride.1 = _halide_buffer_get_stride(input.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
let output.s0.y.max = (output.extent.1 + -1)
let output.s0.y.min = 0
let output.s0.x.max = (output.extent.0 + -1)
let output.s0.x.min = 0
let rows.s0.y.max = (output.extent.1 + -1)
let rows.s0.y.min = 0
let rows.s0.x.max = (output.extent.0 + 2)
let rows.s0.x.min = -3
let input.extent.0.required = ((((output.extent.0 + 5)/64)*64) + 64)
let input.min.0.required = -3
let input.stride.0.required = 1
let input.extent.1.required = ((((output.extent.1 + -1)/4)*4) + 10)
let input.min.1.required = -3
let input.stride.1.required = ((((output.extent.0 + 5)/64)*64) + 64)
let output.extent.0.required = ((((output.extent.0 + -1)/64)*64) + 64)
let output.min.0.required = 0
let output.stride.0.required = 1
let output.extent.1.required = ((((output.extent.1 + -1)/4)*4) + 4)
let output.min.1.required = 0
let output.stride.1.required = ((((output.extent.0 + -1)/64)*64) + 64)
let input.stride.0.constrained = 1
let input.min.0.constrained = 0
let input.stride.1.constrained = ((input.stride.1/64)*64)
let input.min.1.constrained = 0
let output.stride.0.constrained = 1
let output.min.0.constrained = 0
let output.stride.1.constrained = ((output.stride.1/64)*64)
let output.min.1.constrained = 0
let input.stride.0.proposed = 1
let input.min.0.proposed = 0
let input.extent.0.proposed = ((((output.extent.0 + 5)/64)*64) + 64)
let input.stride.1.proposed = (((output.extent.0 + 69)/64)*64)
let input.min.1.proposed = 0
let input.extent.1.proposed = ((((output.extent.1 + -1)/4)*4) + 10)
let output.stride.0.proposed = 1
let output.min.0.proposed = 0
let output.extent.0.proposed = ((((output.extent.0 + -1)/64)*64) + 64)
let output.stride.1.proposed = (((output.extent.0 + 63)/64)*64)
let output.min.1.proposed = 0
let output.extent.1.proposed = ((((output.extent.1 + -1)/4)*4) + 4)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
let output.s0.y.loop_max = (output.extent.1 + -1)
let output.s0.y.loop_min = 0
let output.s0.y.loop_extent = output.extent.1
let output.s0.x.loop_max = (output.extent.0 + -1)
let output.s0.x.loop_min = 0
let output.s0.x.loop_extent = output.extent.0
let output.s0.x.x.loop_extent = ((output.extent.0 + 63)/64)
let output.s0.x.x.loop_max = ((output.extent.0 + -1)/64)
let output.s0.x.x.loop_min = 0
let output.s0.x.xi.loop_extent = 64
let output.s0.x.xi.loop_max = 63
let output.s0.x.xi.loop_min = 0
let output.s0.y.y.loop_extent = ((output.extent.1 + 3)/4)
let output.s0.y.y.loop_max = ((output.extent.1 + -1)/4)
let output.s0.y.y.loop_min = 0
let output.s0.y.yi.loop_extent = 4
let output.s0.y.yi.loop_max = 3
let output.s0.y.yi.loop_min = 0
produce output {
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      let output.s0.y.min_1 = (output.s0.y.y*4)
      let output.s0.y.max_1 = ((output.s0.y.y*4) + 3)
      let output.s0.x.min_1 = 0
      let output.s0.x.max_1 = ((((output.extent.0 + 63)/64)*64) + -1)
      let rows.s0.y.max_1 = ((output.s0.y.y*4) + 3)
      let rows.s0.y.min_1 = (output.s0.y.y*4)
      let rows.s0.x.max_1 = ((((output.extent.0 + 63)/64)*64) + 2)
      let rows.s0.x.min_1 = -3
      prefetch(input, ((((0 + (-3*input.stride.0)) + ((output.s0.y.y*4)*input.stride.1)) - ((0 + (input.min.0*input.stride.0)) + (input.min.1*input.stride.1))) + (0 + (input.stride.1*5))), ((((output.extent.0 + 63)/64)*64) + 64), input.stride.0, 10, input.stride.1)
      let output.s0.y.yi.base = (output.s0.y.y*4)
      let rows.y.max_realized = ((output.s0.y.y*4) + 3)
      let rows.y.min_realized = (output.s0.y.y*4)
      let rows.y.extent_realized = 4
      let rows.x.max_realized = ((((output.extent.0 + 63)/64)*64) + 60)
      let rows.x.min_realized = -3
      let rows.x.extent_realized = ((((output.extent.0 + 63)/64)*64) + 64)
      let rows.extent.0 = ((((output.extent.0 + 63)/64)*64) + 64)
      let rows.min.0 = -3
      let rows.extent.1 = 4
      let rows.min.1 = (output.s0.y.y*4)
      let rows.stride.0 = 1
      let rows.stride.1 = (rows.stride.0*((((output.extent.0 + 63)/64)*64) + 64))
      allocate rows[int32 * ((((output.extent.0 + 63)/64)*64) + 64) * 4]
      let rows.buffer = (let t8 = make_struct((halide_dimension_t *), rows.min.0, rows.extent.0, rows.stride.0, 0, rows.min.1, rows.extent.1, rows.stride.1, 0) in _halide_buffer_init(alloca(size_of_halide_buffer_t()), t8, rows, (uint64)0, reinterpret((halide_device_interface_t *), (uint64)0), 0, 32, 2, t8, (uint64)0))
      let rows.s0.y.loop_max = ((output.s0.y.y*4) + 3)
      let rows.s0.y.loop_min = (output.s0.y.y*4)
      let rows.s0.y.loop_extent = 4
      let rows.s0.x.loop_max = ((((output.extent.0 + 63)/64)*64) + 2)
      let rows.s0.x.loop_min = -3
      let rows.s0.x.loop_extent = ((((output.extent.0 + 63)/64)*64) + 6)
      let rows.s0.x.x.loop_extent = ((output.extent.0 + 127)/64)
      let rows.s0.x.x.loop_max = ((output.extent.0 + 63)/64)
      let rows.s0.x.x.loop_min = 0
      let rows.s0.x.xi.loop_extent = 64
      let rows.s0.x.xi.loop_max = 63
      let rows.s0.x.xi.loop_min = 0
      let rows.s0.y.y.loop_extent = 1
      let rows.s0.y.y.loop_max = 0
      let rows.s0.y.y.loop_min = 0
      let rows.s0.y.yi.loop_extent = 4
      let rows.s0.y.yi.loop_max = 3
      let rows.s0.y.yi.loop_min = 0
      let rows.s0.y.y = 0
      let rows.s0.y.yi.base = (output.s0.y.y*4)
      produce rows {
        for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
          let rows.s0.x.xi.base = ((rows.s0.x.x*64) + -3)
          unrolled (rows.s0.y.yi, 0, 4) {
            let rows.s0.y = ((output.s0.y.y*4) + rows.s0.y.yi)
            vectorized (rows.s0.x.xi, 0, 64) {
              let rows.s0.x = (((rows.s0.x.x*64) + rows.s0.x.xi) + -3)
              rows[(((0 + ((((rows.s0.x.x*64) + rows.s0.x.xi) - rows.min.0)*rows.stride.0)) + ((((output.s0.y.y*4) + rows.s0.y.yi) - rows.min.1)*rows.stride.1)) + (0 + (rows.stride.0*-3)))] = (let t4 = (((rows.s0.x.x*64) + rows.s0.x.xi) + -3) in (let t5 = ((output.s0.y.y*4) + rows.s0.y.yi) in (((int32(input[((((0 + (((rows.s0.x.x*64) + rows.s0.x.xi)*input.stride.0)) + (((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1)) - ((0 + (input.min.0*input.stride.0)) + (input.min.1*input.stride.1))) + ((0 + (input.stride.0*-3)) + (input.stride.1*2)))])*6) + ((int32(input[((((0 + (((rows.s0.x.x*64) + rows.s0.x.xi)*input.stride.0)) + (((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1)) - ((0 + (input.min.0*input.stride.0)) + (input.min.1*input.stride.1))) + ((0 + (input.stride.0*-3)) + (input.stride.1*1)))])*15) + ((int32(input[((((0 + (((rows.s0.x.x*64) + rows.s0.x.xi)*input.stride.0)) + (((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1)) - ((0 + (input.min.0*input.stride.0)) + (input.min.1*input.stride.1))) + (0 + (input.stride.0*-3)))])*20) + ((int32(input[((((0 + (((rows.s0.x.x*64) + rows.s0.x.xi)*input.stride.0)) + (((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1)) - ((0 + (input.min.0*input.stride.0)) + (input.min.1*input.stride.1))) + ((0 + (input.stride.0*-3)) + (input.stride.1*-1)))])*15) + ((int32(input[((((0 + (((rows.s0.x.x*64) + rows.s0.x.xi)*input.stride.0)) + (((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1)) - ((0 + (input.min.0*input.stride.0)) + (input.min.1*input.stride.1))) + ((0 + (input.stride.0*-3)) + (input.stride.1*-2)))])*6) + int32(input[((((0 + (((rows.s0.x.x*64) + rows.s0.x.xi)*input.stride.0)) + (((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1)) - ((0 + (input.min.0*input.stride.0)) + (input.min.1*input.stride.1))) + ((0 + (input.stride.0*-3)) + (input.stride.1*-3)))])))))) + int32(input[((((0 + (((rows.s0.x.x*64) + rows.s0.x.xi)*input.stride.0)) + (((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1)) - ((0 + (input.min.0*input.stride.0)) + (input.min.1*input.stride.1))) + ((0 + (input.stride.0*-3)) + (input.stride.1*3)))]))))
            }
          }
        }
      }
      consume rows {
        for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
          let output.s0.x.xi.base = (output.s0.x.x*64)
          unrolled (output.s0.y.yi, 0, 4) {
            let output.s0.y = ((output.s0.y.y*4) + output.s0.y.yi)
            vectorized (output.s0.x.xi, 0, 64) {
              let output.s0.x = ((output.s0.x.x*64) + output.s0.x.xi)
              output[(((0 + (((output.s0.x.x*64) + output.s0.x.xi)*output.stride.0)) + (((output.s0.y.y*4) + output.s0.y.yi)*output.stride.1)) - ((0 + (output.min.0*output.stride.0)) + (output.min.1*output.stride.1)))] = (let t0 = ((output.s0.x.x*64) + output.s0.x.xi) in (let t1 = ((output.s0.y.y*4) + output.s0.y.yi) in uint8(max(min(((rows[(((0 + ((((output.s0.x.x*64) + output.s0.x.xi) - rows.min.0)*rows.stride.0)) + ((((output.s0.y.y*4) + output.s0.y.yi) - rows.min.1)*rows.stride.1)) + (0 + (rows.stride.0*3)))] + ((rows[(((0 + ((((output.s0.x.x*64) + output.s0.x.xi) - rows.min.0)*rows.stride.0)) + ((((output.s0.y.y*4) + output.s0.y.yi) - rows.min.1)*rows.stride.1)) + (0 + (rows.stride.0*2)))]*6) + ((rows[(((0 + ((((output.s0.x.x*64) + output.s0.x.xi) - rows.min.0)*rows.stride.0)) + ((((output.s0.y.y*4) + output.s0.y.yi) - rows.min.1)*rows.stride.1)) + (0 + (rows.stride.0*1)))]*15) + ((rows[((0 + ((((output.s0.x.x*64) + output.s0.x.xi) - rows.min.0)*rows.stride.0)) + ((((output.s0.y.y*4) + output.s0.y.yi) - rows.min.1)*rows.stride.1))]*20) + ((rows[(((0 + ((((output.s0.x.x*64) + output.s0.x.xi) - rows.min.0)*rows.stride.0)) + ((((output.s0.y.y*4) + output.s0.y.yi) - rows.min.1)*rows.stride.1)) + (0 + (rows.stride.0*-1)))]*15) + (rows[(((0 + ((((output.s0.x.x*64) + output.s0.x.xi) - rows.min.0)*rows.stride.0)) + ((((output.s0.y.y*4) + output.s0.y.yi) - rows.min.1)*rows.stride.1)) + (0 + (rows.stride.0*-3)))] + (rows[(((0 + ((((output.s0.x.x*64) + output.s0.x.xi) - rows.min.0)*rows.stride.0)) + ((((output.s0.y.y*4) + output.s0.y.yi) - rows.min.1)*rows.stride.1)) + (0 + (rows.stride.0*-2)))]*6)))))))/4096), 255), 0))))
            }
          }
        }
      }
    }
  }
}


Skipping rewriting memoized allocations...
Simplifying...
Lowering after second simplifcation:
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input"))
let input = _halide_buffer_get_host(input.buffer)
let input.min.0 = _halide_buffer_get_min(input.buffer, 0)
let input.stride.0 = _halide_buffer_get_stride(input.buffer, 0)
let input.min.1 = _halide_buffer_get_min(input.buffer, 1)
let input.stride.1 = _halide_buffer_get_stride(input.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      prefetch(input, (((input.stride.1*5) + ((input.stride.1*output.s0.y.y)*4)) + -3), ((((output.extent.0 + 63)/64)*64) + 64), 1, 10, input.stride.1)
      allocate rows[int32 * ((((output.extent.0 + 63)/64)*64) + 64) * 4]
      produce rows {
        for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
          unrolled (rows.s0.y.yi, 0, 4) {
            vectorized (rows.s0.x.xi, 0, 64) {
              rows[((((((output.extent.0 + 63)/64)*64) + 64)*rows.s0.y.yi) + ((rows.s0.x.x*64) + rows.s0.x.xi))] = (((int32(input[(((input.stride.1*2) + ((((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi))) + -3)])*6) + ((int32(input[((((((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) + input.stride.1) + -3)])*15) + ((int32(input[(((((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) + -3)])*20) + ((int32(input[((((((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) - input.stride.1) + -3)])*15) + ((int32(input[((((((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) - (input.stride.1*2)) + -3)])*6) + int32(input[((((((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) - (input.stride.1*3)) + -3)])))))) + int32(input[(((input.stride.1*3) + ((((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi))) + -3)]))
            }
          }
        }
      }
      consume rows {
        for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
          unrolled (output.s0.y.yi, 0, 4) {
            vectorized (output.s0.x.xi, 0, 64) {
              output[((((output.s0.y.y*4) + output.s0.y.yi)*output.stride.1) + ((output.s0.x.x*64) + output.s0.x.xi))] = uint8(max(min(((rows[(((((((output.extent.0 + 63)/64)*64) + 64)*output.s0.y.yi) + ((output.s0.x.x*64) + output.s0.x.xi)) + 6)] + ((rows[(((((((output.extent.0 + 63)/64)*64) + 64)*output.s0.y.yi) + ((output.s0.x.x*64) + output.s0.x.xi)) + 5)]*6) + ((rows[(((((((output.extent.0 + 63)/64)*64) + 64)*output.s0.y.yi) + ((output.s0.x.x*64) + output.s0.x.xi)) + 4)]*15) + ((rows[(((((((output.extent.0 + 63)/64)*64) + 64)*output.s0.y.yi) + ((output.s0.x.x*64) + output.s0.x.xi)) + 3)]*20) + ((rows[(((((((output.extent.0 + 63)/64)*64) + 64)*output.s0.y.yi) + ((output.s0.x.x*64) + output.s0.x.xi)) + 2)]*15) + (rows[((((((output.extent.0 + 63)/64)*64) + 64)*output.s0.y.yi) + ((output.s0.x.x*64) + output.s0.x.xi))] + (rows[(((((((output.extent.0 + 63)/64)*64) + 64)*output.s0.y.yi) + ((output.s0.x.x*64) + output.s0.x.xi)) + 1)]*6)))))))/4096), 255), 0))
            }
          }
        }
      }
    }
  }
}


Reduce prefetch dimension...
Lowering after reduce prefetch dimension:
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input"))
let input = _halide_buffer_get_host(input.buffer)
let input.min.0 = _halide_buffer_get_min(input.buffer, 0)
let input.stride.0 = _halide_buffer_get_stride(input.buffer, 0)
let input.min.1 = _halide_buffer_get_min(input.buffer, 1)
let input.stride.1 = _halide_buffer_get_stride(input.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      prefetch(input, (((input.stride.1*5) + ((input.stride.1*output.s0.y.y)*4)) + -3), ((((output.extent.0 + 63)/64)*64) + 64), 1, 10, input.stride.1)
      allocate rows[int32 * ((((output.extent.0 + 63)/64)*64) + 64) * 4]
      produce rows {
        for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
          unrolled (rows.s0.y.yi, 0, 4) {
            vectorized (rows.s0.x.xi, 0, 64) {
              rows[((((((output.extent.0 + 63)/64)*64) + 64)*rows.s0.y.yi) + ((rows.s0.x.x*64) + rows.s0.x.xi))] = (((int32(input[(((input.stride.1*2) + ((((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi))) + -3)])*6) + ((int32(input[((((((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) + input.stride.1) + -3)])*15) + ((int32(input[(((((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) + -3)])*20) + ((int32(input[((((((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) - input.stride.1) + -3)])*15) + ((int32(input[((((((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) - (input.stride.1*2)) + -3)])*6) + int32(input[((((((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) - (input.stride.1*3)) + -3)])))))) + int32(input[(((input.stride.1*3) + ((((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi))) + -3)]))
            }
          }
        }
      }
      consume rows {
        for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
          unrolled (output.s0.y.yi, 0, 4) {
            vectorized (output.s0.x.xi, 0, 64) {
              output[((((output.s0.y.y*4) + output.s0.y.yi)*output.stride.1) + ((output.s0.x.x*64) + output.s0.x.xi))] = uint8(max(min(((rows[(((((((output.extent.0 + 63)/64)*64) + 64)*output.s0.y.yi) + ((output.s0.x.x*64) + output.s0.x.xi)) + 6)] + ((rows[(((((((output.extent.0 + 63)/64)*64) + 64)*output.s0.y.yi) + ((output.s0.x.x*64) + output.s0.x.xi)) + 5)]*6) + ((rows[(((((((output.extent.0 + 63)/64)*64) + 64)*output.s0.y.yi) + ((output.s0.x.x*64) + output.s0.x.xi)) + 4)]*15) + ((rows[(((((((output.extent.0 + 63)/64)*64) + 64)*output.s0.y.yi) + ((output.s0.x.x*64) + output.s0.x.xi)) + 3)]*20) + ((rows[(((((((output.extent.0 + 63)/64)*64) + 64)*output.s0.y.yi) + ((output.s0.x.x*64) + output.s0.x.xi)) + 2)]*15) + (rows[((((((output.extent.0 + 63)/64)*64) + 64)*output.s0.y.yi) + ((output.s0.x.x*64) + output.s0.x.xi))] + (rows[(((((((output.extent.0 + 63)/64)*64) + 64)*output.s0.y.yi) + ((output.s0.x.x*64) + output.s0.x.xi)) + 1)]*6)))))))/4096), 255), 0))
            }
          }
        }
      }
    }
  }
}

Simplifying correlated differences...
Lowering after simplifying correlated differences:
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input"))
let input = _halide_buffer_get_host(input.buffer)
let input.min.0 = _halide_buffer_get_min(input.buffer, 0)
let input.stride.0 = _halide_buffer_get_stride(input.buffer, 0)
let input.min.1 = _halide_buffer_get_min(input.buffer, 1)
let input.stride.1 = _halide_buffer_get_stride(input.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      prefetch(input, (((input.stride.1*5) + ((input.stride.1*output.s0.y.y)*4)) + -3), ((((output.extent.0 + 63)/64)*64) + 64), 1, 10, input.stride.1)
      allocate rows[int32 * ((((output.extent.0 + 63)/64)*64) + 64) * 4]
      produce rows {
        for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
          unrolled (rows.s0.y.yi, 0, 4) {
            vectorized (rows.s0.x.xi, 0, 64) {
              rows[((((((output.extent.0 + 63)/64)*64) + 64)*rows.s0.y.yi) + ((rows.s0.x.x*64) + rows.s0.x.xi))] = (((int32(input[(((input.stride.1*2) + ((((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi))) + -3)])*6) + ((int32(input[((((((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) + input.stride.1) + -3)])*15) + ((int32(input[(((((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) + -3)])*20) + ((int32(input[((((((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) - input.stride.1) + -3)])*15) + ((int32(input[((((((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) - (input.stride.1*2)) + -3)])*6) + int32(input[((((((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) - (input.stride.1*3)) + -3)])))))) + int32(input[(((input.stride.1*3) + ((((output.s0.y.y*4) + rows.s0.y.yi)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi))) + -3)]))
            }
          }
        }
      }
      consume rows {
        for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
          unrolled (output.s0.y.yi, 0, 4) {
            vectorized (output.s0.x.xi, 0, 64) {
              output[((((output.s0.y.y*4) + output.s0.y.yi)*output.stride.1) + ((output.s0.x.x*64) + output.s0.x.xi))] = uint8(max(min(((rows[(((((((output.extent.0 + 63)/64)*64) + 64)*output.s0.y.yi) + ((output.s0.x.x*64) + output.s0.x.xi)) + 6)] + ((rows[(((((((output.extent.0 + 63)/64)*64) + 64)*output.s0.y.yi) + ((output.s0.x.x*64) + output.s0.x.xi)) + 5)]*6) + ((rows[(((((((output.extent.0 + 63)/64)*64) + 64)*output.s0.y.yi) + ((output.s0.x.x*64) + output.s0.x.xi)) + 4)]*15) + ((rows[(((((((output.extent.0 + 63)/64)*64) + 64)*output.s0.y.yi) + ((output.s0.x.x*64) + output.s0.x.xi)) + 3)]*20) + ((rows[(((((((output.extent.0 + 63)/64)*64) + 64)*output.s0.y.yi) + ((output.s0.x.x*64) + output.s0.x.xi)) + 2)]*15) + (rows[((((((output.extent.0 + 63)/64)*64) + 64)*output.s0.y.yi) + ((output.s0.x.x*64) + output.s0.x.xi))] + (rows[(((((((output.extent.0 + 63)/64)*64) + 64)*output.s0.y.yi) + ((output.s0.x.x*64) + output.s0.x.xi)) + 1)]*6)))))))/4096), 255), 0))
            }
          }
        }
      }
    }
  }
}

Unrolling...
Lowering after unrolling:
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input"))
let input = _halide_buffer_get_host(input.buffer)
let input.min.0 = _halide_buffer_get_min(input.buffer, 0)
let input.stride.0 = _halide_buffer_get_stride(input.buffer, 0)
let input.min.1 = _halide_buffer_get_min(input.buffer, 1)
let input.stride.1 = _halide_buffer_get_stride(input.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      prefetch(input, (((input.stride.1*5) + ((input.stride.1*output.s0.y.y)*4)) + -3), ((((output.extent.0 + 63)/64)*64) + 64), 1, 10, input.stride.1)
      allocate rows[int32 * ((((output.extent.0 + 63)/64)*64) + 64) * 4]
      produce rows {
        for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
          vectorized (rows.s0.x.xi, 0, 64) {
            rows[((rows.s0.x.x*64) + rows.s0.x.xi)] = (((int32(input[(((input.stride.1*2) + (((input.stride.1*output.s0.y.y)*4) + ((rows.s0.x.x*64) + rows.s0.x.xi))) + -3)])*6) + ((int32(input[(((((input.stride.1*output.s0.y.y)*4) + ((rows.s0.x.x*64) + rows.s0.x.xi)) + input.stride.1) + -3)])*15) + ((int32(input[((((input.stride.1*output.s0.y.y)*4) + ((rows.s0.x.x*64) + rows.s0.x.xi)) + -3)])*20) + ((int32(input[(((((input.stride.1*output.s0.y.y)*4) + ((rows.s0.x.x*64) + rows.s0.x.xi)) - input.stride.1) + -3)])*15) + ((int32(input[(((((input.stride.1*output.s0.y.y)*4) + ((rows.s0.x.x*64) + rows.s0.x.xi)) - (input.stride.1*2)) + -3)])*6) + int32(input[(((((input.stride.1*output.s0.y.y)*4) + ((rows.s0.x.x*64) + rows.s0.x.xi)) - (input.stride.1*3)) + -3)])))))) + int32(input[(((input.stride.1*3) + (((input.stride.1*output.s0.y.y)*4) + ((rows.s0.x.x*64) + rows.s0.x.xi))) + -3)]))
          }
          vectorized (rows.s0.x.xi, 0, 64) {
            rows[(((((output.extent.0 + 63)/64)*64) + ((rows.s0.x.x*64) + rows.s0.x.xi)) + 64)] = (((int32(input[(((input.stride.1*2) + ((((output.s0.y.y*4) + 1)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi))) + -3)])*6) + ((int32(input[((((((output.s0.y.y*4) + 1)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) + input.stride.1) + -3)])*15) + ((int32(input[(((((output.s0.y.y*4) + 1)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) + -3)])*20) + ((int32(input[((((((output.s0.y.y*4) + 1)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) - input.stride.1) + -3)])*15) + ((int32(input[((((((output.s0.y.y*4) + 1)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) - (input.stride.1*2)) + -3)])*6) + int32(input[((((((output.s0.y.y*4) + 1)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) - (input.stride.1*3)) + -3)])))))) + int32(input[(((input.stride.1*3) + ((((output.s0.y.y*4) + 1)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi))) + -3)]))
          }
          vectorized (rows.s0.x.xi, 0, 64) {
            rows[(((((output.extent.0 + 63)/64)*128) + ((rows.s0.x.x*64) + rows.s0.x.xi)) + 128)] = (((int32(input[(((input.stride.1*2) + ((((output.s0.y.y*4) + 2)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi))) + -3)])*6) + ((int32(input[((((((output.s0.y.y*4) + 2)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) + input.stride.1) + -3)])*15) + ((int32(input[(((((output.s0.y.y*4) + 2)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) + -3)])*20) + ((int32(input[((((((output.s0.y.y*4) + 2)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) - input.stride.1) + -3)])*15) + ((int32(input[((((((output.s0.y.y*4) + 2)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) - (input.stride.1*2)) + -3)])*6) + int32(input[((((((output.s0.y.y*4) + 2)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) - (input.stride.1*3)) + -3)])))))) + int32(input[(((input.stride.1*3) + ((((output.s0.y.y*4) + 2)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi))) + -3)]))
          }
          vectorized (rows.s0.x.xi, 0, 64) {
            rows[(((((output.extent.0 + 63)/64)*192) + ((rows.s0.x.x*64) + rows.s0.x.xi)) + 192)] = (((int32(input[(((input.stride.1*2) + ((((output.s0.y.y*4) + 3)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi))) + -3)])*6) + ((int32(input[((((((output.s0.y.y*4) + 3)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) + input.stride.1) + -3)])*15) + ((int32(input[(((((output.s0.y.y*4) + 3)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) + -3)])*20) + ((int32(input[((((((output.s0.y.y*4) + 3)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) - input.stride.1) + -3)])*15) + ((int32(input[((((((output.s0.y.y*4) + 3)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) - (input.stride.1*2)) + -3)])*6) + int32(input[((((((output.s0.y.y*4) + 3)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi)) - (input.stride.1*3)) + -3)])))))) + int32(input[(((input.stride.1*3) + ((((output.s0.y.y*4) + 3)*input.stride.1) + ((rows.s0.x.x*64) + rows.s0.x.xi))) + -3)]))
          }
        }
      }
      consume rows {
        for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
          vectorized (output.s0.x.xi, 0, 64) {
            output[(((output.s0.y.y*output.stride.1)*4) + ((output.s0.x.x*64) + output.s0.x.xi))] = uint8(max(min(((rows[(((output.s0.x.x*64) + output.s0.x.xi) + 6)] + ((rows[(((output.s0.x.x*64) + output.s0.x.xi) + 5)]*6) + ((rows[(((output.s0.x.x*64) + output.s0.x.xi) + 4)]*15) + ((rows[(((output.s0.x.x*64) + output.s0.x.xi) + 3)]*20) + ((rows[(((output.s0.x.x*64) + output.s0.x.xi) + 2)]*15) + (rows[((output.s0.x.x*64) + output.s0.x.xi)] + (rows[(((output.s0.x.x*64) + output.s0.x.xi) + 1)]*6)))))))/4096), 255), 0))
          }
          vectorized (output.s0.x.xi, 0, 64) {
            output[((((output.s0.y.y*4) + 1)*output.stride.1) + ((output.s0.x.x*64) + output.s0.x.xi))] = uint8(max(min(((rows[(((((output.extent.0 + 63)/64)*64) + ((output.s0.x.x*64) + output.s0.x.xi)) + 70)] + ((rows[(((((output.extent.0 + 63)/64)*64) + ((output.s0.x.x*64) + output.s0.x.xi)) + 69)]*6) + ((rows[(((((output.extent.0 + 63)/64)*64) + ((output.s0.x.x*64) + output.s0.x.xi)) + 68)]*15) + ((rows[(((((output.extent.0 + 63)/64)*64) + ((output.s0.x.x*64) + output.s0.x.xi)) + 67)]*20) + ((rows[(((((output.extent.0 + 63)/64)*64) + ((output.s0.x.x*64) + output.s0.x.xi)) + 66)]*15) + (rows[(((((output.extent.0 + 63)/64)*64) + ((output.s0.x.x*64) + output.s0.x.xi)) + 64)] + (rows[(((((output.extent.0 + 63)/64)*64) + ((output.s0.x.x*64) + output.s0.x.xi)) + 65)]*6)))))))/4096), 255), 0))
          }
          vectorized (output.s0.x.xi, 0, 64) {
            output[((((output.s0.y.y*4) + 2)*output.stride.1) + ((output.s0.x.x*64) + output.s0.x.xi))] = uint8(max(min(((rows[(((((output.extent.0 + 63)/64)*128) + ((output.s0.x.x*64) + output.s0.x.xi)) + 134)] + ((rows[(((((output.extent.0 + 63)/64)*128) + ((output.s0.x.x*64) + output.s0.x.xi)) + 133)]*6) + ((rows[(((((output.extent.0 + 63)/64)*128) + ((output.s0.x.x*64) + output.s0.x.xi)) + 132)]*15) + ((rows[(((((output.extent.0 + 63)/64)*128) + ((output.s0.x.x*64) + output.s0.x.xi)) + 131)]*20) + ((rows[(((((output.extent.0 + 63)/64)*128) + ((output.s0.x.x*64) + output.s0.x.xi)) + 130)]*15) + (rows[(((((output.extent.0 + 63)/64)*128) + ((output.s0.x.x*64) + output.s0.x.xi)) + 128)] + (rows[(((((output.extent.0 + 63)/64)*128) + ((output.s0.x.x*64) + output.s0.x.xi)) + 129)]*6)))))))/4096), 255), 0))
          }
          vectorized (output.s0.x.xi, 0, 64) {
            output[((((output.s0.y.y*4) + 3)*output.stride.1) + ((output.s0.x.x*64) + output.s0.x.xi))] = uint8(max(min(((rows[(((((output.extent.0 + 63)/64)*192) + ((output.s0.x.x*64) + output.s0.x.xi)) + 198)] + ((rows[(((((output.extent.0 + 63)/64)*192) + ((output.s0.x.x*64) + output.s0.x.xi)) + 197)]*6) + ((rows[(((((output.extent.0 + 63)/64)*192) + ((output.s0.x.x*64) + output.s0.x.xi)) + 196)]*15) + ((rows[(((((output.extent.0 + 63)/64)*192) + ((output.s0.x.x*64) + output.s0.x.xi)) + 195)]*20) + ((rows[(((((output.extent.0 + 63)/64)*192) + ((output.s0.x.x*64) + output.s0.x.xi)) + 194)]*15) + (rows[(((((output.extent.0 + 63)/64)*192) + ((output.s0.x.x*64) + output.s0.x.xi)) + 192)] + (rows[(((((output.extent.0 + 63)/64)*192) + ((output.s0.x.x*64) + output.s0.x.xi)) + 193)]*6)))))))/4096), 255), 0))
          }
        }
      }
    }
  }
}


Vectorizing...
Lowering after vectorizing:
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input"))
let input = _halide_buffer_get_host(input.buffer)
let input.min.0 = _halide_buffer_get_min(input.buffer, 0)
let input.stride.0 = _halide_buffer_get_stride(input.buffer, 0)
let input.min.1 = _halide_buffer_get_min(input.buffer, 1)
let input.stride.1 = _halide_buffer_get_stride(input.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      prefetch(input, (((input.stride.1*5) + ((input.stride.1*output.s0.y.y)*4)) + -3), ((((output.extent.0 + 63)/64)*64) + 64), 1, 10, input.stride.1)
      allocate rows[int32 * ((((output.extent.0 + 63)/64)*64) + 64) * 4]
      produce rows {
        for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
          rows[ramp((rows.s0.x.x*64), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp(((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*2) + input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*2) - input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) + (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)]))
          rows[ramp((((((output.extent.0 + 63)/64) + rows.s0.x.x)*64) + 64), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp((((input.stride.1*2) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1))) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1))) + -3), 1, 64) aligned(64, 61)]))
          rows[ramp(((((((output.extent.0 + 63)/64)*2) + rows.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp((((input.stride.1*2) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1))) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1))) + -3), 1, 64) aligned(64, 61)]))
          rows[ramp(((((((output.extent.0 + 63)/64)*3) + rows.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp((((input.stride.1*2) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1))) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1))) + -3), 1, 64) aligned(64, 61)]))
        }
      }
      consume rows {
        for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
          output[ramp((((output.s0.x.x*16) + (output.s0.y.y*output.stride.1))*4), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp(((output.s0.x.x*64) + 6), 1, 64) aligned(64, 6)] + ((rows[ramp(((output.s0.x.x*64) + 5), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((output.s0.x.x*64) + 4), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((output.s0.x.x*64) + 3), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((output.s0.x.x*64) + 2), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp((output.s0.x.x*64), 1, 64) aligned(64, 0)] + (rows[ramp(((output.s0.x.x*64) + 1), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 1)*output.stride.1)), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 70), 1, 64) aligned(64, 6)] + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 69), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 68), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 67), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 66), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 64), 1, 64) aligned(64, 0)] + (rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 65), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 2)*output.stride.1)), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 134), 1, 64) aligned(64, 6)] + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 133), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 132), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 131), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 130), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] + (rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 129), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 3)*output.stride.1)), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 198), 1, 64) aligned(64, 6)] + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 197), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 196), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 195), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 194), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] + (rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 193), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
        }
      }
    }
  }
}


Detecting vector interleavings...
Lowering after rewriting vector interleavings:
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input"))
let input = _halide_buffer_get_host(input.buffer)
let input.min.0 = _halide_buffer_get_min(input.buffer, 0)
let input.stride.0 = _halide_buffer_get_stride(input.buffer, 0)
let input.min.1 = _halide_buffer_get_min(input.buffer, 1)
let input.stride.1 = _halide_buffer_get_stride(input.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      prefetch(input, (((input.stride.1*5) + ((input.stride.1*output.s0.y.y)*4)) + -3), ((((output.extent.0 + 63)/64)*64) + 64), 1, 10, input.stride.1)
      allocate rows[int32 * ((((output.extent.0 + 63)/64)*64) + 64) * 4]
      produce rows {
        for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
          rows[ramp((rows.s0.x.x*64), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp(((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*2) + input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*2) - input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) + (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)]))
          rows[ramp((((((output.extent.0 + 63)/64) + rows.s0.x.x)*64) + 64), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp((((input.stride.1*2) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1))) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1))) + -3), 1, 64) aligned(64, 61)]))
          rows[ramp(((((((output.extent.0 + 63)/64)*2) + rows.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp((((input.stride.1*2) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1))) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1))) + -3), 1, 64) aligned(64, 61)]))
          rows[ramp(((((((output.extent.0 + 63)/64)*3) + rows.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp((((input.stride.1*2) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1))) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1))) + -3), 1, 64) aligned(64, 61)]))
        }
      }
      consume rows {
        for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
          output[ramp((((output.s0.x.x*16) + (output.s0.y.y*output.stride.1))*4), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp(((output.s0.x.x*64) + 6), 1, 64) aligned(64, 6)] + ((rows[ramp(((output.s0.x.x*64) + 5), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((output.s0.x.x*64) + 4), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((output.s0.x.x*64) + 3), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((output.s0.x.x*64) + 2), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp((output.s0.x.x*64), 1, 64) aligned(64, 0)] + (rows[ramp(((output.s0.x.x*64) + 1), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 1)*output.stride.1)), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 70), 1, 64) aligned(64, 6)] + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 69), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 68), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 67), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 66), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 64), 1, 64) aligned(64, 0)] + (rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 65), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 2)*output.stride.1)), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 134), 1, 64) aligned(64, 6)] + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 133), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 132), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 131), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 130), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] + (rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 129), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 3)*output.stride.1)), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 198), 1, 64) aligned(64, 6)] + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 197), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 196), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 195), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 194), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] + (rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 193), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
        }
      }
    }
  }
}


Partitioning loops to simplify boundary conditions...
Lowering after partitioning loops:
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input"))
let input = _halide_buffer_get_host(input.buffer)
let input.min.0 = _halide_buffer_get_min(input.buffer, 0)
let input.stride.0 = _halide_buffer_get_stride(input.buffer, 0)
let input.min.1 = _halide_buffer_get_min(input.buffer, 1)
let input.stride.1 = _halide_buffer_get_stride(input.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      prefetch(input, (((input.stride.1*5) + ((input.stride.1*output.s0.y.y)*4)) + -3), ((((output.extent.0 + 63)/64)*64) + 64), 1, 10, input.stride.1)
      allocate rows[int32 * ((((output.extent.0 + 63)/64)*64) + 64) * 4]
      produce rows {
        for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
          rows[ramp((rows.s0.x.x*64), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp(((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*2) + input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*2) - input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) + (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)]))
          rows[ramp((((((output.extent.0 + 63)/64) + rows.s0.x.x)*64) + 64), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp((((input.stride.1*2) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1))) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1))) + -3), 1, 64) aligned(64, 61)]))
          rows[ramp(((((((output.extent.0 + 63)/64)*2) + rows.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp((((input.stride.1*2) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1))) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1))) + -3), 1, 64) aligned(64, 61)]))
          rows[ramp(((((((output.extent.0 + 63)/64)*3) + rows.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp((((input.stride.1*2) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1))) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1))) + -3), 1, 64) aligned(64, 61)]))
        }
      }
      consume rows {
        for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
          output[ramp((((output.s0.x.x*16) + (output.s0.y.y*output.stride.1))*4), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp(((output.s0.x.x*64) + 6), 1, 64) aligned(64, 6)] + ((rows[ramp(((output.s0.x.x*64) + 5), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((output.s0.x.x*64) + 4), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((output.s0.x.x*64) + 3), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((output.s0.x.x*64) + 2), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp((output.s0.x.x*64), 1, 64) aligned(64, 0)] + (rows[ramp(((output.s0.x.x*64) + 1), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 1)*output.stride.1)), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 70), 1, 64) aligned(64, 6)] + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 69), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 68), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 67), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 66), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 64), 1, 64) aligned(64, 0)] + (rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 65), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 2)*output.stride.1)), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 134), 1, 64) aligned(64, 6)] + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 133), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 132), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 131), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 130), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] + (rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 129), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 3)*output.stride.1)), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 198), 1, 64) aligned(64, 6)] + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 197), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 196), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 195), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 194), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] + (rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 193), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
        }
      }
    }
  }
}


Trimming loops to the region over which they do something...
Lowering after loop trimming:
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input"))
let input = _halide_buffer_get_host(input.buffer)
let input.min.0 = _halide_buffer_get_min(input.buffer, 0)
let input.stride.0 = _halide_buffer_get_stride(input.buffer, 0)
let input.min.1 = _halide_buffer_get_min(input.buffer, 1)
let input.stride.1 = _halide_buffer_get_stride(input.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      prefetch(input, (((input.stride.1*5) + ((input.stride.1*output.s0.y.y)*4)) + -3), ((((output.extent.0 + 63)/64)*64) + 64), 1, 10, input.stride.1)
      allocate rows[int32 * ((((output.extent.0 + 63)/64)*64) + 64) * 4]
      produce rows {
        for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
          rows[ramp((rows.s0.x.x*64), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp(((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*2) + input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*2) - input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) + (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)]))
          rows[ramp((((((output.extent.0 + 63)/64) + rows.s0.x.x)*64) + 64), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp((((input.stride.1*2) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1))) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1))) + -3), 1, 64) aligned(64, 61)]))
          rows[ramp(((((((output.extent.0 + 63)/64)*2) + rows.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp((((input.stride.1*2) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1))) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1))) + -3), 1, 64) aligned(64, 61)]))
          rows[ramp(((((((output.extent.0 + 63)/64)*3) + rows.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp((((input.stride.1*2) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1))) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1))) + -3), 1, 64) aligned(64, 61)]))
        }
      }
      consume rows {
        for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
          output[ramp((((output.s0.x.x*16) + (output.s0.y.y*output.stride.1))*4), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp(((output.s0.x.x*64) + 6), 1, 64) aligned(64, 6)] + ((rows[ramp(((output.s0.x.x*64) + 5), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((output.s0.x.x*64) + 4), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((output.s0.x.x*64) + 3), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((output.s0.x.x*64) + 2), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp((output.s0.x.x*64), 1, 64) aligned(64, 0)] + (rows[ramp(((output.s0.x.x*64) + 1), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 1)*output.stride.1)), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 70), 1, 64) aligned(64, 6)] + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 69), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 68), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 67), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 66), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 64), 1, 64) aligned(64, 0)] + (rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 65), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 2)*output.stride.1)), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 134), 1, 64) aligned(64, 6)] + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 133), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 132), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 131), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 130), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] + (rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 129), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 3)*output.stride.1)), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 198), 1, 64) aligned(64, 6)] + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 197), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 196), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 195), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 194), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] + (rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 193), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
        }
      }
    }
  }
}


Injecting early frees...
Lowering after injecting early frees:
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input"))
let input = _halide_buffer_get_host(input.buffer)
let input.min.0 = _halide_buffer_get_min(input.buffer, 0)
let input.stride.0 = _halide_buffer_get_stride(input.buffer, 0)
let input.min.1 = _halide_buffer_get_min(input.buffer, 1)
let input.stride.1 = _halide_buffer_get_stride(input.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      prefetch(input, (((input.stride.1*5) + ((input.stride.1*output.s0.y.y)*4)) + -3), ((((output.extent.0 + 63)/64)*64) + 64), 1, 10, input.stride.1)
      allocate rows[int32 * ((((output.extent.0 + 63)/64)*64) + 64) * 4]
      produce rows {
        for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
          rows[ramp((rows.s0.x.x*64), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp(((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*2) + input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*2) - input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) + (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)]))
          rows[ramp((((((output.extent.0 + 63)/64) + rows.s0.x.x)*64) + 64), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp((((input.stride.1*2) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1))) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1))) + -3), 1, 64) aligned(64, 61)]))
          rows[ramp(((((((output.extent.0 + 63)/64)*2) + rows.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp((((input.stride.1*2) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1))) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1))) + -3), 1, 64) aligned(64, 61)]))
          rows[ramp(((((((output.extent.0 + 63)/64)*3) + rows.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp((((input.stride.1*2) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1))) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1))) + -3), 1, 64) aligned(64, 61)]))
        }
      }
      consume rows {
        for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
          output[ramp((((output.s0.x.x*16) + (output.s0.y.y*output.stride.1))*4), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp(((output.s0.x.x*64) + 6), 1, 64) aligned(64, 6)] + ((rows[ramp(((output.s0.x.x*64) + 5), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((output.s0.x.x*64) + 4), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((output.s0.x.x*64) + 3), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((output.s0.x.x*64) + 2), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp((output.s0.x.x*64), 1, 64) aligned(64, 0)] + (rows[ramp(((output.s0.x.x*64) + 1), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 1)*output.stride.1)), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 70), 1, 64) aligned(64, 6)] + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 69), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 68), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 67), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 66), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 64), 1, 64) aligned(64, 0)] + (rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 65), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 2)*output.stride.1)), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 134), 1, 64) aligned(64, 6)] + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 133), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 132), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 131), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 130), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] + (rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 129), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 3)*output.stride.1)), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 198), 1, 64) aligned(64, 6)] + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 197), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 196), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 195), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 194), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] + (rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 193), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
        }
      }
      free rows
    }
  }
}


Simplifying correlated differences...
Lowering after simplifying correlated differences:
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input"))
let input = _halide_buffer_get_host(input.buffer)
let input.min.0 = _halide_buffer_get_min(input.buffer, 0)
let input.stride.0 = _halide_buffer_get_stride(input.buffer, 0)
let input.min.1 = _halide_buffer_get_min(input.buffer, 1)
let input.stride.1 = _halide_buffer_get_stride(input.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      prefetch(input, (((input.stride.1*5) + ((input.stride.1*output.s0.y.y)*4)) + -3), ((((output.extent.0 + 63)/64)*64) + 64), 1, 10, input.stride.1)
      allocate rows[int32 * ((((output.extent.0 + 63)/64)*64) + 64) * 4]
      produce rows {
        for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
          rows[ramp((rows.s0.x.x*64), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp(((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*2) + input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*2) - input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) + (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)]))
          rows[ramp((((((output.extent.0 + 63)/64) + rows.s0.x.x)*64) + 64), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp((((input.stride.1*2) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1))) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1))) + -3), 1, 64) aligned(64, 61)]))
          rows[ramp(((((((output.extent.0 + 63)/64)*2) + rows.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp((((input.stride.1*2) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1))) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1))) + -3), 1, 64) aligned(64, 61)]))
          rows[ramp(((((((output.extent.0 + 63)/64)*3) + rows.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp((((input.stride.1*2) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1))) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1))) + -3), 1, 64) aligned(64, 61)]))
        }
      }
      consume rows {
        for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
          output[ramp((((output.s0.x.x*16) + (output.s0.y.y*output.stride.1))*4), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp(((output.s0.x.x*64) + 6), 1, 64) aligned(64, 6)] + ((rows[ramp(((output.s0.x.x*64) + 5), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((output.s0.x.x*64) + 4), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((output.s0.x.x*64) + 3), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((output.s0.x.x*64) + 2), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp((output.s0.x.x*64), 1, 64) aligned(64, 0)] + (rows[ramp(((output.s0.x.x*64) + 1), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 1)*output.stride.1)), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 70), 1, 64) aligned(64, 6)] + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 69), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 68), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 67), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 66), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 64), 1, 64) aligned(64, 0)] + (rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 65), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 2)*output.stride.1)), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 134), 1, 64) aligned(64, 6)] + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 133), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 132), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 131), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 130), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] + (rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 129), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 3)*output.stride.1)), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 198), 1, 64) aligned(64, 6)] + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 197), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 196), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 195), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 194), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] + (rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 193), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
        }
      }
      free rows
    }
  }
}

Bounding small allocations...
Lowering after bounding small allocations:
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input"))
let input = _halide_buffer_get_host(input.buffer)
let input.min.0 = _halide_buffer_get_min(input.buffer, 0)
let input.stride.0 = _halide_buffer_get_stride(input.buffer, 0)
let input.min.1 = _halide_buffer_get_min(input.buffer, 1)
let input.stride.1 = _halide_buffer_get_stride(input.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      prefetch(input, (((input.stride.1*5) + ((input.stride.1*output.s0.y.y)*4)) + -3), ((((output.extent.0 + 63)/64)*64) + 64), 1, 10, input.stride.1)
      allocate rows[int32 * ((((output.extent.0 + 63)/64)*64) + 64) * 4]
      produce rows {
        for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
          rows[ramp((rows.s0.x.x*64), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp(((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*2) + input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*2) - input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y))*4) + (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)]))
          rows[ramp((((((output.extent.0 + 63)/64) + rows.s0.x.x)*64) + 64), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp((((input.stride.1*2) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1))) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1))) + -3), 1, 64) aligned(64, 61)]))
          rows[ramp(((((((output.extent.0 + 63)/64)*2) + rows.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp((((input.stride.1*2) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1))) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1))) + -3), 1, 64) aligned(64, 61)]))
          rows[ramp(((((((output.extent.0 + 63)/64)*3) + rows.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] = (((int32x64(input[ramp((((input.stride.1*2) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1))) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1))) + -3), 1, 64) aligned(64, 61)]))
        }
      }
      consume rows {
        for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
          output[ramp((((output.s0.x.x*16) + (output.s0.y.y*output.stride.1))*4), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp(((output.s0.x.x*64) + 6), 1, 64) aligned(64, 6)] + ((rows[ramp(((output.s0.x.x*64) + 5), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((output.s0.x.x*64) + 4), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((output.s0.x.x*64) + 3), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((output.s0.x.x*64) + 2), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp((output.s0.x.x*64), 1, 64) aligned(64, 0)] + (rows[ramp(((output.s0.x.x*64) + 1), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 1)*output.stride.1)), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 70), 1, 64) aligned(64, 6)] + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 69), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 68), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 67), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 66), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 64), 1, 64) aligned(64, 0)] + (rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 65), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 2)*output.stride.1)), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 134), 1, 64) aligned(64, 6)] + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 133), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 132), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 131), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 130), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] + (rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 129), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 3)*output.stride.1)), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 198), 1, 64) aligned(64, 6)] + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 197), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 196), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 195), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 194), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] + (rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 193), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
        }
      }
      free rows
    }
  }
}


Simplifying...
Lowering unsafe promises...
Lowering after lowering unsafe promises:
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input"))
let input = _halide_buffer_get_host(input.buffer)
let input.min.0 = _halide_buffer_get_min(input.buffer, 0)
let input.stride.0 = _halide_buffer_get_stride(input.buffer, 0)
let input.min.1 = _halide_buffer_get_min(input.buffer, 1)
let input.stride.1 = _halide_buffer_get_stride(input.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      prefetch(input, (((input.stride.1*5) + ((input.stride.1*output.s0.y.y)*4)) + -3), ((((output.extent.0 + 63)/64)*64) + 64), 1, 10, input.stride.1)
      allocate rows[int32 * ((((output.extent.0 + 63)/64)*64) + 64) * 4]
      produce rows {
        for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
          rows[ramp((rows.s0.x.x*64), 1, 64) aligned(64, 0)] = (let t9 = ((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y)) in (((int32x64(input[ramp(((((t9*2) + input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp((((t9*4) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((t9*4) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((t9*4) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((t9*2) - input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp((((t9*4) - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((t9*4) + (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))
          rows[ramp((((((output.extent.0 + 63)/64) + rows.s0.x.x)*64) + 64), 1, 64) aligned(64, 0)] = (let t10 = ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) in (((int32x64(input[ramp((((input.stride.1*2) + t10) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((t10 + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((t10 + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((t10 - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((t10 - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((t10 - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + t10) + -3), 1, 64) aligned(64, 61)])))
          rows[ramp(((((((output.extent.0 + 63)/64)*2) + rows.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] = (let t11 = ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) in (((int32x64(input[ramp((((input.stride.1*2) + t11) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((t11 + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((t11 + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((t11 - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((t11 - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((t11 - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + t11) + -3), 1, 64) aligned(64, 61)])))
          rows[ramp(((((((output.extent.0 + 63)/64)*3) + rows.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] = (let t12 = ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) in (((int32x64(input[ramp((((input.stride.1*2) + t12) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((t12 + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((t12 + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((t12 - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((t12 - (input.stride.1*2)) + -3), 1, 64) aligned(64, 61)])*x64(6)) + int32x64(input[ramp(((t12 - (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))))) + int32x64(input[ramp((((input.stride.1*3) + t12) + -3), 1, 64) aligned(64, 61)])))
        }
      }
      consume rows {
        for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
          output[ramp((((output.s0.x.x*16) + (output.s0.y.y*output.stride.1))*4), 1, 64) aligned(64, 0)] = uint8x64(max(min(((rows[ramp(((output.s0.x.x*64) + 6), 1, 64) aligned(64, 6)] + ((rows[ramp(((output.s0.x.x*64) + 5), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((output.s0.x.x*64) + 4), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((output.s0.x.x*64) + 3), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((output.s0.x.x*64) + 2), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp((output.s0.x.x*64), 1, 64) aligned(64, 0)] + (rows[ramp(((output.s0.x.x*64) + 1), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 1)*output.stride.1)), 1, 64) aligned(64, 0)] = (let t13 = (((output.extent.0 + 63)/64) + output.s0.x.x) in uint8x64(max(min(((rows[ramp(((t13*64) + 70), 1, 64) aligned(64, 6)] + ((rows[ramp(((t13*64) + 69), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((t13*64) + 68), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((t13*64) + 67), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((t13*64) + 66), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp(((t13*64) + 64), 1, 64) aligned(64, 0)] + (rows[ramp(((t13*64) + 65), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0))))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 2)*output.stride.1)), 1, 64) aligned(64, 0)] = (let t14 = ((((output.extent.0 + 63)/64)*2) + output.s0.x.x) in uint8x64(max(min(((rows[ramp(((t14*64) + 134), 1, 64) aligned(64, 6)] + ((rows[ramp(((t14*64) + 133), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((t14*64) + 132), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((t14*64) + 131), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((t14*64) + 130), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp(((t14*64) + 128), 1, 64) aligned(64, 0)] + (rows[ramp(((t14*64) + 129), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0))))
          output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 3)*output.stride.1)), 1, 64) aligned(64, 0)] = (let t15 = ((((output.extent.0 + 63)/64)*3) + output.s0.x.x) in uint8x64(max(min(((rows[ramp(((t15*64) + 198), 1, 64) aligned(64, 6)] + ((rows[ramp(((t15*64) + 197), 1, 64) aligned(64, 5)]*x64(6)) + ((rows[ramp(((t15*64) + 196), 1, 64) aligned(64, 4)]*x64(15)) + ((rows[ramp(((t15*64) + 195), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((t15*64) + 194), 1, 64) aligned(64, 2)]*x64(15)) + (rows[ramp(((t15*64) + 192), 1, 64) aligned(64, 0)] + (rows[ramp(((t15*64) + 193), 1, 64) aligned(64, 1)]*x64(6))))))))/x64(4096)), x64(255)), x64(0))))
        }
      }
      free rows
    }
  }
}


Lowering after final simplification:
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input"))
let input = _halide_buffer_get_host(input.buffer)
let input.min.0 = _halide_buffer_get_min(input.buffer, 0)
let input.stride.0 = _halide_buffer_get_stride(input.buffer, 0)
let input.min.1 = _halide_buffer_get_min(input.buffer, 1)
let input.stride.1 = _halide_buffer_get_stride(input.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  let t36 = (input.stride.1*2)
  let t37 = (input.stride.1*3)
  let t38 = ((output.extent.0 + 63)/64)
  let t23 = ((output.extent.1 + 3)/4)
  let t26 = ((output.extent.0 + 127)/64)
  let t29 = (-3 - t37)
  let t32 = (-3 - t36)
  let t25 = ((t38*64) + 64)
  let t24 = ((input.stride.1*5) + -3)
  let t30 = (t37 + -3)
  let t31 = (t36 + -3)
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, t23) {
      prefetch(input, (((input.stride.1*output.s0.y.y)*4) + t24), t25, 1, 10, input.stride.1)
      allocate rows[int32 * t25 * 4]
      produce rows {
        let t51 = (output.s0.y.y*4)
        let t49 = ((t51 + 3)*input.stride.1)
        let t47 = ((t51 + 2)*input.stride.1)
        let t46 = ((t51 + 1)*input.stride.1)
        let t43 = (input.stride.1*output.s0.y.y)
        for (rows.s0.x.x, 0, t26) {
          rows[ramp((rows.s0.x.x*64), 1, 64) aligned(64, 0)] = ((((int32x64(input[ramp(((((((rows.s0.x.x*16) + t43)*2) - input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp((((((rows.s0.x.x*16) + t43)*4) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*16) + t43)*4) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((((rows.s0.x.x*16) + t43)*2) + input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp((((((rows.s0.x.x*16) + t43)*4) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp(((((rows.s0.x.x*16) + t43)*4) + t29), 1, 64) aligned(64, 61)])) + int32x64(input[ramp(((((rows.s0.x.x*16) + t43)*4) + t30), 1, 64) aligned(64, 61)]))
          rows[ramp((((rows.s0.x.x + t38)*64) + 64), 1, 64) aligned(64, 0)] = ((((int32x64(input[ramp((((rows.s0.x.x*64) + t46) + t32), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + t46) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + t46) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + t46) + t31), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp(((((rows.s0.x.x*64) + t46) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp((((rows.s0.x.x*64) + t46) + t29), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((((rows.s0.x.x*64) + t46) + t30), 1, 64) aligned(64, 61)]))
          rows[ramp(((((t38*2) + rows.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] = ((((int32x64(input[ramp((((rows.s0.x.x*64) + t47) + t32), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + t47) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + t47) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + t47) + t31), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp(((((rows.s0.x.x*64) + t47) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp((((rows.s0.x.x*64) + t47) + t29), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((((rows.s0.x.x*64) + t47) + t30), 1, 64) aligned(64, 61)]))
          rows[ramp(((((t38*3) + rows.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] = ((((int32x64(input[ramp((((rows.s0.x.x*64) + t49) + t32), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + t49) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + t49) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + t49) + t31), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp(((((rows.s0.x.x*64) + t49) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp((((rows.s0.x.x*64) + t49) + t29), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((((rows.s0.x.x*64) + t49) + t30), 1, 64) aligned(64, 61)]))
        }
      }
      consume rows {
        let t58 = (output.s0.y.y*4)
        let t57 = ((t58 + 3)*output.stride.1)
        let t55 = ((t58 + 2)*output.stride.1)
        let t53 = ((t58 + 1)*output.stride.1)
        let t52 = (output.s0.y.y*output.stride.1)
        for (output.s0.x.x, 0, t38) {
          output[ramp((((output.s0.x.x*16) + t52)*4), 1, 64) aligned(64, 0)] = uint8x64(max(min((((rows[ramp(((output.s0.x.x*64) + 1), 1, 64) aligned(64, 1)]*x64(6)) + (rows[ramp((output.s0.x.x*64), 1, 64) aligned(64, 0)] + ((rows[ramp(((output.s0.x.x*64) + 2), 1, 64) aligned(64, 2)]*x64(15)) + ((rows[ramp(((output.s0.x.x*64) + 3), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((output.s0.x.x*64) + 4), 1, 64) aligned(64, 4)]*x64(15)) + (rows[ramp(((output.s0.x.x*64) + 6), 1, 64) aligned(64, 6)] + (rows[ramp(((output.s0.x.x*64) + 5), 1, 64) aligned(64, 5)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + t53), 1, 64) aligned(64, 0)] = uint8x64(max(min((((rows[ramp((((output.s0.x.x + t38)*64) + 65), 1, 64) aligned(64, 1)]*x64(6)) + (rows[ramp((((output.s0.x.x + t38)*64) + 64), 1, 64) aligned(64, 0)] + ((rows[ramp((((output.s0.x.x + t38)*64) + 66), 1, 64) aligned(64, 2)]*x64(15)) + ((rows[ramp((((output.s0.x.x + t38)*64) + 67), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp((((output.s0.x.x + t38)*64) + 68), 1, 64) aligned(64, 4)]*x64(15)) + (rows[ramp((((output.s0.x.x + t38)*64) + 70), 1, 64) aligned(64, 6)] + (rows[ramp((((output.s0.x.x + t38)*64) + 69), 1, 64) aligned(64, 5)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + t55), 1, 64) aligned(64, 0)] = uint8x64(max(min((((rows[ramp(((((t38*2) + output.s0.x.x)*64) + 129), 1, 64) aligned(64, 1)]*x64(6)) + (rows[ramp(((((t38*2) + output.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] + ((rows[ramp(((((t38*2) + output.s0.x.x)*64) + 130), 1, 64) aligned(64, 2)]*x64(15)) + ((rows[ramp(((((t38*2) + output.s0.x.x)*64) + 131), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((((t38*2) + output.s0.x.x)*64) + 132), 1, 64) aligned(64, 4)]*x64(15)) + (rows[ramp(((((t38*2) + output.s0.x.x)*64) + 134), 1, 64) aligned(64, 6)] + (rows[ramp(((((t38*2) + output.s0.x.x)*64) + 133), 1, 64) aligned(64, 5)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + t57), 1, 64) aligned(64, 0)] = uint8x64(max(min((((rows[ramp(((((t38*3) + output.s0.x.x)*64) + 193), 1, 64) aligned(64, 1)]*x64(6)) + (rows[ramp(((((t38*3) + output.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] + ((rows[ramp(((((t38*3) + output.s0.x.x)*64) + 194), 1, 64) aligned(64, 2)]*x64(15)) + ((rows[ramp(((((t38*3) + output.s0.x.x)*64) + 195), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((((t38*3) + output.s0.x.x)*64) + 196), 1, 64) aligned(64, 4)]*x64(15)) + (rows[ramp(((((t38*3) + output.s0.x.x)*64) + 198), 1, 64) aligned(64, 6)] + (rows[ramp(((((t38*3) + output.s0.x.x)*64) + 197), 1, 64) aligned(64, 5)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
        }
      }
      free rows
    }
  }
}


Skipping Hexagon offload...
Target triple of initial module: hexagon-unknown--elf
Generating llvm bitcode...
Generating llvm bitcode prolog for function gaussian7x7_hvx64...
Unpredicating loads and stores...
Lowering after unpredicating loads/stores:
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input"))
let input = _halide_buffer_get_host(input.buffer)
let input.min.0 = _halide_buffer_get_min(input.buffer, 0)
let input.stride.0 = _halide_buffer_get_stride(input.buffer, 0)
let input.min.1 = _halide_buffer_get_min(input.buffer, 1)
let input.stride.1 = _halide_buffer_get_stride(input.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  let t36 = (input.stride.1*2)
  let t37 = (input.stride.1*3)
  let t38 = ((output.extent.0 + 63)/64)
  let t23 = ((output.extent.1 + 3)/4)
  let t26 = ((output.extent.0 + 127)/64)
  let t29 = (-3 - t37)
  let t32 = (-3 - t36)
  let t25 = ((t38*64) + 64)
  let t24 = ((input.stride.1*5) + -3)
  let t30 = (t37 + -3)
  let t31 = (t36 + -3)
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, t23) {
      prefetch(input, (((input.stride.1*output.s0.y.y)*4) + t24), t25, 1, 10, input.stride.1)
      allocate rows[int32 * t25 * 4]
      produce rows {
        let t51 = (output.s0.y.y*4)
        let t49 = ((t51 + 3)*input.stride.1)
        let t47 = ((t51 + 2)*input.stride.1)
        let t46 = ((t51 + 1)*input.stride.1)
        let t43 = (input.stride.1*output.s0.y.y)
        for (rows.s0.x.x, 0, t26) {
          rows[ramp((rows.s0.x.x*64), 1, 64) aligned(64, 0)] = ((((int32x64(input[ramp(((((((rows.s0.x.x*16) + t43)*2) - input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp((((((rows.s0.x.x*16) + t43)*4) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*16) + t43)*4) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((((rows.s0.x.x*16) + t43)*2) + input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp((((((rows.s0.x.x*16) + t43)*4) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp(((((rows.s0.x.x*16) + t43)*4) + t29), 1, 64) aligned(64, 61)])) + int32x64(input[ramp(((((rows.s0.x.x*16) + t43)*4) + t30), 1, 64) aligned(64, 61)]))
          rows[ramp((((rows.s0.x.x + t38)*64) + 64), 1, 64) aligned(64, 0)] = ((((int32x64(input[ramp((((rows.s0.x.x*64) + t46) + t32), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + t46) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + t46) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + t46) + t31), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp(((((rows.s0.x.x*64) + t46) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp((((rows.s0.x.x*64) + t46) + t29), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((((rows.s0.x.x*64) + t46) + t30), 1, 64) aligned(64, 61)]))
          rows[ramp(((((t38*2) + rows.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] = ((((int32x64(input[ramp((((rows.s0.x.x*64) + t47) + t32), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + t47) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + t47) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + t47) + t31), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp(((((rows.s0.x.x*64) + t47) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp((((rows.s0.x.x*64) + t47) + t29), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((((rows.s0.x.x*64) + t47) + t30), 1, 64) aligned(64, 61)]))
          rows[ramp(((((t38*3) + rows.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] = ((((int32x64(input[ramp((((rows.s0.x.x*64) + t49) + t32), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + t49) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + t49) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + t49) + t31), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp(((((rows.s0.x.x*64) + t49) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp((((rows.s0.x.x*64) + t49) + t29), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((((rows.s0.x.x*64) + t49) + t30), 1, 64) aligned(64, 61)]))
        }
      }
      consume rows {
        let t58 = (output.s0.y.y*4)
        let t57 = ((t58 + 3)*output.stride.1)
        let t55 = ((t58 + 2)*output.stride.1)
        let t53 = ((t58 + 1)*output.stride.1)
        let t52 = (output.s0.y.y*output.stride.1)
        for (output.s0.x.x, 0, t38) {
          output[ramp((((output.s0.x.x*16) + t52)*4), 1, 64) aligned(64, 0)] = uint8x64(max(min((((rows[ramp(((output.s0.x.x*64) + 1), 1, 64) aligned(64, 1)]*x64(6)) + (rows[ramp((output.s0.x.x*64), 1, 64) aligned(64, 0)] + ((rows[ramp(((output.s0.x.x*64) + 2), 1, 64) aligned(64, 2)]*x64(15)) + ((rows[ramp(((output.s0.x.x*64) + 3), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((output.s0.x.x*64) + 4), 1, 64) aligned(64, 4)]*x64(15)) + (rows[ramp(((output.s0.x.x*64) + 6), 1, 64) aligned(64, 6)] + (rows[ramp(((output.s0.x.x*64) + 5), 1, 64) aligned(64, 5)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + t53), 1, 64) aligned(64, 0)] = uint8x64(max(min((((rows[ramp((((output.s0.x.x + t38)*64) + 65), 1, 64) aligned(64, 1)]*x64(6)) + (rows[ramp((((output.s0.x.x + t38)*64) + 64), 1, 64) aligned(64, 0)] + ((rows[ramp((((output.s0.x.x + t38)*64) + 66), 1, 64) aligned(64, 2)]*x64(15)) + ((rows[ramp((((output.s0.x.x + t38)*64) + 67), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp((((output.s0.x.x + t38)*64) + 68), 1, 64) aligned(64, 4)]*x64(15)) + (rows[ramp((((output.s0.x.x + t38)*64) + 70), 1, 64) aligned(64, 6)] + (rows[ramp((((output.s0.x.x + t38)*64) + 69), 1, 64) aligned(64, 5)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + t55), 1, 64) aligned(64, 0)] = uint8x64(max(min((((rows[ramp(((((t38*2) + output.s0.x.x)*64) + 129), 1, 64) aligned(64, 1)]*x64(6)) + (rows[ramp(((((t38*2) + output.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] + ((rows[ramp(((((t38*2) + output.s0.x.x)*64) + 130), 1, 64) aligned(64, 2)]*x64(15)) + ((rows[ramp(((((t38*2) + output.s0.x.x)*64) + 131), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((((t38*2) + output.s0.x.x)*64) + 132), 1, 64) aligned(64, 4)]*x64(15)) + (rows[ramp(((((t38*2) + output.s0.x.x)*64) + 134), 1, 64) aligned(64, 6)] + (rows[ramp(((((t38*2) + output.s0.x.x)*64) + 133), 1, 64) aligned(64, 5)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + t57), 1, 64) aligned(64, 0)] = uint8x64(max(min((((rows[ramp(((((t38*3) + output.s0.x.x)*64) + 193), 1, 64) aligned(64, 1)]*x64(6)) + (rows[ramp(((((t38*3) + output.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] + ((rows[ramp(((((t38*3) + output.s0.x.x)*64) + 194), 1, 64) aligned(64, 2)]*x64(15)) + ((rows[ramp(((((t38*3) + output.s0.x.x)*64) + 195), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((((t38*3) + output.s0.x.x)*64) + 196), 1, 64) aligned(64, 4)]*x64(15)) + (rows[ramp(((((t38*3) + output.s0.x.x)*64) + 198), 1, 64) aligned(64, 6)] + (rows[ramp(((((t38*3) + output.s0.x.x)*64) + 197), 1, 64) aligned(64, 5)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
        }
      }
      free rows
    }
  }
}


Optimizing shuffles...
Lowering after optimizing shuffles:
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input"))
let input = _halide_buffer_get_host(input.buffer)
let input.min.0 = _halide_buffer_get_min(input.buffer, 0)
let input.stride.0 = _halide_buffer_get_stride(input.buffer, 0)
let input.min.1 = _halide_buffer_get_min(input.buffer, 1)
let input.stride.1 = _halide_buffer_get_stride(input.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  let t36 = (input.stride.1*2)
  let t37 = (input.stride.1*3)
  let t38 = ((output.extent.0 + 63)/64)
  let t23 = ((output.extent.1 + 3)/4)
  let t26 = ((output.extent.0 + 127)/64)
  let t29 = (-3 - t37)
  let t32 = (-3 - t36)
  let t25 = ((t38*64) + 64)
  let t24 = ((input.stride.1*5) + -3)
  let t30 = (t37 + -3)
  let t31 = (t36 + -3)
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, t23) {
      prefetch(input, (((input.stride.1*output.s0.y.y)*4) + t24), t25, 1, 10, input.stride.1)
      allocate rows[int32 * t25 * 4]
      produce rows {
        let t51 = (output.s0.y.y*4)
        let t49 = ((t51 + 3)*input.stride.1)
        let t47 = ((t51 + 2)*input.stride.1)
        let t46 = ((t51 + 1)*input.stride.1)
        let t43 = (input.stride.1*output.s0.y.y)
        for (rows.s0.x.x, 0, t26) {
          rows[ramp((rows.s0.x.x*64), 1, 64) aligned(64, 0)] = ((((int32x64(input[ramp(((((((rows.s0.x.x*16) + t43)*2) - input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp((((((rows.s0.x.x*16) + t43)*4) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((((rows.s0.x.x*16) + t43)*4) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((((rows.s0.x.x*16) + t43)*2) + input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp((((((rows.s0.x.x*16) + t43)*4) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp(((((rows.s0.x.x*16) + t43)*4) + t29), 1, 64) aligned(64, 61)])) + int32x64(input[ramp(((((rows.s0.x.x*16) + t43)*4) + t30), 1, 64) aligned(64, 61)]))
          rows[ramp((((rows.s0.x.x + t38)*64) + 64), 1, 64) aligned(64, 0)] = ((((int32x64(input[ramp((((rows.s0.x.x*64) + t46) + t32), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + t46) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + t46) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + t46) + t31), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp(((((rows.s0.x.x*64) + t46) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp((((rows.s0.x.x*64) + t46) + t29), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((((rows.s0.x.x*64) + t46) + t30), 1, 64) aligned(64, 61)]))
          rows[ramp(((((t38*2) + rows.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] = ((((int32x64(input[ramp((((rows.s0.x.x*64) + t47) + t32), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + t47) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + t47) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + t47) + t31), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp(((((rows.s0.x.x*64) + t47) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp((((rows.s0.x.x*64) + t47) + t29), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((((rows.s0.x.x*64) + t47) + t30), 1, 64) aligned(64, 61)]))
          rows[ramp(((((t38*3) + rows.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] = ((((int32x64(input[ramp((((rows.s0.x.x*64) + t49) + t32), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((((rows.s0.x.x*64) + t49) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + t49) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((rows.s0.x.x*64) + t49) + t31), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp(((((rows.s0.x.x*64) + t49) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp((((rows.s0.x.x*64) + t49) + t29), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((((rows.s0.x.x*64) + t49) + t30), 1, 64) aligned(64, 61)]))
        }
      }
      consume rows {
        let t58 = (output.s0.y.y*4)
        let t57 = ((t58 + 3)*output.stride.1)
        let t55 = ((t58 + 2)*output.stride.1)
        let t53 = ((t58 + 1)*output.stride.1)
        let t52 = (output.s0.y.y*output.stride.1)
        for (output.s0.x.x, 0, t38) {
          output[ramp((((output.s0.x.x*16) + t52)*4), 1, 64) aligned(64, 0)] = uint8x64(max(min((((rows[ramp(((output.s0.x.x*64) + 1), 1, 64) aligned(64, 1)]*x64(6)) + (rows[ramp((output.s0.x.x*64), 1, 64) aligned(64, 0)] + ((rows[ramp(((output.s0.x.x*64) + 2), 1, 64) aligned(64, 2)]*x64(15)) + ((rows[ramp(((output.s0.x.x*64) + 3), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((output.s0.x.x*64) + 4), 1, 64) aligned(64, 4)]*x64(15)) + (rows[ramp(((output.s0.x.x*64) + 6), 1, 64) aligned(64, 6)] + (rows[ramp(((output.s0.x.x*64) + 5), 1, 64) aligned(64, 5)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + t53), 1, 64) aligned(64, 0)] = uint8x64(max(min((((rows[ramp((((output.s0.x.x + t38)*64) + 65), 1, 64) aligned(64, 1)]*x64(6)) + (rows[ramp((((output.s0.x.x + t38)*64) + 64), 1, 64) aligned(64, 0)] + ((rows[ramp((((output.s0.x.x + t38)*64) + 66), 1, 64) aligned(64, 2)]*x64(15)) + ((rows[ramp((((output.s0.x.x + t38)*64) + 67), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp((((output.s0.x.x + t38)*64) + 68), 1, 64) aligned(64, 4)]*x64(15)) + (rows[ramp((((output.s0.x.x + t38)*64) + 70), 1, 64) aligned(64, 6)] + (rows[ramp((((output.s0.x.x + t38)*64) + 69), 1, 64) aligned(64, 5)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + t55), 1, 64) aligned(64, 0)] = uint8x64(max(min((((rows[ramp(((((t38*2) + output.s0.x.x)*64) + 129), 1, 64) aligned(64, 1)]*x64(6)) + (rows[ramp(((((t38*2) + output.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] + ((rows[ramp(((((t38*2) + output.s0.x.x)*64) + 130), 1, 64) aligned(64, 2)]*x64(15)) + ((rows[ramp(((((t38*2) + output.s0.x.x)*64) + 131), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((((t38*2) + output.s0.x.x)*64) + 132), 1, 64) aligned(64, 4)]*x64(15)) + (rows[ramp(((((t38*2) + output.s0.x.x)*64) + 134), 1, 64) aligned(64, 6)] + (rows[ramp(((((t38*2) + output.s0.x.x)*64) + 133), 1, 64) aligned(64, 5)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
          output[ramp(((output.s0.x.x*64) + t57), 1, 64) aligned(64, 0)] = uint8x64(max(min((((rows[ramp(((((t38*3) + output.s0.x.x)*64) + 193), 1, 64) aligned(64, 1)]*x64(6)) + (rows[ramp(((((t38*3) + output.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] + ((rows[ramp(((((t38*3) + output.s0.x.x)*64) + 194), 1, 64) aligned(64, 2)]*x64(15)) + ((rows[ramp(((((t38*3) + output.s0.x.x)*64) + 195), 1, 64) aligned(64, 3)]*x64(20)) + ((rows[ramp(((((t38*3) + output.s0.x.x)*64) + 196), 1, 64) aligned(64, 4)]*x64(15)) + (rows[ramp(((((t38*3) + output.s0.x.x)*64) + 198), 1, 64) aligned(64, 6)] + (rows[ramp(((((t38*3) + output.s0.x.x)*64) + 197), 1, 64) aligned(64, 5)]*x64(6))))))))/x64(4096)), x64(255)), x64(0)))
        }
      }
      free rows
    }
  }
}


Aligning loads for HVX....
Lowering after aligning loads:
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input"))
let input = _halide_buffer_get_host(input.buffer)
let input.min.0 = _halide_buffer_get_min(input.buffer, 0)
let input.stride.0 = _halide_buffer_get_stride(input.buffer, 0)
let input.min.1 = _halide_buffer_get_min(input.buffer, 1)
let input.stride.1 = _halide_buffer_get_stride(input.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  let t36 = (input.stride.1*2)
  let t37 = (input.stride.1*3)
  let t38 = ((output.extent.0 + 63)/64)
  let t23 = ((output.extent.1 + 3)/4)
  let t26 = ((output.extent.0 + 127)/64)
  let t29 = (-3 - t37)
  let t32 = (-3 - t36)
  let t25 = ((t38*64) + 64)
  let t24 = ((input.stride.1*5) + -3)
  let t30 = (t37 + -3)
  let t31 = (t36 + -3)
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, t23) {
      prefetch(input, (((input.stride.1*output.s0.y.y)*4) + t24), t25, 1, 10, input.stride.1)
      allocate rows[int32 * t25 * 4]
      produce rows {
        let t51 = (output.s0.y.y*4)
        let t49 = ((t51 + 3)*input.stride.1)
        let t47 = ((t51 + 2)*input.stride.1)
        let t46 = ((t51 + 1)*input.stride.1)
        let t43 = (input.stride.1*output.s0.y.y)
        for (rows.s0.x.x, 0, t26) {
          rows[ramp((rows.s0.x.x*64), 1, 64) aligned(64, 0)] = (let t59 = ((rows.s0.x.x*16) + t43) in ((((int32x64(input[ramp(((((t59*2) - input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp((((t59*4) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((t59*4) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((t59*2) + input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp((((t59*4) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp(((t59*4) + t29), 1, 64) aligned(64, 61)])) + int32x64(input[ramp(((t59*4) + t30), 1, 64) aligned(64, 61)])))
          rows[ramp((((rows.s0.x.x + t38)*64) + 64), 1, 64) aligned(64, 0)] = (let t60 = ((rows.s0.x.x*64) + t46) in ((((int32x64(input[ramp((t60 + t32), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((t60 - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((t60 + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((t60 + t31), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp(((t60 + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp((t60 + t29), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((t60 + t30), 1, 64) aligned(64, 61)])))
          rows[ramp(((((t38*2) + rows.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] = (let t61 = ((rows.s0.x.x*64) + t47) in ((((int32x64(input[ramp((t61 + t32), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((t61 - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((t61 + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((t61 + t31), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp(((t61 + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp((t61 + t29), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((t61 + t30), 1, 64) aligned(64, 61)])))
          rows[ramp(((((t38*3) + rows.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] = (let t62 = ((rows.s0.x.x*64) + t49) in ((((int32x64(input[ramp((t62 + t32), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((t62 - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((t62 + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((t62 + t31), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp(((t62 + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp((t62 + t29), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((t62 + t30), 1, 64) aligned(64, 61)])))
        }
      }
      consume rows {
        let t58 = (output.s0.y.y*4)
        let t57 = ((t58 + 3)*output.stride.1)
        let t55 = ((t58 + 2)*output.stride.1)
        let t53 = ((t58 + 1)*output.stride.1)
        let t52 = (output.s0.y.y*output.stride.1)
        for (output.s0.x.x, 0, t38) {
          output[ramp((((output.s0.x.x*16) + t52)*4), 1, 64) aligned(64, 0)] = (let t63 = rows[ramp((output.s0.x.x*64), 1, 16) aligned(64, 0)] in (let t64 = rows[ramp(((output.s0.x.x*64) + 16), 1, 16) aligned(64, 16)] in (let t65 = concat_vectors(t63, t64) in (let t66 = rows[ramp(((output.s0.x.x*64) + 32), 1, 16) aligned(64, 32)] in (let t67 = concat_vectors(t64, t66) in (let t68 = rows[ramp(((output.s0.x.x*64) + 48), 1, 16) aligned(64, 48)] in (let t69 = concat_vectors(t66, t68) in (let t70 = concat_vectors(t68, rows[ramp(((output.s0.x.x*64) + 64), 1, 16) aligned(64, 0)]) in uint8x64(max(min((((concat_vectors(slice_vectors(t65, 1, 1, 16), slice_vectors(t67, 1, 1, 16), slice_vectors(t69, 1, 1, 16), slice_vectors(t70, 1, 1, 16))*x64(6)) + (concat_vectors(t63, t64, t66, t68) + ((concat_vectors(slice_vectors(t65, 2, 1, 16), slice_vectors(t67, 2, 1, 16), slice_vectors(t69, 2, 1, 16), slice_vectors(t70, 2, 1, 16))*x64(15)) + ((concat_vectors(slice_vectors(t65, 3, 1, 16), slice_vectors(t67, 3, 1, 16), slice_vectors(t69, 3, 1, 16), slice_vectors(t70, 3, 1, 16))*x64(20)) + ((concat_vectors(slice_vectors(t65, 4, 1, 16), slice_vectors(t67, 4, 1, 16), slice_vectors(t69, 4, 1, 16), slice_vectors(t70, 4, 1, 16))*x64(15)) + (concat_vectors(slice_vectors(t65, 6, 1, 16), slice_vectors(t67, 6, 1, 16), slice_vectors(t69, 6, 1, 16), slice_vectors(t70, 6, 1, 16)) + (concat_vectors(slice_vectors(t65, 5, 1, 16), slice_vectors(t67, 5, 1, 16), slice_vectors(t69, 5, 1, 16), slice_vectors(t70, 5, 1, 16))*x64(6))))))))/x64(4096)), x64(255)), x64(0)))))))))))
          output[ramp(((output.s0.x.x*64) + t53), 1, 64) aligned(64, 0)] = (let t71 = (output.s0.x.x + t38) in (let t72 = rows[ramp(((t71*64) + 64), 1, 16) aligned(64, 0)] in (let t73 = rows[ramp(((t71*64) + 80), 1, 16) aligned(64, 16)] in (let t74 = concat_vectors(t72, t73) in (let t75 = rows[ramp(((t71*64) + 96), 1, 16) aligned(64, 32)] in (let t76 = concat_vectors(t73, t75) in (let t77 = rows[ramp(((t71*64) + 112), 1, 16) aligned(64, 48)] in (let t78 = concat_vectors(t75, t77) in (let t79 = concat_vectors(t77, rows[ramp(((t71*64) + 128), 1, 16) aligned(64, 0)]) in uint8x64(max(min((((concat_vectors(slice_vectors(t74, 1, 1, 16), slice_vectors(t76, 1, 1, 16), slice_vectors(t78, 1, 1, 16), slice_vectors(t79, 1, 1, 16))*x64(6)) + (concat_vectors(t72, t73, t75, t77) + ((concat_vectors(slice_vectors(t74, 2, 1, 16), slice_vectors(t76, 2, 1, 16), slice_vectors(t78, 2, 1, 16), slice_vectors(t79, 2, 1, 16))*x64(15)) + ((concat_vectors(slice_vectors(t74, 3, 1, 16), slice_vectors(t76, 3, 1, 16), slice_vectors(t78, 3, 1, 16), slice_vectors(t79, 3, 1, 16))*x64(20)) + ((concat_vectors(slice_vectors(t74, 4, 1, 16), slice_vectors(t76, 4, 1, 16), slice_vectors(t78, 4, 1, 16), slice_vectors(t79, 4, 1, 16))*x64(15)) + (concat_vectors(slice_vectors(t74, 6, 1, 16), slice_vectors(t76, 6, 1, 16), slice_vectors(t78, 6, 1, 16), slice_vectors(t79, 6, 1, 16)) + (concat_vectors(slice_vectors(t74, 5, 1, 16), slice_vectors(t76, 5, 1, 16), slice_vectors(t78, 5, 1, 16), slice_vectors(t79, 5, 1, 16))*x64(6))))))))/x64(4096)), x64(255)), x64(0))))))))))))
          output[ramp(((output.s0.x.x*64) + t55), 1, 64) aligned(64, 0)] = (let t80 = ((t38*2) + output.s0.x.x) in (let t81 = rows[ramp(((t80*64) + 128), 1, 16) aligned(64, 0)] in (let t82 = rows[ramp(((t80*64) + 144), 1, 16) aligned(64, 16)] in (let t83 = concat_vectors(t81, t82) in (let t84 = rows[ramp(((t80*64) + 160), 1, 16) aligned(64, 32)] in (let t85 = concat_vectors(t82, t84) in (let t86 = rows[ramp(((t80*64) + 176), 1, 16) aligned(64, 48)] in (let t87 = concat_vectors(t84, t86) in (let t88 = concat_vectors(t86, rows[ramp(((t80*64) + 192), 1, 16) aligned(64, 0)]) in uint8x64(max(min((((concat_vectors(slice_vectors(t83, 1, 1, 16), slice_vectors(t85, 1, 1, 16), slice_vectors(t87, 1, 1, 16), slice_vectors(t88, 1, 1, 16))*x64(6)) + (concat_vectors(t81, t82, t84, t86) + ((concat_vectors(slice_vectors(t83, 2, 1, 16), slice_vectors(t85, 2, 1, 16), slice_vectors(t87, 2, 1, 16), slice_vectors(t88, 2, 1, 16))*x64(15)) + ((concat_vectors(slice_vectors(t83, 3, 1, 16), slice_vectors(t85, 3, 1, 16), slice_vectors(t87, 3, 1, 16), slice_vectors(t88, 3, 1, 16))*x64(20)) + ((concat_vectors(slice_vectors(t83, 4, 1, 16), slice_vectors(t85, 4, 1, 16), slice_vectors(t87, 4, 1, 16), slice_vectors(t88, 4, 1, 16))*x64(15)) + (concat_vectors(slice_vectors(t83, 6, 1, 16), slice_vectors(t85, 6, 1, 16), slice_vectors(t87, 6, 1, 16), slice_vectors(t88, 6, 1, 16)) + (concat_vectors(slice_vectors(t83, 5, 1, 16), slice_vectors(t85, 5, 1, 16), slice_vectors(t87, 5, 1, 16), slice_vectors(t88, 5, 1, 16))*x64(6))))))))/x64(4096)), x64(255)), x64(0))))))))))))
          output[ramp(((output.s0.x.x*64) + t57), 1, 64) aligned(64, 0)] = (let t89 = ((t38*3) + output.s0.x.x) in (let t90 = rows[ramp(((t89*64) + 192), 1, 16) aligned(64, 0)] in (let t91 = rows[ramp(((t89*64) + 208), 1, 16) aligned(64, 16)] in (let t92 = concat_vectors(t90, t91) in (let t93 = rows[ramp(((t89*64) + 224), 1, 16) aligned(64, 32)] in (let t94 = concat_vectors(t91, t93) in (let t95 = rows[ramp(((t89*64) + 240), 1, 16) aligned(64, 48)] in (let t96 = concat_vectors(t93, t95) in (let t97 = concat_vectors(t95, rows[ramp(((t89*64) + 256), 1, 16) aligned(64, 0)]) in uint8x64(max(min((((concat_vectors(slice_vectors(t92, 1, 1, 16), slice_vectors(t94, 1, 1, 16), slice_vectors(t96, 1, 1, 16), slice_vectors(t97, 1, 1, 16))*x64(6)) + (concat_vectors(t90, t91, t93, t95) + ((concat_vectors(slice_vectors(t92, 2, 1, 16), slice_vectors(t94, 2, 1, 16), slice_vectors(t96, 2, 1, 16), slice_vectors(t97, 2, 1, 16))*x64(15)) + ((concat_vectors(slice_vectors(t92, 3, 1, 16), slice_vectors(t94, 3, 1, 16), slice_vectors(t96, 3, 1, 16), slice_vectors(t97, 3, 1, 16))*x64(20)) + ((concat_vectors(slice_vectors(t92, 4, 1, 16), slice_vectors(t94, 4, 1, 16), slice_vectors(t96, 4, 1, 16), slice_vectors(t97, 4, 1, 16))*x64(15)) + (concat_vectors(slice_vectors(t92, 6, 1, 16), slice_vectors(t94, 6, 1, 16), slice_vectors(t96, 6, 1, 16), slice_vectors(t97, 6, 1, 16)) + (concat_vectors(slice_vectors(t92, 5, 1, 16), slice_vectors(t94, 5, 1, 16), slice_vectors(t96, 5, 1, 16), slice_vectors(t97, 5, 1, 16))*x64(6))))))))/x64(4096)), x64(255)), x64(0))))))))))))
        }
      }
      free rows
    }
  }
}


Carrying values across loop iterations...
Lowering after forwarding stores:
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input"))
let input = _halide_buffer_get_host(input.buffer)
let input.min.0 = _halide_buffer_get_min(input.buffer, 0)
let input.stride.0 = _halide_buffer_get_stride(input.buffer, 0)
let input.min.1 = _halide_buffer_get_min(input.buffer, 1)
let input.stride.1 = _halide_buffer_get_stride(input.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  let t29 = (-3 - (input.stride.1*3))
  let t32 = (-3 - (input.stride.1*2))
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      prefetch(input, ((((input.stride.1*output.s0.y.y)*4) + (input.stride.1*5)) + -3), ((((output.extent.0 + 63)/64)*64) + 64), 1, 10, input.stride.1)
      allocate rows[int32 * ((((output.extent.0 + 63)/64)*64) + 64) * 4]
      produce rows {
        for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
          rows[ramp((rows.s0.x.x*64), 1, 64) aligned(64, 0)] = (let t59 = ((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y)) in ((((int32x64(input[ramp(((((t59*2) - input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp((((t59*4) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((t59*4) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((t59*2) + input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp((((t59*4) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp(((t59*4) + t29), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((((t59*4) + (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))
          rows[ramp((((((output.extent.0 + 63)/64) + rows.s0.x.x)*64) + 64), 1, 64) aligned(64, 0)] = (let t60 = ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) in ((((int32x64(input[ramp((t32 + t60), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((t60 - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((t60 + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((input.stride.1*2) + t60) + -3), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp(((input.stride.1 + t60) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp((t29 + t60), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((((input.stride.1*3) + t60) + -3), 1, 64) aligned(64, 61)])))
          rows[ramp(((((((output.extent.0 + 63)/64)*2) + rows.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] = (let t61 = ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) in ((((int32x64(input[ramp((t32 + t61), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((t61 - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((t61 + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((input.stride.1*2) + t61) + -3), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp(((input.stride.1 + t61) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp((t29 + t61), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((((input.stride.1*3) + t61) + -3), 1, 64) aligned(64, 61)])))
          rows[ramp(((((((output.extent.0 + 63)/64)*3) + rows.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] = (let t62 = ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) in ((((int32x64(input[ramp((t32 + t62), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((t62 - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((t62 + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((input.stride.1*2) + t62) + -3), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp(((input.stride.1 + t62) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp((t29 + t62), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((((input.stride.1*3) + t62) + -3), 1, 64) aligned(64, 61)])))
        }
      }
      consume rows {
        if ((0 < output.extent.0)) {
          allocate c4[int32 * 32] in Stack
          c4[ramp(0, 1, 16)] = rows[ramp(((((output.extent.0 + 63)/64)*192) + 192), 1, 16) aligned(192, 0)]
          allocate c3[int32 * 32] in Stack
          c3[ramp(0, 1, 16)] = rows[ramp(((((output.extent.0 + 63)/64)*128) + 128), 1, 16) aligned(128, 0)]
          allocate c2[int32 * 32] in Stack
          c2[ramp(0, 1, 16)] = rows[ramp(((((output.extent.0 + 63)/64)*64) + 64), 1, 16) aligned(64, 0)]
          allocate c1[int32 * 32] in Stack
          c1[ramp(0, 1, 16)] = rows[ramp(0, 1, 16)]
          for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
            c1[ramp(16, 1, 16)] = rows[ramp(((output.s0.x.x*64) + 64), 1, 16) aligned(64, 0)]
            c2[ramp(16, 1, 16)] = rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 128), 1, 16) aligned(64, 0)]
            c3[ramp(16, 1, 16)] = rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 192), 1, 16) aligned(64, 0)]
            c4[ramp(16, 1, 16)] = rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 256), 1, 16) aligned(64, 0)]
            output[ramp((((output.s0.x.x*16) + (output.s0.y.y*output.stride.1))*4), 1, 64) aligned(64, 0)] = (let t98 = c1[ramp(0, 1, 16)] in (let t99 = rows[ramp(((output.s0.x.x*64) + 16), 1, 16) aligned(64, 16)] in (let t100 = concat_vectors(t98, t99) in (let t101 = rows[ramp(((output.s0.x.x*64) + 32), 1, 16) aligned(64, 32)] in (let t102 = concat_vectors(t99, t101) in (let t103 = rows[ramp(((output.s0.x.x*64) + 48), 1, 16) aligned(64, 48)] in (let t104 = concat_vectors(t101, t103) in (let t105.s = c1[ramp(16, 1, 16)] in uint8x64(max(min((((concat_vectors(slice_vectors(t100, 1, 1, 16), slice_vectors(t102, 1, 1, 16), slice_vectors(t104, 1, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 1, 1, 16))*x64(6)) + (concat_vectors(t98, t99, t101, t103) + ((concat_vectors(slice_vectors(t100, 2, 1, 16), slice_vectors(t102, 2, 1, 16), slice_vectors(t104, 2, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 2, 1, 16))*x64(15)) + ((concat_vectors(slice_vectors(t100, 3, 1, 16), slice_vectors(t102, 3, 1, 16), slice_vectors(t104, 3, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 3, 1, 16))*x64(20)) + ((concat_vectors(slice_vectors(t100, 4, 1, 16), slice_vectors(t102, 4, 1, 16), slice_vectors(t104, 4, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 4, 1, 16))*x64(15)) + (concat_vectors(slice_vectors(t100, 6, 1, 16), slice_vectors(t102, 6, 1, 16), slice_vectors(t104, 6, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 6, 1, 16)) + (concat_vectors(slice_vectors(t100, 5, 1, 16), slice_vectors(t102, 5, 1, 16), slice_vectors(t104, 5, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 5, 1, 16))*x64(6))))))))/x64(4096)), x64(255)), x64(0)))))))))))
            output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 1)*output.stride.1)), 1, 64) aligned(64, 0)] = (let t106 = c2[ramp(0, 1, 16)] in (let t108 = rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 80), 1, 16) aligned(64, 16)] in (let t109 = concat_vectors(t106, t108) in (let t110 = rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 96), 1, 16) aligned(64, 32)] in (let t111 = concat_vectors(t108, t110) in (let t112 = rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 112), 1, 16) aligned(64, 48)] in (let t113 = concat_vectors(t110, t112) in (let t114.s = c2[ramp(16, 1, 16)] in uint8x64(max(min((((concat_vectors(slice_vectors(t109, 1, 1, 16), slice_vectors(t111, 1, 1, 16), slice_vectors(t113, 1, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 1, 1, 16))*x64(6)) + (concat_vectors(t106, t108, t110, t112) + ((concat_vectors(slice_vectors(t109, 2, 1, 16), slice_vectors(t111, 2, 1, 16), slice_vectors(t113, 2, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 2, 1, 16))*x64(15)) + ((concat_vectors(slice_vectors(t109, 3, 1, 16), slice_vectors(t111, 3, 1, 16), slice_vectors(t113, 3, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 3, 1, 16))*x64(20)) + ((concat_vectors(slice_vectors(t109, 4, 1, 16), slice_vectors(t111, 4, 1, 16), slice_vectors(t113, 4, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 4, 1, 16))*x64(15)) + (concat_vectors(slice_vectors(t109, 6, 1, 16), slice_vectors(t111, 6, 1, 16), slice_vectors(t113, 6, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 6, 1, 16)) + (concat_vectors(slice_vectors(t109, 5, 1, 16), slice_vectors(t111, 5, 1, 16), slice_vectors(t113, 5, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 5, 1, 16))*x64(6))))))))/x64(4096)), x64(255)), x64(0)))))))))))
            output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 2)*output.stride.1)), 1, 64) aligned(64, 0)] = (let t115 = c3[ramp(0, 1, 16)] in (let t117 = rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 144), 1, 16) aligned(64, 16)] in (let t118 = concat_vectors(t115, t117) in (let t119 = rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 160), 1, 16) aligned(64, 32)] in (let t120 = concat_vectors(t117, t119) in (let t121 = rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 176), 1, 16) aligned(64, 48)] in (let t122 = concat_vectors(t119, t121) in (let t123.s = c3[ramp(16, 1, 16)] in uint8x64(max(min((((concat_vectors(slice_vectors(t118, 1, 1, 16), slice_vectors(t120, 1, 1, 16), slice_vectors(t122, 1, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 1, 1, 16))*x64(6)) + (concat_vectors(t115, t117, t119, t121) + ((concat_vectors(slice_vectors(t118, 2, 1, 16), slice_vectors(t120, 2, 1, 16), slice_vectors(t122, 2, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 2, 1, 16))*x64(15)) + ((concat_vectors(slice_vectors(t118, 3, 1, 16), slice_vectors(t120, 3, 1, 16), slice_vectors(t122, 3, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 3, 1, 16))*x64(20)) + ((concat_vectors(slice_vectors(t118, 4, 1, 16), slice_vectors(t120, 4, 1, 16), slice_vectors(t122, 4, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 4, 1, 16))*x64(15)) + (concat_vectors(slice_vectors(t118, 6, 1, 16), slice_vectors(t120, 6, 1, 16), slice_vectors(t122, 6, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 6, 1, 16)) + (concat_vectors(slice_vectors(t118, 5, 1, 16), slice_vectors(t120, 5, 1, 16), slice_vectors(t122, 5, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 5, 1, 16))*x64(6))))))))/x64(4096)), x64(255)), x64(0)))))))))))
            output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 3)*output.stride.1)), 1, 64) aligned(64, 0)] = (let t124 = c4[ramp(0, 1, 16)] in (let t126 = rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 208), 1, 16) aligned(64, 16)] in (let t127 = concat_vectors(t124, t126) in (let t128 = rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 224), 1, 16) aligned(64, 32)] in (let t129 = concat_vectors(t126, t128) in (let t130 = rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 240), 1, 16) aligned(64, 48)] in (let t131 = concat_vectors(t128, t130) in (let t132.s = c4[ramp(16, 1, 16)] in uint8x64(max(min((((concat_vectors(slice_vectors(t127, 1, 1, 16), slice_vectors(t129, 1, 1, 16), slice_vectors(t131, 1, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 1, 1, 16))*x64(6)) + (concat_vectors(t124, t126, t128, t130) + ((concat_vectors(slice_vectors(t127, 2, 1, 16), slice_vectors(t129, 2, 1, 16), slice_vectors(t131, 2, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 2, 1, 16))*x64(15)) + ((concat_vectors(slice_vectors(t127, 3, 1, 16), slice_vectors(t129, 3, 1, 16), slice_vectors(t131, 3, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 3, 1, 16))*x64(20)) + ((concat_vectors(slice_vectors(t127, 4, 1, 16), slice_vectors(t129, 4, 1, 16), slice_vectors(t131, 4, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 4, 1, 16))*x64(15)) + (concat_vectors(slice_vectors(t127, 6, 1, 16), slice_vectors(t129, 6, 1, 16), slice_vectors(t131, 6, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 6, 1, 16)) + (concat_vectors(slice_vectors(t127, 5, 1, 16), slice_vectors(t129, 5, 1, 16), slice_vectors(t131, 5, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 5, 1, 16))*x64(6))))))))/x64(4096)), x64(255)), x64(0)))))))))))
            c1[ramp(0, 1, 16)] = c1[ramp(16, 1, 16)]
            c2[ramp(0, 1, 16)] = c2[ramp(16, 1, 16)]
            c3[ramp(0, 1, 16)] = c3[ramp(16, 1, 16)]
            c4[ramp(0, 1, 16)] = c4[ramp(16, 1, 16)]
          }
        }
      }
      free rows
    }
  }
}


Eliminating boolean vectors from Hexagon code...
Lowering after eliminating boolean vectors: assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input"))
let input = _halide_buffer_get_host(input.buffer)
let input.min.0 = _halide_buffer_get_min(input.buffer, 0)
let input.stride.0 = _halide_buffer_get_stride(input.buffer, 0)
let input.min.1 = _halide_buffer_get_min(input.buffer, 1)
let input.stride.1 = _halide_buffer_get_stride(input.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  let t29 = (-3 - (input.stride.1*3))
  let t32 = (-3 - (input.stride.1*2))
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      prefetch(input, ((((input.stride.1*output.s0.y.y)*4) + (input.stride.1*5)) + -3), ((((output.extent.0 + 63)/64)*64) + 64), 1, 10, input.stride.1)
      allocate rows[int32 * ((((output.extent.0 + 63)/64)*64) + 64) * 4]
      produce rows {
        for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
          rows[ramp((rows.s0.x.x*64), 1, 64) aligned(64, 0)] = (let t59 = ((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y)) in ((((int32x64(input[ramp(((((t59*2) - input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp((((t59*4) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((t59*4) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((t59*2) + input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp((((t59*4) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp(((t59*4) + t29), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((((t59*4) + (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))
          rows[ramp((((((output.extent.0 + 63)/64) + rows.s0.x.x)*64) + 64), 1, 64) aligned(64, 0)] = (let t60 = ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) in ((((int32x64(input[ramp((t32 + t60), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((t60 - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((t60 + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((input.stride.1*2) + t60) + -3), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp(((input.stride.1 + t60) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp((t29 + t60), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((((input.stride.1*3) + t60) + -3), 1, 64) aligned(64, 61)])))
          rows[ramp(((((((output.extent.0 + 63)/64)*2) + rows.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] = (let t61 = ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) in ((((int32x64(input[ramp((t32 + t61), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((t61 - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((t61 + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((input.stride.1*2) + t61) + -3), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp(((input.stride.1 + t61) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp((t29 + t61), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((((input.stride.1*3) + t61) + -3), 1, 64) aligned(64, 61)])))
          rows[ramp(((((((output.extent.0 + 63)/64)*3) + rows.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] = (let t62 = ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) in ((((int32x64(input[ramp((t32 + t62), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((t62 - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((t62 + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((input.stride.1*2) + t62) + -3), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp(((input.stride.1 + t62) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp((t29 + t62), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((((input.stride.1*3) + t62) + -3), 1, 64) aligned(64, 61)])))
        }
      }
      consume rows {
        if ((0 < output.extent.0)) {
          allocate c4[int32 * 32] in Stack
          c4[ramp(0, 1, 16)] = rows[ramp(((((output.extent.0 + 63)/64)*192) + 192), 1, 16) aligned(192, 0)]
          allocate c3[int32 * 32] in Stack
          c3[ramp(0, 1, 16)] = rows[ramp(((((output.extent.0 + 63)/64)*128) + 128), 1, 16) aligned(128, 0)]
          allocate c2[int32 * 32] in Stack
          c2[ramp(0, 1, 16)] = rows[ramp(((((output.extent.0 + 63)/64)*64) + 64), 1, 16) aligned(64, 0)]
          allocate c1[int32 * 32] in Stack
          c1[ramp(0, 1, 16)] = rows[ramp(0, 1, 16)]
          for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
            c1[ramp(16, 1, 16)] = rows[ramp(((output.s0.x.x*64) + 64), 1, 16) aligned(64, 0)]
            c2[ramp(16, 1, 16)] = rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 128), 1, 16) aligned(64, 0)]
            c3[ramp(16, 1, 16)] = rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 192), 1, 16) aligned(64, 0)]
            c4[ramp(16, 1, 16)] = rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 256), 1, 16) aligned(64, 0)]
            output[ramp((((output.s0.x.x*16) + (output.s0.y.y*output.stride.1))*4), 1, 64) aligned(64, 0)] = (let t98 = c1[ramp(0, 1, 16)] in (let t99 = rows[ramp(((output.s0.x.x*64) + 16), 1, 16) aligned(64, 16)] in (let t100 = concat_vectors(t98, t99) in (let t101 = rows[ramp(((output.s0.x.x*64) + 32), 1, 16) aligned(64, 32)] in (let t102 = concat_vectors(t99, t101) in (let t103 = rows[ramp(((output.s0.x.x*64) + 48), 1, 16) aligned(64, 48)] in (let t104 = concat_vectors(t101, t103) in (let t105.s = c1[ramp(16, 1, 16)] in uint8x64(max(min((((concat_vectors(slice_vectors(t100, 1, 1, 16), slice_vectors(t102, 1, 1, 16), slice_vectors(t104, 1, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 1, 1, 16))*x64(6)) + (concat_vectors(t98, t99, t101, t103) + ((concat_vectors(slice_vectors(t100, 2, 1, 16), slice_vectors(t102, 2, 1, 16), slice_vectors(t104, 2, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 2, 1, 16))*x64(15)) + ((concat_vectors(slice_vectors(t100, 3, 1, 16), slice_vectors(t102, 3, 1, 16), slice_vectors(t104, 3, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 3, 1, 16))*x64(20)) + ((concat_vectors(slice_vectors(t100, 4, 1, 16), slice_vectors(t102, 4, 1, 16), slice_vectors(t104, 4, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 4, 1, 16))*x64(15)) + (concat_vectors(slice_vectors(t100, 6, 1, 16), slice_vectors(t102, 6, 1, 16), slice_vectors(t104, 6, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 6, 1, 16)) + (concat_vectors(slice_vectors(t100, 5, 1, 16), slice_vectors(t102, 5, 1, 16), slice_vectors(t104, 5, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 5, 1, 16))*x64(6))))))))/x64(4096)), x64(255)), x64(0)))))))))))
            output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 1)*output.stride.1)), 1, 64) aligned(64, 0)] = (let t106 = c2[ramp(0, 1, 16)] in (let t108 = rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 80), 1, 16) aligned(64, 16)] in (let t109 = concat_vectors(t106, t108) in (let t110 = rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 96), 1, 16) aligned(64, 32)] in (let t111 = concat_vectors(t108, t110) in (let t112 = rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 112), 1, 16) aligned(64, 48)] in (let t113 = concat_vectors(t110, t112) in (let t114.s = c2[ramp(16, 1, 16)] in uint8x64(max(min((((concat_vectors(slice_vectors(t109, 1, 1, 16), slice_vectors(t111, 1, 1, 16), slice_vectors(t113, 1, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 1, 1, 16))*x64(6)) + (concat_vectors(t106, t108, t110, t112) + ((concat_vectors(slice_vectors(t109, 2, 1, 16), slice_vectors(t111, 2, 1, 16), slice_vectors(t113, 2, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 2, 1, 16))*x64(15)) + ((concat_vectors(slice_vectors(t109, 3, 1, 16), slice_vectors(t111, 3, 1, 16), slice_vectors(t113, 3, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 3, 1, 16))*x64(20)) + ((concat_vectors(slice_vectors(t109, 4, 1, 16), slice_vectors(t111, 4, 1, 16), slice_vectors(t113, 4, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 4, 1, 16))*x64(15)) + (concat_vectors(slice_vectors(t109, 6, 1, 16), slice_vectors(t111, 6, 1, 16), slice_vectors(t113, 6, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 6, 1, 16)) + (concat_vectors(slice_vectors(t109, 5, 1, 16), slice_vectors(t111, 5, 1, 16), slice_vectors(t113, 5, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 5, 1, 16))*x64(6))))))))/x64(4096)), x64(255)), x64(0)))))))))))
            output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 2)*output.stride.1)), 1, 64) aligned(64, 0)] = (let t115 = c3[ramp(0, 1, 16)] in (let t117 = rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 144), 1, 16) aligned(64, 16)] in (let t118 = concat_vectors(t115, t117) in (let t119 = rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 160), 1, 16) aligned(64, 32)] in (let t120 = concat_vectors(t117, t119) in (let t121 = rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 176), 1, 16) aligned(64, 48)] in (let t122 = concat_vectors(t119, t121) in (let t123.s = c3[ramp(16, 1, 16)] in uint8x64(max(min((((concat_vectors(slice_vectors(t118, 1, 1, 16), slice_vectors(t120, 1, 1, 16), slice_vectors(t122, 1, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 1, 1, 16))*x64(6)) + (concat_vectors(t115, t117, t119, t121) + ((concat_vectors(slice_vectors(t118, 2, 1, 16), slice_vectors(t120, 2, 1, 16), slice_vectors(t122, 2, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 2, 1, 16))*x64(15)) + ((concat_vectors(slice_vectors(t118, 3, 1, 16), slice_vectors(t120, 3, 1, 16), slice_vectors(t122, 3, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 3, 1, 16))*x64(20)) + ((concat_vectors(slice_vectors(t118, 4, 1, 16), slice_vectors(t120, 4, 1, 16), slice_vectors(t122, 4, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 4, 1, 16))*x64(15)) + (concat_vectors(slice_vectors(t118, 6, 1, 16), slice_vectors(t120, 6, 1, 16), slice_vectors(t122, 6, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 6, 1, 16)) + (concat_vectors(slice_vectors(t118, 5, 1, 16), slice_vectors(t120, 5, 1, 16), slice_vectors(t122, 5, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 5, 1, 16))*x64(6))))))))/x64(4096)), x64(255)), x64(0)))))))))))
            output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 3)*output.stride.1)), 1, 64) aligned(64, 0)] = (let t124 = c4[ramp(0, 1, 16)] in (let t126 = rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 208), 1, 16) aligned(64, 16)] in (let t127 = concat_vectors(t124, t126) in (let t128 = rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 224), 1, 16) aligned(64, 32)] in (let t129 = concat_vectors(t126, t128) in (let t130 = rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 240), 1, 16) aligned(64, 48)] in (let t131 = concat_vectors(t128, t130) in (let t132.s = c4[ramp(16, 1, 16)] in uint8x64(max(min((((concat_vectors(slice_vectors(t127, 1, 1, 16), slice_vectors(t129, 1, 1, 16), slice_vectors(t131, 1, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 1, 1, 16))*x64(6)) + (concat_vectors(t124, t126, t128, t130) + ((concat_vectors(slice_vectors(t127, 2, 1, 16), slice_vectors(t129, 2, 1, 16), slice_vectors(t131, 2, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 2, 1, 16))*x64(15)) + ((concat_vectors(slice_vectors(t127, 3, 1, 16), slice_vectors(t129, 3, 1, 16), slice_vectors(t131, 3, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 3, 1, 16))*x64(20)) + ((concat_vectors(slice_vectors(t127, 4, 1, 16), slice_vectors(t129, 4, 1, 16), slice_vectors(t131, 4, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 4, 1, 16))*x64(15)) + (concat_vectors(slice_vectors(t127, 6, 1, 16), slice_vectors(t129, 6, 1, 16), slice_vectors(t131, 6, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 6, 1, 16)) + (concat_vectors(slice_vectors(t127, 5, 1, 16), slice_vectors(t129, 5, 1, 16), slice_vectors(t131, 5, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 5, 1, 16))*x64(6))))))))/x64(4096)), x64(255)), x64(0)))))))))))
            c1[ramp(0, 1, 16)] = c1[ramp(16, 1, 16)]
            c2[ramp(0, 1, 16)] = c2[ramp(16, 1, 16)]
            c3[ramp(0, 1, 16)] = c3[ramp(16, 1, 16)]
            c4[ramp(0, 1, 16)] = c4[ramp(16, 1, 16)]
          }
        }
      }
      free rows
    }
  }
}


Optimizing Hexagon instructions...
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input"))
let input = _halide_buffer_get_host(input.buffer)
let input.min.0 = _halide_buffer_get_min(input.buffer, 0)
let input.stride.0 = _halide_buffer_get_stride(input.buffer, 0)
let input.min.1 = _halide_buffer_get_min(input.buffer, 1)
let input.stride.1 = _halide_buffer_get_stride(input.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  let t29 = (-3 - (input.stride.1*3))
  let t32 = (-3 - (input.stride.1*2))
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      prefetch(input, ((((input.stride.1*output.s0.y.y)*4) + (input.stride.1*5)) + -3), ((((output.extent.0 + 63)/64)*64) + 64), 1, 10, input.stride.1)
      allocate rows[int32 * ((((output.extent.0 + 63)/64)*64) + 64) * 4]
      produce rows {
        for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
          rows[ramp((rows.s0.x.x*64), 1, 64) aligned(64, 0)] = (let t59 = ((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y)) in ((((int32x64(input[ramp(((((t59*2) - input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp((((t59*4) - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp(((t59*4) + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp(((((t59*2) + input.stride.1)*2) + -3), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp((((t59*4) + input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp(((t59*4) + t29), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((((t59*4) + (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])))
          rows[ramp((((((output.extent.0 + 63)/64) + rows.s0.x.x)*64) + 64), 1, 64) aligned(64, 0)] = (let t60 = ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) in ((((int32x64(input[ramp((t32 + t60), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((t60 - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((t60 + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((input.stride.1*2) + t60) + -3), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp(((input.stride.1 + t60) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp((t29 + t60), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((((input.stride.1*3) + t60) + -3), 1, 64) aligned(64, 61)])))
          rows[ramp(((((((output.extent.0 + 63)/64)*2) + rows.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] = (let t61 = ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) in ((((int32x64(input[ramp((t32 + t61), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((t61 - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((t61 + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((input.stride.1*2) + t61) + -3), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp(((input.stride.1 + t61) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp((t29 + t61), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((((input.stride.1*3) + t61) + -3), 1, 64) aligned(64, 61)])))
          rows[ramp(((((((output.extent.0 + 63)/64)*3) + rows.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] = (let t62 = ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) in ((((int32x64(input[ramp((t32 + t62), 1, 64) aligned(64, 61)])*x64(6)) + ((int32x64(input[ramp(((t62 - input.stride.1) + -3), 1, 64) aligned(64, 61)])*x64(15)) + ((int32x64(input[ramp((t62 + -3), 1, 64) aligned(64, 61)])*x64(20)) + ((int32x64(input[ramp((((input.stride.1*2) + t62) + -3), 1, 64) aligned(64, 61)])*x64(6)) + (int32x64(input[ramp(((input.stride.1 + t62) + -3), 1, 64) aligned(64, 61)])*x64(15)))))) + int32x64(input[ramp((t29 + t62), 1, 64) aligned(64, 61)])) + int32x64(input[ramp((((input.stride.1*3) + t62) + -3), 1, 64) aligned(64, 61)])))
        }
      }
      consume rows {
        if ((0 < output.extent.0)) {
          allocate c4[int32 * 32] in Stack
          c4[ramp(0, 1, 16)] = rows[ramp(((((output.extent.0 + 63)/64)*192) + 192), 1, 16) aligned(192, 0)]
          allocate c3[int32 * 32] in Stack
          c3[ramp(0, 1, 16)] = rows[ramp(((((output.extent.0 + 63)/64)*128) + 128), 1, 16) aligned(128, 0)]
          allocate c2[int32 * 32] in Stack
          c2[ramp(0, 1, 16)] = rows[ramp(((((output.extent.0 + 63)/64)*64) + 64), 1, 16) aligned(64, 0)]
          allocate c1[int32 * 32] in Stack
          c1[ramp(0, 1, 16)] = rows[ramp(0, 1, 16)]
          for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
            c1[ramp(16, 1, 16)] = rows[ramp(((output.s0.x.x*64) + 64), 1, 16) aligned(64, 0)]
            c2[ramp(16, 1, 16)] = rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 128), 1, 16) aligned(64, 0)]
            c3[ramp(16, 1, 16)] = rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 192), 1, 16) aligned(64, 0)]
            c4[ramp(16, 1, 16)] = rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 256), 1, 16) aligned(64, 0)]
            output[ramp((((output.s0.x.x*16) + (output.s0.y.y*output.stride.1))*4), 1, 64) aligned(64, 0)] = (let t98 = c1[ramp(0, 1, 16)] in (let t99 = rows[ramp(((output.s0.x.x*64) + 16), 1, 16) aligned(64, 16)] in (let t100 = concat_vectors(t98, t99) in (let t101 = rows[ramp(((output.s0.x.x*64) + 32), 1, 16) aligned(64, 32)] in (let t102 = concat_vectors(t99, t101) in (let t103 = rows[ramp(((output.s0.x.x*64) + 48), 1, 16) aligned(64, 48)] in (let t104 = concat_vectors(t101, t103) in (let t105.s = c1[ramp(16, 1, 16)] in uint8x64(max(min((((concat_vectors(slice_vectors(t100, 1, 1, 16), slice_vectors(t102, 1, 1, 16), slice_vectors(t104, 1, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 1, 1, 16))*x64(6)) + (concat_vectors(t98, t99, t101, t103) + ((concat_vectors(slice_vectors(t100, 2, 1, 16), slice_vectors(t102, 2, 1, 16), slice_vectors(t104, 2, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 2, 1, 16))*x64(15)) + ((concat_vectors(slice_vectors(t100, 3, 1, 16), slice_vectors(t102, 3, 1, 16), slice_vectors(t104, 3, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 3, 1, 16))*x64(20)) + ((concat_vectors(slice_vectors(t100, 4, 1, 16), slice_vectors(t102, 4, 1, 16), slice_vectors(t104, 4, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 4, 1, 16))*x64(15)) + (concat_vectors(slice_vectors(t100, 6, 1, 16), slice_vectors(t102, 6, 1, 16), slice_vectors(t104, 6, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 6, 1, 16)) + (concat_vectors(slice_vectors(t100, 5, 1, 16), slice_vectors(t102, 5, 1, 16), slice_vectors(t104, 5, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 5, 1, 16))*x64(6))))))))/x64(4096)), x64(255)), x64(0)))))))))))
            output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 1)*output.stride.1)), 1, 64) aligned(64, 0)] = (let t106 = c2[ramp(0, 1, 16)] in (let t108 = rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 80), 1, 16) aligned(64, 16)] in (let t109 = concat_vectors(t106, t108) in (let t110 = rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 96), 1, 16) aligned(64, 32)] in (let t111 = concat_vectors(t108, t110) in (let t112 = rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 112), 1, 16) aligned(64, 48)] in (let t113 = concat_vectors(t110, t112) in (let t114.s = c2[ramp(16, 1, 16)] in uint8x64(max(min((((concat_vectors(slice_vectors(t109, 1, 1, 16), slice_vectors(t111, 1, 1, 16), slice_vectors(t113, 1, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 1, 1, 16))*x64(6)) + (concat_vectors(t106, t108, t110, t112) + ((concat_vectors(slice_vectors(t109, 2, 1, 16), slice_vectors(t111, 2, 1, 16), slice_vectors(t113, 2, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 2, 1, 16))*x64(15)) + ((concat_vectors(slice_vectors(t109, 3, 1, 16), slice_vectors(t111, 3, 1, 16), slice_vectors(t113, 3, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 3, 1, 16))*x64(20)) + ((concat_vectors(slice_vectors(t109, 4, 1, 16), slice_vectors(t111, 4, 1, 16), slice_vectors(t113, 4, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 4, 1, 16))*x64(15)) + (concat_vectors(slice_vectors(t109, 6, 1, 16), slice_vectors(t111, 6, 1, 16), slice_vectors(t113, 6, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 6, 1, 16)) + (concat_vectors(slice_vectors(t109, 5, 1, 16), slice_vectors(t111, 5, 1, 16), slice_vectors(t113, 5, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 5, 1, 16))*x64(6))))))))/x64(4096)), x64(255)), x64(0)))))))))))
            output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 2)*output.stride.1)), 1, 64) aligned(64, 0)] = (let t115 = c3[ramp(0, 1, 16)] in (let t117 = rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 144), 1, 16) aligned(64, 16)] in (let t118 = concat_vectors(t115, t117) in (let t119 = rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 160), 1, 16) aligned(64, 32)] in (let t120 = concat_vectors(t117, t119) in (let t121 = rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 176), 1, 16) aligned(64, 48)] in (let t122 = concat_vectors(t119, t121) in (let t123.s = c3[ramp(16, 1, 16)] in uint8x64(max(min((((concat_vectors(slice_vectors(t118, 1, 1, 16), slice_vectors(t120, 1, 1, 16), slice_vectors(t122, 1, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 1, 1, 16))*x64(6)) + (concat_vectors(t115, t117, t119, t121) + ((concat_vectors(slice_vectors(t118, 2, 1, 16), slice_vectors(t120, 2, 1, 16), slice_vectors(t122, 2, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 2, 1, 16))*x64(15)) + ((concat_vectors(slice_vectors(t118, 3, 1, 16), slice_vectors(t120, 3, 1, 16), slice_vectors(t122, 3, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 3, 1, 16))*x64(20)) + ((concat_vectors(slice_vectors(t118, 4, 1, 16), slice_vectors(t120, 4, 1, 16), slice_vectors(t122, 4, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 4, 1, 16))*x64(15)) + (concat_vectors(slice_vectors(t118, 6, 1, 16), slice_vectors(t120, 6, 1, 16), slice_vectors(t122, 6, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 6, 1, 16)) + (concat_vectors(slice_vectors(t118, 5, 1, 16), slice_vectors(t120, 5, 1, 16), slice_vectors(t122, 5, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 5, 1, 16))*x64(6))))))))/x64(4096)), x64(255)), x64(0)))))))))))
            output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 3)*output.stride.1)), 1, 64) aligned(64, 0)] = (let t124 = c4[ramp(0, 1, 16)] in (let t126 = rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 208), 1, 16) aligned(64, 16)] in (let t127 = concat_vectors(t124, t126) in (let t128 = rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 224), 1, 16) aligned(64, 32)] in (let t129 = concat_vectors(t126, t128) in (let t130 = rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 240), 1, 16) aligned(64, 48)] in (let t131 = concat_vectors(t128, t130) in (let t132.s = c4[ramp(16, 1, 16)] in uint8x64(max(min((((concat_vectors(slice_vectors(t127, 1, 1, 16), slice_vectors(t129, 1, 1, 16), slice_vectors(t131, 1, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 1, 1, 16))*x64(6)) + (concat_vectors(t124, t126, t128, t130) + ((concat_vectors(slice_vectors(t127, 2, 1, 16), slice_vectors(t129, 2, 1, 16), slice_vectors(t131, 2, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 2, 1, 16))*x64(15)) + ((concat_vectors(slice_vectors(t127, 3, 1, 16), slice_vectors(t129, 3, 1, 16), slice_vectors(t131, 3, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 3, 1, 16))*x64(20)) + ((concat_vectors(slice_vectors(t127, 4, 1, 16), slice_vectors(t129, 4, 1, 16), slice_vectors(t131, 4, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 4, 1, 16))*x64(15)) + (concat_vectors(slice_vectors(t127, 6, 1, 16), slice_vectors(t129, 6, 1, 16), slice_vectors(t131, 6, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 6, 1, 16)) + (concat_vectors(slice_vectors(t127, 5, 1, 16), slice_vectors(t129, 5, 1, 16), slice_vectors(t131, 5, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 5, 1, 16))*x64(6))))))))/x64(4096)), x64(255)), x64(0)))))))))))
            c1[ramp(0, 1, 16)] = c1[ramp(16, 1, 16)]
            c2[ramp(0, 1, 16)] = c2[ramp(16, 1, 16)]
            c3[ramp(0, 1, 16)] = c3[ramp(16, 1, 16)]
            c4[ramp(0, 1, 16)] = c4[ramp(16, 1, 16)]
          }
        }
      }
      free rows
    }
  }
}


assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input"))
let input = _halide_buffer_get_host(input.buffer)
let input.min.0 = _halide_buffer_get_min(input.buffer, 0)
let input.stride.0 = _halide_buffer_get_stride(input.buffer, 0)
let input.min.1 = _halide_buffer_get_min(input.buffer, 1)
let input.stride.1 = _halide_buffer_get_stride(input.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  let t29 = (-3 - (input.stride.1*3))
  let t32 = (-3 - (input.stride.1*2))
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      prefetch(input, ((((input.stride.1*output.s0.y.y)*4) + (input.stride.1*5)) + -3), ((((output.extent.0 + 63)/64)*64) + 64), 1, 10, input.stride.1)
      allocate rows[int32 * ((((output.extent.0 + 63)/64)*64) + 64) * 4]
      produce rows {
        for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
          rows[ramp((rows.s0.x.x*64), 1, 64) aligned(64, 0)] = halide.hexagon.interleave.vw((let t59 = ((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y)) in halide.hexagon.acc_add_2mpy.vw.vh.vh.b.b(halide.hexagon.deinterleave.vw(halide.hexagon.acc_add_4mpy.vw.vub.b(halide.hexagon.unpack.vuh(halide.hexagon.unpack.vub(input[ramp((((t59*4) + (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])), interleave_vectors(input[ramp(((((t59*2) - input.stride.1)*2) + -3), 1, 64) aligned(64, 61)], input[ramp((((t59*4) - input.stride.1) + -3), 1, 64) aligned(64, 61)], input[ramp(((t59*4) + -3), 1, 64) aligned(64, 61)], input[ramp(((((t59*2) + input.stride.1)*2) + -3), 1, 64) aligned(64, 61)]), reinterpret(int32, concat_vectors((int8)6, (int8)15, (int8)20, (int8)6)))), halide.hexagon.unpack.vub(input[ramp((((t59*4) + input.stride.1) + -3), 1, 64) aligned(64, 61)]), halide.hexagon.unpack.vub(input[ramp(((t59*4) + t29), 1, 64) aligned(64, 61)]), (int8)15, (int8)1)))
          rows[ramp((((((output.extent.0 + 63)/64) + rows.s0.x.x)*64) + 64), 1, 64) aligned(64, 0)] = halide.hexagon.interleave.vw((let t60 = ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) in halide.hexagon.acc_add_2mpy.vw.vh.vh.b.b(halide.hexagon.deinterleave.vw(halide.hexagon.acc_add_4mpy.vw.vub.b(halide.hexagon.unpack.vuh(halide.hexagon.unpack.vub(input[ramp((((input.stride.1*3) + t60) + -3), 1, 64) aligned(64, 61)])), interleave_vectors(input[ramp((t32 + t60), 1, 64) aligned(64, 61)], input[ramp(((t60 - input.stride.1) + -3), 1, 64) aligned(64, 61)], input[ramp((t60 + -3), 1, 64) aligned(64, 61)], input[ramp((((input.stride.1*2) + t60) + -3), 1, 64) aligned(64, 61)]), reinterpret(int32, concat_vectors((int8)6, (int8)15, (int8)20, (int8)6)))), halide.hexagon.unpack.vub(input[ramp(((input.stride.1 + t60) + -3), 1, 64) aligned(64, 61)]), halide.hexagon.unpack.vub(input[ramp((t29 + t60), 1, 64) aligned(64, 61)]), (int8)15, (int8)1)))
          rows[ramp(((((((output.extent.0 + 63)/64)*2) + rows.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] = halide.hexagon.interleave.vw((let t61 = ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) in halide.hexagon.acc_add_2mpy.vw.vh.vh.b.b(halide.hexagon.deinterleave.vw(halide.hexagon.acc_add_4mpy.vw.vub.b(halide.hexagon.unpack.vuh(halide.hexagon.unpack.vub(input[ramp((((input.stride.1*3) + t61) + -3), 1, 64) aligned(64, 61)])), interleave_vectors(input[ramp((t32 + t61), 1, 64) aligned(64, 61)], input[ramp(((t61 - input.stride.1) + -3), 1, 64) aligned(64, 61)], input[ramp((t61 + -3), 1, 64) aligned(64, 61)], input[ramp((((input.stride.1*2) + t61) + -3), 1, 64) aligned(64, 61)]), reinterpret(int32, concat_vectors((int8)6, (int8)15, (int8)20, (int8)6)))), halide.hexagon.unpack.vub(input[ramp(((input.stride.1 + t61) + -3), 1, 64) aligned(64, 61)]), halide.hexagon.unpack.vub(input[ramp((t29 + t61), 1, 64) aligned(64, 61)]), (int8)15, (int8)1)))
          rows[ramp(((((((output.extent.0 + 63)/64)*3) + rows.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] = halide.hexagon.interleave.vw((let t62 = ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) in halide.hexagon.acc_add_2mpy.vw.vh.vh.b.b(halide.hexagon.deinterleave.vw(halide.hexagon.acc_add_4mpy.vw.vub.b(halide.hexagon.unpack.vuh(halide.hexagon.unpack.vub(input[ramp((((input.stride.1*3) + t62) + -3), 1, 64) aligned(64, 61)])), interleave_vectors(input[ramp((t32 + t62), 1, 64) aligned(64, 61)], input[ramp(((t62 - input.stride.1) + -3), 1, 64) aligned(64, 61)], input[ramp((t62 + -3), 1, 64) aligned(64, 61)], input[ramp((((input.stride.1*2) + t62) + -3), 1, 64) aligned(64, 61)]), reinterpret(int32, concat_vectors((int8)6, (int8)15, (int8)20, (int8)6)))), halide.hexagon.unpack.vub(input[ramp(((input.stride.1 + t62) + -3), 1, 64) aligned(64, 61)]), halide.hexagon.unpack.vub(input[ramp((t29 + t62), 1, 64) aligned(64, 61)]), (int8)15, (int8)1)))
        }
      }
      consume rows {
        if ((0 < output.extent.0)) {
          allocate c4[int32 * 32] in Stack
          c4[ramp(0, 1, 16)] = rows[ramp(((((output.extent.0 + 63)/64)*192) + 192), 1, 16) aligned(192, 0)]
          allocate c3[int32 * 32] in Stack
          c3[ramp(0, 1, 16)] = rows[ramp(((((output.extent.0 + 63)/64)*128) + 128), 1, 16) aligned(128, 0)]
          allocate c2[int32 * 32] in Stack
          c2[ramp(0, 1, 16)] = rows[ramp(((((output.extent.0 + 63)/64)*64) + 64), 1, 16) aligned(64, 0)]
          allocate c1[int32 * 32] in Stack
          c1[ramp(0, 1, 16)] = rows[ramp(0, 1, 16)]
          for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
            c1[ramp(16, 1, 16)] = rows[ramp(((output.s0.x.x*64) + 64), 1, 16) aligned(64, 0)]
            c2[ramp(16, 1, 16)] = rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 128), 1, 16) aligned(64, 0)]
            c3[ramp(16, 1, 16)] = rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 192), 1, 16) aligned(64, 0)]
            c4[ramp(16, 1, 16)] = rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 256), 1, 16) aligned(64, 0)]
            output[ramp((((output.s0.x.x*16) + (output.s0.y.y*output.stride.1))*4), 1, 64) aligned(64, 0)] = (let t98 = c1[ramp(0, 1, 16)] in (let t99 = rows[ramp(((output.s0.x.x*64) + 16), 1, 16) aligned(64, 16)] in (let t100 = concat_vectors(t98, t99) in (let t101 = rows[ramp(((output.s0.x.x*64) + 32), 1, 16) aligned(64, 32)] in (let t102 = concat_vectors(t99, t101) in (let t103 = rows[ramp(((output.s0.x.x*64) + 48), 1, 16) aligned(64, 48)] in (let t104 = concat_vectors(t101, t103) in (let t105.s = c1[ramp(16, 1, 16)] in halide.hexagon.pack_satub.vh(halide.hexagon.trunc_sath_shr.vw.uw(halide.hexagon.deinterleave.vw(halide.hexagon.add_mul.vw.vw.h((concat_vectors(t98, t99, t101, t103) + halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(concat_vectors(slice_vectors(t100, 6, 1, 16), slice_vectors(t102, 6, 1, 16), slice_vectors(t104, 6, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 6, 1, 16)), concat_vectors(slice_vectors(t100, 5, 1, 16), slice_vectors(t102, 5, 1, 16), slice_vectors(t104, 5, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 5, 1, 16)), (int16)6), concat_vectors(slice_vectors(t100, 4, 1, 16), slice_vectors(t102, 4, 1, 16), slice_vectors(t104, 4, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 4, 1, 16)), (int16)15), concat_vectors(slice_vectors(t100, 3, 1, 16), slice_vectors(t102, 3, 1, 16), slice_vectors(t104, 3, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 3, 1, 16)), (int16)20), concat_vectors(slice_vectors(t100, 2, 1, 16), slice_vectors(t102, 2, 1, 16), slice_vectors(t104, 2, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 2, 1, 16)), (int16)15)), concat_vectors(slice_vectors(t100, 1, 1, 16), slice_vectors(t102, 1, 1, 16), slice_vectors(t104, 1, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 1, 1, 16)), (int16)6)), 12))))))))))
            output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 1)*output.stride.1)), 1, 64) aligned(64, 0)] = (let t106 = c2[ramp(0, 1, 16)] in (let t108 = rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 80), 1, 16) aligned(64, 16)] in (let t109 = concat_vectors(t106, t108) in (let t110 = rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 96), 1, 16) aligned(64, 32)] in (let t111 = concat_vectors(t108, t110) in (let t112 = rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 112), 1, 16) aligned(64, 48)] in (let t113 = concat_vectors(t110, t112) in (let t114.s = c2[ramp(16, 1, 16)] in halide.hexagon.pack_satub.vh(halide.hexagon.trunc_sath_shr.vw.uw(halide.hexagon.deinterleave.vw(halide.hexagon.add_mul.vw.vw.h((concat_vectors(t106, t108, t110, t112) + halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(concat_vectors(slice_vectors(t109, 6, 1, 16), slice_vectors(t111, 6, 1, 16), slice_vectors(t113, 6, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 6, 1, 16)), concat_vectors(slice_vectors(t109, 5, 1, 16), slice_vectors(t111, 5, 1, 16), slice_vectors(t113, 5, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 5, 1, 16)), (int16)6), concat_vectors(slice_vectors(t109, 4, 1, 16), slice_vectors(t111, 4, 1, 16), slice_vectors(t113, 4, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 4, 1, 16)), (int16)15), concat_vectors(slice_vectors(t109, 3, 1, 16), slice_vectors(t111, 3, 1, 16), slice_vectors(t113, 3, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 3, 1, 16)), (int16)20), concat_vectors(slice_vectors(t109, 2, 1, 16), slice_vectors(t111, 2, 1, 16), slice_vectors(t113, 2, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 2, 1, 16)), (int16)15)), concat_vectors(slice_vectors(t109, 1, 1, 16), slice_vectors(t111, 1, 1, 16), slice_vectors(t113, 1, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 1, 1, 16)), (int16)6)), 12))))))))))
            output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 2)*output.stride.1)), 1, 64) aligned(64, 0)] = (let t115 = c3[ramp(0, 1, 16)] in (let t117 = rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 144), 1, 16) aligned(64, 16)] in (let t118 = concat_vectors(t115, t117) in (let t119 = rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 160), 1, 16) aligned(64, 32)] in (let t120 = concat_vectors(t117, t119) in (let t121 = rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 176), 1, 16) aligned(64, 48)] in (let t122 = concat_vectors(t119, t121) in (let t123.s = c3[ramp(16, 1, 16)] in halide.hexagon.pack_satub.vh(halide.hexagon.trunc_sath_shr.vw.uw(halide.hexagon.deinterleave.vw(halide.hexagon.add_mul.vw.vw.h((concat_vectors(t115, t117, t119, t121) + halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(concat_vectors(slice_vectors(t118, 6, 1, 16), slice_vectors(t120, 6, 1, 16), slice_vectors(t122, 6, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 6, 1, 16)), concat_vectors(slice_vectors(t118, 5, 1, 16), slice_vectors(t120, 5, 1, 16), slice_vectors(t122, 5, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 5, 1, 16)), (int16)6), concat_vectors(slice_vectors(t118, 4, 1, 16), slice_vectors(t120, 4, 1, 16), slice_vectors(t122, 4, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 4, 1, 16)), (int16)15), concat_vectors(slice_vectors(t118, 3, 1, 16), slice_vectors(t120, 3, 1, 16), slice_vectors(t122, 3, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 3, 1, 16)), (int16)20), concat_vectors(slice_vectors(t118, 2, 1, 16), slice_vectors(t120, 2, 1, 16), slice_vectors(t122, 2, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 2, 1, 16)), (int16)15)), concat_vectors(slice_vectors(t118, 1, 1, 16), slice_vectors(t120, 1, 1, 16), slice_vectors(t122, 1, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 1, 1, 16)), (int16)6)), 12))))))))))
            output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 3)*output.stride.1)), 1, 64) aligned(64, 0)] = (let t124 = c4[ramp(0, 1, 16)] in (let t126 = rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 208), 1, 16) aligned(64, 16)] in (let t127 = concat_vectors(t124, t126) in (let t128 = rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 224), 1, 16) aligned(64, 32)] in (let t129 = concat_vectors(t126, t128) in (let t130 = rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 240), 1, 16) aligned(64, 48)] in (let t131 = concat_vectors(t128, t130) in (let t132.s = c4[ramp(16, 1, 16)] in halide.hexagon.pack_satub.vh(halide.hexagon.trunc_sath_shr.vw.uw(halide.hexagon.deinterleave.vw(halide.hexagon.add_mul.vw.vw.h((concat_vectors(t124, t126, t128, t130) + halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(concat_vectors(slice_vectors(t127, 6, 1, 16), slice_vectors(t129, 6, 1, 16), slice_vectors(t131, 6, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 6, 1, 16)), concat_vectors(slice_vectors(t127, 5, 1, 16), slice_vectors(t129, 5, 1, 16), slice_vectors(t131, 5, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 5, 1, 16)), (int16)6), concat_vectors(slice_vectors(t127, 4, 1, 16), slice_vectors(t129, 4, 1, 16), slice_vectors(t131, 4, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 4, 1, 16)), (int16)15), concat_vectors(slice_vectors(t127, 3, 1, 16), slice_vectors(t129, 3, 1, 16), slice_vectors(t131, 3, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 3, 1, 16)), (int16)20), concat_vectors(slice_vectors(t127, 2, 1, 16), slice_vectors(t129, 2, 1, 16), slice_vectors(t131, 2, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 2, 1, 16)), (int16)15)), concat_vectors(slice_vectors(t127, 1, 1, 16), slice_vectors(t129, 1, 1, 16), slice_vectors(t131, 1, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 1, 1, 16)), (int16)6)), 12))))))))))
            c1[ramp(0, 1, 16)] = c1[ramp(16, 1, 16)]
            c2[ramp(0, 1, 16)] = c2[ramp(16, 1, 16)]
            c3[ramp(0, 1, 16)] = c3[ramp(16, 1, 16)]
            c4[ramp(0, 1, 16)] = c4[ramp(16, 1, 16)]
          }
        }
      }
      free rows
    }
  }
}

Adding calls to qurt_hvx_lock, if necessary...
Hexagon function body:
let hvx_lock_result = halide_qurt_hvx_lock(64)
assert((hvx_lock_result == 0), hvx_lock_result)
register_destructor("halide_qurt_hvx_unlock_as_destructor", reinterpret((void *), (uint64)1))
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input"))
let input = _halide_buffer_get_host(input.buffer)
let input.min.0 = _halide_buffer_get_min(input.buffer, 0)
let input.stride.0 = _halide_buffer_get_stride(input.buffer, 0)
let input.min.1 = _halide_buffer_get_min(input.buffer, 1)
let input.stride.1 = _halide_buffer_get_stride(input.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
assert((input.stride.0 == 1), 0)
assert((input.min.0 == 0), 0)
assert(((input.stride.1 % 64) == 0), 0)
assert((input.min.1 == 0), 0)
assert((output.stride.0 == 1), 0)
assert((output.min.0 == 0), 0)
assert(((output.stride.1 % 64) == 0), 0)
assert((output.min.1 == 0), 0)
produce output {
  let t29 = (-3 - (input.stride.1*3))
  let t32 = (-3 - (input.stride.1*2))
  for<Hexagon> (output.s0.__outermost, 0, 1) {
    for (output.s0.y.y, 0, ((output.extent.1 + 3)/4)) {
      prefetch(input, ((((input.stride.1*output.s0.y.y)*4) + (input.stride.1*5)) + -3), ((((output.extent.0 + 63)/64)*64) + 64), 1, 10, input.stride.1)
      allocate rows[int32 * ((((output.extent.0 + 63)/64)*64) + 64) * 4]
      produce rows {
        for (rows.s0.x.x, 0, ((output.extent.0 + 127)/64)) {
          rows[ramp((rows.s0.x.x*64), 1, 64) aligned(64, 0)] = halide.hexagon.interleave.vw((let t59 = ((rows.s0.x.x*16) + (input.stride.1*output.s0.y.y)) in halide.hexagon.acc_add_2mpy.vw.vh.vh.b.b(halide.hexagon.deinterleave.vw(halide.hexagon.acc_add_4mpy.vw.vub.b(halide.hexagon.unpack.vuh(halide.hexagon.unpack.vub(input[ramp((((t59*4) + (input.stride.1*3)) + -3), 1, 64) aligned(64, 61)])), interleave_vectors(input[ramp(((((t59*2) - input.stride.1)*2) + -3), 1, 64) aligned(64, 61)], input[ramp((((t59*4) - input.stride.1) + -3), 1, 64) aligned(64, 61)], input[ramp(((t59*4) + -3), 1, 64) aligned(64, 61)], input[ramp(((((t59*2) + input.stride.1)*2) + -3), 1, 64) aligned(64, 61)]), reinterpret(int32, concat_vectors((int8)6, (int8)15, (int8)20, (int8)6)))), halide.hexagon.unpack.vub(input[ramp((((t59*4) + input.stride.1) + -3), 1, 64) aligned(64, 61)]), halide.hexagon.unpack.vub(input[ramp(((t59*4) + t29), 1, 64) aligned(64, 61)]), (int8)15, (int8)1)))
          rows[ramp((((((output.extent.0 + 63)/64) + rows.s0.x.x)*64) + 64), 1, 64) aligned(64, 0)] = halide.hexagon.interleave.vw((let t60 = ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 1)*input.stride.1)) in halide.hexagon.acc_add_2mpy.vw.vh.vh.b.b(halide.hexagon.deinterleave.vw(halide.hexagon.acc_add_4mpy.vw.vub.b(halide.hexagon.unpack.vuh(halide.hexagon.unpack.vub(input[ramp((((input.stride.1*3) + t60) + -3), 1, 64) aligned(64, 61)])), interleave_vectors(input[ramp((t32 + t60), 1, 64) aligned(64, 61)], input[ramp(((t60 - input.stride.1) + -3), 1, 64) aligned(64, 61)], input[ramp((t60 + -3), 1, 64) aligned(64, 61)], input[ramp((((input.stride.1*2) + t60) + -3), 1, 64) aligned(64, 61)]), reinterpret(int32, concat_vectors((int8)6, (int8)15, (int8)20, (int8)6)))), halide.hexagon.unpack.vub(input[ramp(((input.stride.1 + t60) + -3), 1, 64) aligned(64, 61)]), halide.hexagon.unpack.vub(input[ramp((t29 + t60), 1, 64) aligned(64, 61)]), (int8)15, (int8)1)))
          rows[ramp(((((((output.extent.0 + 63)/64)*2) + rows.s0.x.x)*64) + 128), 1, 64) aligned(64, 0)] = halide.hexagon.interleave.vw((let t61 = ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 2)*input.stride.1)) in halide.hexagon.acc_add_2mpy.vw.vh.vh.b.b(halide.hexagon.deinterleave.vw(halide.hexagon.acc_add_4mpy.vw.vub.b(halide.hexagon.unpack.vuh(halide.hexagon.unpack.vub(input[ramp((((input.stride.1*3) + t61) + -3), 1, 64) aligned(64, 61)])), interleave_vectors(input[ramp((t32 + t61), 1, 64) aligned(64, 61)], input[ramp(((t61 - input.stride.1) + -3), 1, 64) aligned(64, 61)], input[ramp((t61 + -3), 1, 64) aligned(64, 61)], input[ramp((((input.stride.1*2) + t61) + -3), 1, 64) aligned(64, 61)]), reinterpret(int32, concat_vectors((int8)6, (int8)15, (int8)20, (int8)6)))), halide.hexagon.unpack.vub(input[ramp(((input.stride.1 + t61) + -3), 1, 64) aligned(64, 61)]), halide.hexagon.unpack.vub(input[ramp((t29 + t61), 1, 64) aligned(64, 61)]), (int8)15, (int8)1)))
          rows[ramp(((((((output.extent.0 + 63)/64)*3) + rows.s0.x.x)*64) + 192), 1, 64) aligned(64, 0)] = halide.hexagon.interleave.vw((let t62 = ((rows.s0.x.x*64) + (((output.s0.y.y*4) + 3)*input.stride.1)) in halide.hexagon.acc_add_2mpy.vw.vh.vh.b.b(halide.hexagon.deinterleave.vw(halide.hexagon.acc_add_4mpy.vw.vub.b(halide.hexagon.unpack.vuh(halide.hexagon.unpack.vub(input[ramp((((input.stride.1*3) + t62) + -3), 1, 64) aligned(64, 61)])), interleave_vectors(input[ramp((t32 + t62), 1, 64) aligned(64, 61)], input[ramp(((t62 - input.stride.1) + -3), 1, 64) aligned(64, 61)], input[ramp((t62 + -3), 1, 64) aligned(64, 61)], input[ramp((((input.stride.1*2) + t62) + -3), 1, 64) aligned(64, 61)]), reinterpret(int32, concat_vectors((int8)6, (int8)15, (int8)20, (int8)6)))), halide.hexagon.unpack.vub(input[ramp(((input.stride.1 + t62) + -3), 1, 64) aligned(64, 61)]), halide.hexagon.unpack.vub(input[ramp((t29 + t62), 1, 64) aligned(64, 61)]), (int8)15, (int8)1)))
        }
      }
      consume rows {
        if ((0 < output.extent.0)) {
          allocate c4[int32 * 32] in Stack
          c4[ramp(0, 1, 16)] = rows[ramp(((((output.extent.0 + 63)/64)*192) + 192), 1, 16) aligned(192, 0)]
          allocate c3[int32 * 32] in Stack
          c3[ramp(0, 1, 16)] = rows[ramp(((((output.extent.0 + 63)/64)*128) + 128), 1, 16) aligned(128, 0)]
          allocate c2[int32 * 32] in Stack
          c2[ramp(0, 1, 16)] = rows[ramp(((((output.extent.0 + 63)/64)*64) + 64), 1, 16) aligned(64, 0)]
          allocate c1[int32 * 32] in Stack
          c1[ramp(0, 1, 16)] = rows[ramp(0, 1, 16)]
          for (output.s0.x.x, 0, ((output.extent.0 + 63)/64)) {
            c1[ramp(16, 1, 16)] = rows[ramp(((output.s0.x.x*64) + 64), 1, 16) aligned(64, 0)]
            c2[ramp(16, 1, 16)] = rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 128), 1, 16) aligned(64, 0)]
            c3[ramp(16, 1, 16)] = rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 192), 1, 16) aligned(64, 0)]
            c4[ramp(16, 1, 16)] = rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 256), 1, 16) aligned(64, 0)]
            output[ramp((((output.s0.x.x*16) + (output.s0.y.y*output.stride.1))*4), 1, 64) aligned(64, 0)] = (let t98 = c1[ramp(0, 1, 16)] in (let t99 = rows[ramp(((output.s0.x.x*64) + 16), 1, 16) aligned(64, 16)] in (let t100 = concat_vectors(t98, t99) in (let t101 = rows[ramp(((output.s0.x.x*64) + 32), 1, 16) aligned(64, 32)] in (let t102 = concat_vectors(t99, t101) in (let t103 = rows[ramp(((output.s0.x.x*64) + 48), 1, 16) aligned(64, 48)] in (let t104 = concat_vectors(t101, t103) in (let t105.s = c1[ramp(16, 1, 16)] in halide.hexagon.pack_satub.vh(halide.hexagon.trunc_sath_shr.vw.uw(halide.hexagon.deinterleave.vw(halide.hexagon.add_mul.vw.vw.h((concat_vectors(t98, t99, t101, t103) + halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(concat_vectors(slice_vectors(t100, 6, 1, 16), slice_vectors(t102, 6, 1, 16), slice_vectors(t104, 6, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 6, 1, 16)), concat_vectors(slice_vectors(t100, 5, 1, 16), slice_vectors(t102, 5, 1, 16), slice_vectors(t104, 5, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 5, 1, 16)), (int16)6), concat_vectors(slice_vectors(t100, 4, 1, 16), slice_vectors(t102, 4, 1, 16), slice_vectors(t104, 4, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 4, 1, 16)), (int16)15), concat_vectors(slice_vectors(t100, 3, 1, 16), slice_vectors(t102, 3, 1, 16), slice_vectors(t104, 3, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 3, 1, 16)), (int16)20), concat_vectors(slice_vectors(t100, 2, 1, 16), slice_vectors(t102, 2, 1, 16), slice_vectors(t104, 2, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 2, 1, 16)), (int16)15)), concat_vectors(slice_vectors(t100, 1, 1, 16), slice_vectors(t102, 1, 1, 16), slice_vectors(t104, 1, 1, 16), slice_vectors(concat_vectors(t103, t105.s), 1, 1, 16)), (int16)6)), 12))))))))))
            output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 1)*output.stride.1)), 1, 64) aligned(64, 0)] = (let t106 = c2[ramp(0, 1, 16)] in (let t108 = rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 80), 1, 16) aligned(64, 16)] in (let t109 = concat_vectors(t106, t108) in (let t110 = rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 96), 1, 16) aligned(64, 32)] in (let t111 = concat_vectors(t108, t110) in (let t112 = rows[ramp((((((output.extent.0 + 63)/64) + output.s0.x.x)*64) + 112), 1, 16) aligned(64, 48)] in (let t113 = concat_vectors(t110, t112) in (let t114.s = c2[ramp(16, 1, 16)] in halide.hexagon.pack_satub.vh(halide.hexagon.trunc_sath_shr.vw.uw(halide.hexagon.deinterleave.vw(halide.hexagon.add_mul.vw.vw.h((concat_vectors(t106, t108, t110, t112) + halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(concat_vectors(slice_vectors(t109, 6, 1, 16), slice_vectors(t111, 6, 1, 16), slice_vectors(t113, 6, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 6, 1, 16)), concat_vectors(slice_vectors(t109, 5, 1, 16), slice_vectors(t111, 5, 1, 16), slice_vectors(t113, 5, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 5, 1, 16)), (int16)6), concat_vectors(slice_vectors(t109, 4, 1, 16), slice_vectors(t111, 4, 1, 16), slice_vectors(t113, 4, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 4, 1, 16)), (int16)15), concat_vectors(slice_vectors(t109, 3, 1, 16), slice_vectors(t111, 3, 1, 16), slice_vectors(t113, 3, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 3, 1, 16)), (int16)20), concat_vectors(slice_vectors(t109, 2, 1, 16), slice_vectors(t111, 2, 1, 16), slice_vectors(t113, 2, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 2, 1, 16)), (int16)15)), concat_vectors(slice_vectors(t109, 1, 1, 16), slice_vectors(t111, 1, 1, 16), slice_vectors(t113, 1, 1, 16), slice_vectors(concat_vectors(t112, t114.s), 1, 1, 16)), (int16)6)), 12))))))))))
            output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 2)*output.stride.1)), 1, 64) aligned(64, 0)] = (let t115 = c3[ramp(0, 1, 16)] in (let t117 = rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 144), 1, 16) aligned(64, 16)] in (let t118 = concat_vectors(t115, t117) in (let t119 = rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 160), 1, 16) aligned(64, 32)] in (let t120 = concat_vectors(t117, t119) in (let t121 = rows[ramp(((((((output.extent.0 + 63)/64)*2) + output.s0.x.x)*64) + 176), 1, 16) aligned(64, 48)] in (let t122 = concat_vectors(t119, t121) in (let t123.s = c3[ramp(16, 1, 16)] in halide.hexagon.pack_satub.vh(halide.hexagon.trunc_sath_shr.vw.uw(halide.hexagon.deinterleave.vw(halide.hexagon.add_mul.vw.vw.h((concat_vectors(t115, t117, t119, t121) + halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(concat_vectors(slice_vectors(t118, 6, 1, 16), slice_vectors(t120, 6, 1, 16), slice_vectors(t122, 6, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 6, 1, 16)), concat_vectors(slice_vectors(t118, 5, 1, 16), slice_vectors(t120, 5, 1, 16), slice_vectors(t122, 5, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 5, 1, 16)), (int16)6), concat_vectors(slice_vectors(t118, 4, 1, 16), slice_vectors(t120, 4, 1, 16), slice_vectors(t122, 4, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 4, 1, 16)), (int16)15), concat_vectors(slice_vectors(t118, 3, 1, 16), slice_vectors(t120, 3, 1, 16), slice_vectors(t122, 3, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 3, 1, 16)), (int16)20), concat_vectors(slice_vectors(t118, 2, 1, 16), slice_vectors(t120, 2, 1, 16), slice_vectors(t122, 2, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 2, 1, 16)), (int16)15)), concat_vectors(slice_vectors(t118, 1, 1, 16), slice_vectors(t120, 1, 1, 16), slice_vectors(t122, 1, 1, 16), slice_vectors(concat_vectors(t121, t123.s), 1, 1, 16)), (int16)6)), 12))))))))))
            output[ramp(((output.s0.x.x*64) + (((output.s0.y.y*4) + 3)*output.stride.1)), 1, 64) aligned(64, 0)] = (let t124 = c4[ramp(0, 1, 16)] in (let t126 = rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 208), 1, 16) aligned(64, 16)] in (let t127 = concat_vectors(t124, t126) in (let t128 = rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 224), 1, 16) aligned(64, 32)] in (let t129 = concat_vectors(t126, t128) in (let t130 = rows[ramp(((((((output.extent.0 + 63)/64)*3) + output.s0.x.x)*64) + 240), 1, 16) aligned(64, 48)] in (let t131 = concat_vectors(t128, t130) in (let t132.s = c4[ramp(16, 1, 16)] in halide.hexagon.pack_satub.vh(halide.hexagon.trunc_sath_shr.vw.uw(halide.hexagon.deinterleave.vw(halide.hexagon.add_mul.vw.vw.h((concat_vectors(t124, t126, t128, t130) + halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(halide.hexagon.add_mul.vw.vw.h(concat_vectors(slice_vectors(t127, 6, 1, 16), slice_vectors(t129, 6, 1, 16), slice_vectors(t131, 6, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 6, 1, 16)), concat_vectors(slice_vectors(t127, 5, 1, 16), slice_vectors(t129, 5, 1, 16), slice_vectors(t131, 5, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 5, 1, 16)), (int16)6), concat_vectors(slice_vectors(t127, 4, 1, 16), slice_vectors(t129, 4, 1, 16), slice_vectors(t131, 4, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 4, 1, 16)), (int16)15), concat_vectors(slice_vectors(t127, 3, 1, 16), slice_vectors(t129, 3, 1, 16), slice_vectors(t131, 3, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 3, 1, 16)), (int16)20), concat_vectors(slice_vectors(t127, 2, 1, 16), slice_vectors(t129, 2, 1, 16), slice_vectors(t131, 2, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 2, 1, 16)), (int16)15)), concat_vectors(slice_vectors(t127, 1, 1, 16), slice_vectors(t129, 1, 1, 16), slice_vectors(t131, 1, 1, 16), slice_vectors(concat_vectors(t130, t132.s), 1, 1, 16)), (int16)6)), 12))))))))))
            c1[ramp(0, 1, 16)] = c1[ramp(16, 1, 16)]
            c2[ramp(0, 1, 16)] = c2[ramp(16, 1, 16)]
            c3[ramp(0, 1, 16)] = c3[ramp(16, 1, 16)]
            c4[ramp(0, 1, 16)] = c4[ramp(16, 1, 16)]
          }
        }
      }
      free rows
    }
  }
}

0x5562d5de25c0
Done generating llvm bitcode
Adding module pass: Target Transform Information
Adding function pass: Target Transform Information
Adding function pass: Instrument function entry/exit with calls to e.g. mcount() (pre inlining)
Adding function pass: Type-Based Alias Analysis
Adding function pass: Scoped NoAlias Alias Analysis
Adding function pass: Simplify the CFG
Adding function pass: SROA
Adding function pass: Early CSE
Adding function pass: Lower 'expect' Intrinsics
Adding module pass: Force set function attributes
Adding module pass: Type-Based Alias Analysis
Adding module pass: Scoped NoAlias Alias Analysis
Adding module pass: Infer set function attributes
Adding module pass: Call-site splitting
Adding module pass: Interprocedural Sparse Conditional Constant Propagation
Adding module pass: Called Value Propagation
Adding module pass: Global Variable Optimizer
Adding module pass: Promote Memory to Register
Adding module pass: Dead Argument Elimination
Adding module pass: Combine redundant instructions
Adding module pass: Simplify the CFG
Adding module pass: Globals Alias Analysis
Adding module pass: Remove unused exception handling info
Adding module pass: Function Integration/Inlining
Adding module pass: Deduce function attributes
Adding module pass: Promote 'by reference' arguments to scalars
Adding module pass: SROA
Adding module pass: Early CSE w/ MemorySSA
Adding module pass: Speculatively execute instructions if target has divergent branches
Adding module pass: Jump Threading
Adding module pass: Value Propagation
Adding module pass: Simplify the CFG
Adding module pass: Combine pattern based expressions
Adding module pass: Combine redundant instructions
Adding module pass: Conditionally eliminate dead library calls
Adding module pass: PGOMemOPSize
Adding module pass: Tail Call Elimination
Adding module pass: Simplify the CFG
Adding module pass: Reassociate expressions
Adding module pass: Rotate Loops
Adding module pass: Loop Invariant Code Motion
Adding module pass: Unswitch loops
Adding module pass: Simplify the CFG
Adding module pass: Combine redundant instructions
Adding module pass: Induction Variable Simplification
Adding module pass: Recognize loop idioms
Adding module pass: Recognize Hexagon-specific loop idioms
Adding module pass: Delete dead loops
Adding module pass: Unroll loops
Adding module pass: Hexagon-specific loop carried reuse for HVX vectors
Adding module pass: MergedLoadStoreMotion
Adding module pass: Global Value Numbering
Adding module pass: MemCpy Optimization
Adding module pass: Sparse Conditional Constant Propagation
Adding module pass: Bit-Tracking Dead Code Elimination
Adding module pass: Combine redundant instructions
Adding module pass: Jump Threading
Adding module pass: Value Propagation
Adding module pass: Dead Store Elimination
Adding module pass: Loop Invariant Code Motion
Adding module pass: Aggressive Dead Code Elimination
Adding module pass: Simplify the CFG
Adding module pass: Combine redundant instructions
Adding module pass: A No-Op Barrier Pass
Adding module pass: Eliminate Available Externally Globals
Adding module pass: Deduce function attributes in RPO
Adding module pass: Global Variable Optimizer
Adding module pass: Dead Global Elimination
Adding module pass: Globals Alias Analysis
Adding module pass: Float to int
Adding module pass: Rotate Loops
Adding module pass: Loop Distribution
Adding module pass: Loop Vectorization
Adding module pass: Loop Load Elimination
Adding module pass: Combine redundant instructions
Adding module pass: Simplify the CFG
Adding module pass: SLP Vectorizer
Adding module pass: Combine redundant instructions
Adding module pass: Unroll loops
Adding module pass: Combine redundant instructions
Adding module pass: Loop Invariant Code Motion
Adding module pass: Warn about non-applied transformations
Adding module pass: Alignment from assumptions
Adding module pass: Strip Unused Function Prototypes
Adding module pass: Dead Global Elimination
Adding module pass: Merge Duplicate Global Constants
Adding module pass: Loop Sink
Adding module pass: Remove redundant instructions
Adding module pass: Hoist/decompose integer division and remainder
Adding module pass: Simplify the CFG
; ModuleID = 'gaussian7x7_hvx64'
source_filename = "/home/maaz/Desktop/Research/rake/halide/src/runtime/qurt_allocator.cpp"
target datalayout = "e-m:e-p:32:32:32-a:0-n16:32-i64:64:64-i32:32:32-i16:16:16-i1:8:8-f32:32:32-f64:64:64-v32:32:32-v64:64:64-v512:512:512-v1024:1024:1024-v2048:2048:2048"
target triple = "hexagon-unknown--elf"

%struct.halide_parallel_task_t = type { i32 (i8*, i32, i32, i8*, i8*)*, i8*, i8*, %struct.halide_semaphore_acquire_t*, i32, i32, i32, i32, i8 }
%struct.halide_semaphore_acquire_t = type { %struct.halide_semaphore_t*, i32 }
%struct.halide_semaphore_t = type { [2 x i64] }
%struct.halide_mutex = type { [1 x i32] }
%struct.halide_device_allocation_pool = type { i32 (i8*)*, %struct.halide_device_allocation_pool* }
%struct.halide_filter_argument_t = type { i8*, i32, i32, %struct.halide_type_t, %struct.halide_scalar_value_t*, %struct.halide_scalar_value_t*, %struct.halide_scalar_value_t*, %struct.halide_scalar_value_t*, i64** }
%struct.halide_type_t = type { i8, i8, i16 }
%struct.halide_scalar_value_t = type { %union.anon }
%union.anon = type { i64 }
%struct.halide_filter_metadata_t = type { i32, i32, %struct.halide_filter_argument_t*, i8*, i8* }
%struct.halide_thread = type opaque
%struct.halide_buffer_t = type { i64, %struct.halide_device_interface_t*, i8*, i64, %struct.halide_type_t, i32, %struct.halide_dimension_t*, i8* }
%struct.halide_device_interface_t = type { i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, void (i8*, %struct.halide_device_interface_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*)*, i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, i32, i32, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, i64, %struct.halide_device_interface_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, i32*, i32*)*, %struct.halide_device_interface_impl_t* }
%struct.halide_device_interface_impl_t = type { void ()*, void ()*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, i32, i32, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, i64)*, i32 (i8*, %struct.halide_buffer_t*)* }
%struct.halide_dimension_t = type { i32, i32, i32, i32 }
%"struct.Halide::Runtime::Internal::device_copy" = type { i64, i64, i64, [16 x i64], [16 x i64], [16 x i64], i64 }
%struct.halide_buffer_t.23 = type { i64, %struct.halide_device_interface_t.20*, i8*, i64, %struct.halide_type_t, i32, %struct.halide_dimension_t*, i8* }
%struct.halide_device_interface_t.20 = type { i32 (i8*, %struct.halide_buffer_t.23*, %struct.halide_device_interface_t.20*)*, {}*, {}*, void (i8*, %struct.halide_device_interface_t.20*)*, {}*, i32 (i8*, %struct.halide_buffer_t.23*, %struct.halide_device_interface_t.20*)*, i32 (i8*, %struct.halide_buffer_t.23*, %struct.halide_device_interface_t.20*)*, {}*, i32 (i8*, %struct.halide_buffer_t.23*, %struct.halide_device_interface_t.20*, %struct.halide_buffer_t.23*)*, i32 (i8*, %struct.halide_buffer_t.23*, %struct.halide_buffer_t.23*)*, i32 (i8*, %struct.halide_buffer_t.23*, i32, i32, %struct.halide_buffer_t.23*)*, {}*, i32 (i8*, %struct.halide_buffer_t.23*, i64, %struct.halide_device_interface_t.20*)*, {}*, i32 (i8*, i32*, i32*)*, %struct.halide_device_interface_impl_t* }
%struct.buffer_t = type { i64, i8*, [4 x i32], [4 x i32], [4 x i32], i32, i8, i8, [6 x i8] }
%"struct.Halide::Runtime::Internal::old_dev_wrapper" = type { i64, %struct.halide_device_interface_t* }
%"struct.Halide::Runtime::Internal::CpuFeatures" = type { [2 x i64], [2 x i64] }

@_ZN6Halide7Runtime8Internal11buf_is_usedE = linkonce global [10 x i32] zeroinitializer, align 4
@_ZN6Halide7Runtime8Internal7mem_bufE = linkonce local_unnamed_addr global [10 x i8*] zeroinitializer, align 4
@_ZN6Halide7Runtime8Internal13custom_mallocE = linkonce local_unnamed_addr global i8* (i8*, i32)* @halide_default_malloc, align 4
@_ZN6Halide7Runtime8Internal11custom_freeE = linkonce local_unnamed_addr global void (i8*, i8*)* @halide_default_free, align 4
@.str = private unnamed_addr constant [45 x i8] c"custom allocators not supported on Hexagon.\0A\00", align 1
@llvm.global_dtors = appending global [1 x { i32, void ()*, i8* }] [{ i32, void ()*, i8* } { i32 65535, void ()* @_ZN6Halide7Runtime8Internal24halide_allocator_cleanupEv, i8* null }]
@_ZN6Halide7Runtime8Internal14custom_do_taskE = linkonce local_unnamed_addr global i32 (i8*, i32 (i8*, i32, i8*)*, i32, i8*)* @halide_default_do_task, align 4
@_ZN6Halide7Runtime8Internal19custom_do_loop_taskE = linkonce local_unnamed_addr global i32 (i8*, i32 (i8*, i32, i32, i8*, i8*)*, i32, i32, i8*, i8*)* @halide_default_do_loop_task, align 4
@_ZN6Halide7Runtime8Internal17custom_do_par_forE = linkonce local_unnamed_addr global i32 (i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*)* @halide_default_do_par_for, align 4
@_ZN6Halide7Runtime8Internal24custom_do_parallel_tasksE = linkonce local_unnamed_addr global i32 (i8*, i32, %struct.halide_parallel_task_t*, i8*)* @halide_default_do_parallel_tasks, align 4
@.str.1 = private unnamed_addr constant [67 x i8] c"halide_default_do_parallel_tasks not implemented on this platform.\00", align 1
@_ZN6Halide7Runtime8Internal21custom_semaphore_initE = linkonce local_unnamed_addr global i32 (%struct.halide_semaphore_t*, i32)* @halide_default_semaphore_init, align 4
@.str.1.2 = private unnamed_addr constant [64 x i8] c"halide_default_semaphore_init not implemented on this platform.\00", align 1
@_ZN6Halide7Runtime8Internal28custom_semaphore_try_acquireE = linkonce local_unnamed_addr global i1 (%struct.halide_semaphore_t*, i32)* @halide_default_semaphore_try_acquire, align 4
@.str.3 = private unnamed_addr constant [71 x i8] c"halide_default_semaphore_try_acquire not implemented on this platform.\00", align 1
@_ZN6Halide7Runtime8Internal24custom_semaphore_releaseE = linkonce local_unnamed_addr global i32 (%struct.halide_semaphore_t*, i32)* @halide_default_semaphore_release, align 4
@.str.2 = private unnamed_addr constant [67 x i8] c"halide_default_semaphore_release not implemented on this platform.\00", align 1
@.str.4 = private unnamed_addr constant [54 x i8] c"halide_spawn_thread not implemented on this platform.\00", align 1
@.str.5 = private unnamed_addr constant [53 x i8] c"halide_join_thread not implemented on this platform.\00", align 1
@.str.6 = private unnamed_addr constant [69 x i8] c"halide_set_num_threads: only supports a value of 1 on this platform.\00", align 1
@_ZN6Halide7Runtime8Internal17halide_gpu_deviceE = linkonce local_unnamed_addr global i32 0, align 4
@_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE = linkonce global i32 0, align 4
@_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE = linkonce local_unnamed_addr global i8 0, align 1
@.str.7 = private unnamed_addr constant [14 x i8] c"HL_GPU_DEVICE\00", align 1
@.str.8 = private unnamed_addr constant [5 x i8] c"-nan\00", align 1
@.str.1.9 = private unnamed_addr constant [4 x i8] c"nan\00", align 1
@.str.2.10 = private unnamed_addr constant [5 x i8] c"-inf\00", align 1
@.str.3.11 = private unnamed_addr constant [4 x i8] c"inf\00", align 1
@.str.4.12 = private unnamed_addr constant [14 x i8] c"-0.000000e+00\00", align 1
@.str.5.13 = private unnamed_addr constant [13 x i8] c"0.000000e+00\00", align 1
@.str.6.14 = private unnamed_addr constant [10 x i8] c"-0.000000\00", align 1
@.str.7.15 = private unnamed_addr constant [9 x i8] c"0.000000\00", align 1
@.str.8.16 = private unnamed_addr constant [2 x i8] c"-\00", align 1
@.str.10 = private unnamed_addr constant [3 x i8] c"e+\00", align 1
@.str.11 = private unnamed_addr constant [3 x i8] c"e-\00", align 1
@.str.12 = private unnamed_addr constant [17 x i8] c"0123456789abcdef\00", align 1
@.str.17 = private unnamed_addr constant [14 x i8] c"bad_type_code\00", align 1
@.str.16 = private unnamed_addr constant [7 x i8] c"handle\00", align 1
@.str.15 = private unnamed_addr constant [6 x i8] c"float\00", align 1
@.str.14 = private unnamed_addr constant [5 x i8] c"uint\00", align 1
@.str.13 = private unnamed_addr constant [4 x i8] c"int\00", align 1
@.str.18 = private unnamed_addr constant [2 x i8] c"x\00", align 1
@.str.19 = private unnamed_addr constant [5 x i8] c"NULL\00", align 1
@.str.20 = private unnamed_addr constant [8 x i8] c"buffer(\00", align 1
@.str.22 = private unnamed_addr constant [4 x i8] c", {\00", align 1
@.str.23 = private unnamed_addr constant [2 x i8] c"}\00", align 1
@_ZN6Halide7Runtime8Internal36halide_reuse_device_allocations_flagE = linkonce local_unnamed_addr global i8 0, align 1
@_ZN6Halide7Runtime8Internal21allocation_pools_lockE = linkonce global %struct.halide_mutex zeroinitializer, align 4
@_ZN6Halide7Runtime8Internal23device_allocation_poolsE = linkonce local_unnamed_addr global %struct.halide_device_allocation_pool* null, align 4
@_ZN6Halide7Runtime8Internal17device_copy_mutexE = linkonce global %struct.halide_mutex zeroinitializer, align 4
@.str.9.17 = private unnamed_addr constant [20 x i8] c"halide_copy_to_host\00", align 1
@.str.10.18 = private unnamed_addr constant [22 x i8] c"halide_copy_to_device\00", align 1
@.str.12.19 = private unnamed_addr constant [61 x i8] c"halide_copy_to_device does not support switching interfaces\0A\00", align 1
@.str.18.20 = private unnamed_addr constant [21 x i8] c"halide_device_malloc\00", align 1
@.str.20.21 = private unnamed_addr constant [59 x i8] c"halide_device_malloc doesn't support switching interfaces\0A\00", align 1
@.str.17.22 = private unnamed_addr constant [19 x i8] c"halide_device_sync\00", align 1
@.str.21.23 = private unnamed_addr constant [19 x i8] c"halide_device_free\00", align 1
@.str.22.24 = private unnamed_addr constant [110 x i8] c"/home/maaz/Desktop/Research/rake/halide/src/runtime/device_interface.cpp:247 Assert failed: buf->device == 0\0A\00", align 1
@.str.23.25 = private unnamed_addr constant [30 x i8] c"halide_device_and_host_malloc\00", align 1
@.str.25 = private unnamed_addr constant [68 x i8] c"halide_device_and_host_malloc doesn't support switching interfaces\0A\00", align 1
@.str.26 = private unnamed_addr constant [42 x i8] c"allocating host and device memory failed\0A\00", align 1
@.str.27 = private unnamed_addr constant [28 x i8] c"halide_device_and_host_free\00", align 1
@.str.28 = private unnamed_addr constant [110 x i8] c"/home/maaz/Desktop/Research/rake/halide/src/runtime/device_interface.cpp:312 Assert failed: buf->device == 0\0A\00", align 1
@.str.29 = private unnamed_addr constant [38 x i8] c"halide_default_device_and_host_malloc\00", align 1
@.str.30 = private unnamed_addr constant [36 x i8] c"halide_default_device_and_host_free\00", align 1
@.str.31 = private unnamed_addr constant [26 x i8] c"halide_device_wrap_native\00", align 1
@.str.32 = private unnamed_addr constant [64 x i8] c"halide_device_wrap_native doesn't support switching interfaces\0A\00", align 1
@.str.33 = private unnamed_addr constant [28 x i8] c"halide_device_detach_native\00", align 1
@.str.34 = private unnamed_addr constant [110 x i8] c"/home/maaz/Desktop/Research/rake/halide/src/runtime/device_interface.cpp:399 Assert failed: buf->device == 0\0A\00", align 1
@.str.35 = private unnamed_addr constant [34 x i8] c"halide_default_device_wrap_native\00", align 1
@.str.36 = private unnamed_addr constant [36 x i8] c"halide_default_device_detach_native\00", align 1
@.str.42 = private unnamed_addr constant [64 x i8] c"halide_buffer_copy does not support switching device interfaces\00", align 1
@.str.58 = private unnamed_addr constant [44 x i8] c"device_interface does not support cropping\0A\00", align 1
@.str.59 = private unnamed_addr constant [43 x i8] c"device_interface does not support slicing\0A\00", align 1
@.str.60 = private unnamed_addr constant [52 x i8] c"destination buffer already has a device allocation\0A\00", align 1
@.str.61 = private unnamed_addr constant [48 x i8] c"src and dst must have identical dimensionality\0A\00", align 1
@.str.64 = private unnamed_addr constant [52 x i8] c"dst must have exactly one fewer dimension than src\0A\00", align 1
@.str.37 = private unnamed_addr constant [41 x i8] c"Bounds inference call to external stage \00", align 1
@.str.75 = private unnamed_addr constant [7 x i8] c"<NULL>\00", align 1
@.str.1.38 = private unnamed_addr constant [27 x i8] c" returned non-zero value: \00", align 1
@.str.2.39 = private unnamed_addr constant [24 x i8] c"Call to external stage \00", align 1
@.str.3.40 = private unnamed_addr constant [18 x i8] c"Bounds given for \00", align 1
@.str.4.41 = private unnamed_addr constant [5 x i8] c" in \00", align 1
@.str.5.42 = private unnamed_addr constant [8 x i8] c" (from \00", align 1
@.str.6.43 = private unnamed_addr constant [5 x i8] c" to \00", align 1
@.str.7.44 = private unnamed_addr constant [38 x i8] c") do not cover required region (from \00", align 1
@.str.9.46 = private unnamed_addr constant [11 x i8] c" has type \00", align 1
@.str.10.47 = private unnamed_addr constant [38 x i8] c" but type of the buffer passed in is \00", align 1
@.str.11.48 = private unnamed_addr constant [31 x i8] c" requires a buffer of exactly \00", align 1
@.str.12.49 = private unnamed_addr constant [43 x i8] c" dimensions, but the buffer passed in has \00", align 1
@.str.13.50 = private unnamed_addr constant [12 x i8] c" dimensions\00", align 1
@.str.14.51 = private unnamed_addr constant [17 x i8] c" is accessed at \00", align 1
@.str.15.52 = private unnamed_addr constant [28 x i8] c", which is before the min (\00", align 1
@.str.16.53 = private unnamed_addr constant [16 x i8] c") in dimension \00", align 1
@.str.17.54 = private unnamed_addr constant [28 x i8] c", which is beyond the max (\00", align 1
@.str.18.55 = private unnamed_addr constant [29 x i8] c"Total allocation for buffer \00", align 1
@.str.19.56 = private unnamed_addr constant [5 x i8] c" is \00", align 1
@.str.20.57 = private unnamed_addr constant [37 x i8] c", which exceeds the maximum size of \00", align 1
@.str.21.58 = private unnamed_addr constant [24 x i8] c"The extents for buffer \00", align 1
@.str.22.59 = private unnamed_addr constant [12 x i8] c" dimension \00", align 1
@.str.23.60 = private unnamed_addr constant [15 x i8] c" is negative (\00", align 1
@.str.24.61 = private unnamed_addr constant [31 x i8] c"Product of extents for buffer \00", align 1
@.str.25.62 = private unnamed_addr constant [29 x i8] c"Applying the constraints on \00", align 1
@.str.26.63 = private unnamed_addr constant [54 x i8] c" to the required region made it smaller in dimension \00", align 1
@.str.27.64 = private unnamed_addr constant [3 x i8] c". \00", align 1
@.str.28.65 = private unnamed_addr constant [16 x i8] c"Required size: \00", align 1
@.str.29.66 = private unnamed_addr constant [19 x i8] c"Constrained size: \00", align 1
@.str.30.67 = private unnamed_addr constant [2 x i8] c".\00", align 1
@.str.31.68 = private unnamed_addr constant [22 x i8] c"Constraint violated: \00", align 1
@.str.32.69 = private unnamed_addr constant [3 x i8] c" (\00", align 1
@.str.33.70 = private unnamed_addr constant [6 x i8] c") == \00", align 1
@.str.34.71 = private unnamed_addr constant [11 x i8] c"Parameter \00", align 1
@.str.35.72 = private unnamed_addr constant [23 x i8] c" but must be at least \00", align 1
@.str.36.73 = private unnamed_addr constant [22 x i8] c" but must be at most \00", align 1
@.str.37.74 = private unnamed_addr constant [44 x i8] c"Out of memory (halide_malloc returned NULL)\00", align 1
@.str.38 = private unnamed_addr constant [17 x i8] c"Buffer argument \00", align 1
@.str.39 = private unnamed_addr constant [9 x i8] c" is NULL\00", align 1
@.str.40 = private unnamed_addr constant [25 x i8] c"Failed to dump function \00", align 1
@.str.41 = private unnamed_addr constant [10 x i8] c" to file \00", align 1
@.str.42.75 = private unnamed_addr constant [13 x i8] c" with error \00", align 1
@.str.43 = private unnamed_addr constant [51 x i8] c"Failed to upgrade buffer_t to halide_buffer_t for \00", align 1
@.str.44 = private unnamed_addr constant [3 x i8] c": \00", align 1
@.str.45 = private unnamed_addr constant [53 x i8] c"Failed to downgrade halide_buffer_t to buffer_t for \00", align 1
@.str.46 = private unnamed_addr constant [21 x i8] c"The host pointer of \00", align 1
@.str.47 = private unnamed_addr constant [22 x i8] c" is not aligned to a \00", align 1
@.str.48 = private unnamed_addr constant [17 x i8] c" bytes boundary.\00", align 1
@.str.49 = private unnamed_addr constant [55 x i8] c" is null, but the pipeline will access it on the host.\00", align 1
@.str.50 = private unnamed_addr constant [30 x i8] c"The folded storage dimension \00", align 1
@.str.51 = private unnamed_addr constant [5 x i8] c" of \00", align 1
@.str.52 = private unnamed_addr constant [36 x i8] c" was accessed out of order by loop \00", align 1
@.str.53 = private unnamed_addr constant [23 x i8] c"Cannot fold dimension \00", align 1
@.str.54 = private unnamed_addr constant [36 x i8] c" because an extern stage accesses [\00", align 1
@.str.55 = private unnamed_addr constant [3 x i8] c", \00", align 1
@.str.56 = private unnamed_addr constant [3 x i8] c"],\00", align 1
@.str.57 = private unnamed_addr constant [47 x i8] c" which is outside the range currently valid: [\00", align 1
@.str.58.76 = private unnamed_addr constant [3 x i8] c"].\00", align 1
@.str.59.77 = private unnamed_addr constant [47 x i8] c" which wraps around the boundary of the fold, \00", align 1
@.str.60.78 = private unnamed_addr constant [30 x i8] c"which occurs at multiples of \00", align 1
@.str.61.79 = private unnamed_addr constant [18 x i8] c"The fold factor (\00", align 1
@.str.62 = private unnamed_addr constant [16 x i8] c") of dimension \00", align 1
@.str.63 = private unnamed_addr constant [61 x i8] c" is too small to store the required region accessed by loop \00", align 1
@.str.64.80 = private unnamed_addr constant [3 x i8] c").\00", align 1
@.str.65 = private unnamed_addr constant [22 x i8] c"Requirement Failed: (\00", align 1
@.str.67 = private unnamed_addr constant [59 x i8] c"A schedule specialized with specialize_fail() was chosen: \00", align 1
@.str.68 = private unnamed_addr constant [55 x i8] c"Buffer has a non-zero device but no device interface.\0A\00", align 1
@.str.69 = private unnamed_addr constant [56 x i8] c"Buffer has a non-null devie_interface but device is 0.\0A\00", align 1
@.str.70 = private unnamed_addr constant [49 x i8] c"Buffer has both host and device dirty bits set.\0A\00", align 1
@.str.71 = private unnamed_addr constant [26 x i8] c"Buffer pointer passed to \00", align 1
@.str.72 = private unnamed_addr constant [11 x i8] c" is null.\0A\00", align 1
@.str.73 = private unnamed_addr constant [37 x i8] c"Integer division or modulo by zero.\0A\00", align 1
@.str.89 = private unnamed_addr constant [1 x i8] zeroinitializer, align 1
@.str.1.90 = private unnamed_addr constant [44 x i8] c"elem_size of buffer was not in [1, 2, 4, 8]\00", align 1
@.str.2.91 = private unnamed_addr constant [33 x i8] c"buffer has incorrect elem_size (\00", align 1
@.str.3.92 = private unnamed_addr constant [3 x i8] c") \00", align 1
@.str.4.93 = private unnamed_addr constant [20 x i8] c"for expected type (\00", align 1
@.str.5.94 = private unnamed_addr constant [2 x i8] c")\00", align 1
@.str.6.96 = private unnamed_addr constant [65 x i8] c"Internal error: buffer host mismatch in halide_upgrade_buffer_t.\00", align 1
@.str.7.97 = private unnamed_addr constant [37 x i8] c"buffer has more than four dimensions\00", align 1
@.str.102 = private unnamed_addr constant [34 x i8] c"HVX lock size must be 64 or 128.\0A\00", align 1
@.str.8.103 = private unnamed_addr constant [35 x i8] c"Printer buffer allocation failed.\0A\00", align 1
@.str.5.104 = private unnamed_addr constant [22 x i8] c"qurt_hvx_lock failed\0A\00", align 1
@.str.7.105 = private unnamed_addr constant [24 x i8] c"qurt_hvx_unlock failed\0A\00", align 1
@_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE = linkonce local_unnamed_addr global i32 (i32, i64*)* @halide_default_can_use_target_features, align 4
@_ZN6Halide7Runtime8Internal36halide_cpu_features_initialized_lockE = linkonce global %struct.halide_mutex zeroinitializer, align 4
@_ZN6Halide7Runtime8Internal31halide_cpu_features_initializedE = linkonce local_unnamed_addr global i8 0, align 1
@_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE = linkonce global [4 x i64] zeroinitializer, align 8
@.str.106 = private unnamed_addr constant [81 x i8] c"Internal error: wrong structure size passed to halide_can_use_target_features()\0A\00", align 1
@0 = private constant i64 0
@1 = private constant i64 0
@2 = private constant [4 x i64*] [i64* @0, i64* null, i64* @1, i64* null]
@str = private constant [6 x i8] c"input\00", align 32
@3 = private constant i64 0
@4 = private constant i64 0
@5 = private constant [4 x i64*] [i64* @3, i64* null, i64* @4, i64* null]
@str.112 = private constant [7 x i8] c"output\00", align 32
@6 = private constant [2 x %struct.halide_filter_argument_t] [%struct.halide_filter_argument_t { i8* getelementptr inbounds ([6 x i8], [6 x i8]* @str, i32 0, i32 0), i32 1, i32 2, %struct.halide_type_t { i8 1, i8 8, i16 1 }, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, i64** getelementptr inbounds ([4 x i64*], [4 x i64*]* @2, i32 0, i32 0) }, %struct.halide_filter_argument_t { i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.112, i32 0, i32 0), i32 2, i32 2, %struct.halide_type_t { i8 1, i8 8, i16 1 }, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, %struct.halide_scalar_value_t* null, i64** getelementptr inbounds ([4 x i64*], [4 x i64*]* @5, i32 0, i32 0) }]
@str.113 = private constant [50 x i8] c"hexagon-32-noos-hvx_64-no_asserts-no_bounds_query\00", align 64
@str.114 = private constant [18 x i8] c"gaussian7x7_hvx64\00", align 32
@gaussian7x7_hvx64_metadata_storage = private constant %struct.halide_filter_metadata_t { i32 1, i32 2, %struct.halide_filter_argument_t* getelementptr inbounds ([2 x %struct.halide_filter_argument_t], [2 x %struct.halide_filter_argument_t]* @6, i32 0, i32 0), i8* getelementptr inbounds ([50 x i8], [50 x i8]* @str.113, i32 0, i32 0), i8* getelementptr inbounds ([18 x i8], [18 x i8]* @str.114, i32 0, i32 0) }
@switch.table.halide_type_to_string = private unnamed_addr constant [4 x i8*] [i8* getelementptr inbounds ([4 x i8], [4 x i8]* @.str.13, i32 0, i32 0), i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.14, i32 0, i32 0), i8* getelementptr inbounds ([6 x i8], [6 x i8]* @.str.15, i32 0, i32 0), i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.16, i32 0, i32 0)], align 4

; Function Attrs: nounwind
define linkonce i8* @_ZN6Halide7Runtime8Internal14aligned_mallocEjj(i32, i32) local_unnamed_addr #0 {
  %3 = add i32 %0, -1
  %4 = add i32 %3, %1
  %5 = sub i32 0, %0
  %6 = and i32 %4, %5
  %7 = add i32 %6, %0
  %8 = tail call i8* @malloc(i32 %7) #11
  %9 = icmp eq i8* %8, null
  br i1 %9, label %18, label %10

; <label>:10:                                     ; preds = %2
  %11 = ptrtoint i8* %8 to i32
  %12 = add i32 %0, 3
  %13 = add i32 %12, %11
  %14 = and i32 %13, %5
  %15 = inttoptr i32 %14 to i8*
  %16 = inttoptr i32 %14 to i8**
  %17 = getelementptr inbounds i8*, i8** %16, i32 -1
  store i8* %8, i8** %17, align 4, !tbaa !8
  br label %18

; <label>:18:                                     ; preds = %10, %2
  %19 = phi i8* [ %15, %10 ], [ null, %2 ]
  ret i8* %19
}

; Function Attrs: nounwind
declare noalias i8* @malloc(i32) local_unnamed_addr #1

; Function Attrs: nounwind
define linkonce void @_ZN6Halide7Runtime8Internal12aligned_freeEPv(i8*) local_unnamed_addr #0 {
  %2 = icmp eq i8* %0, null
  br i1 %2, label %7, label %3

; <label>:3:                                      ; preds = %1
  %4 = getelementptr inbounds i8, i8* %0, i32 -4
  %5 = bitcast i8* %4 to i8**
  %6 = load i8*, i8** %5, align 4, !tbaa !8
  tail call void @free(i8* %6) #11
  br label %7

; <label>:7:                                      ; preds = %3, %1
  ret void
}

; Function Attrs: nounwind
declare void @free(i8* nocapture) local_unnamed_addr #1

; Function Attrs: nounwind
define linkonce void @_ZN6Halide7Runtime8Internal24halide_allocator_cleanupEv() #0 {
  %1 = load i8*, i8** getelementptr inbounds ([10 x i8*], [10 x i8*]* @_ZN6Halide7Runtime8Internal7mem_bufE, i32 0, i32 0), align 4, !tbaa !8
  tail call void @_ZN6Halide7Runtime8Internal12aligned_freeEPv(i8* %1) #12
  %2 = load i8*, i8** getelementptr inbounds ([10 x i8*], [10 x i8*]* @_ZN6Halide7Runtime8Internal7mem_bufE, i32 0, i32 1), align 4, !tbaa !8
  tail call void @_ZN6Halide7Runtime8Internal12aligned_freeEPv(i8* %2) #12
  %3 = load i8*, i8** getelementptr inbounds ([10 x i8*], [10 x i8*]* @_ZN6Halide7Runtime8Internal7mem_bufE, i32 0, i32 2), align 4, !tbaa !8
  tail call void @_ZN6Halide7Runtime8Internal12aligned_freeEPv(i8* %3) #12
  %4 = load i8*, i8** getelementptr inbounds ([10 x i8*], [10 x i8*]* @_ZN6Halide7Runtime8Internal7mem_bufE, i32 0, i32 3), align 4, !tbaa !8
  tail call void @_ZN6Halide7Runtime8Internal12aligned_freeEPv(i8* %4) #12
  %5 = load i8*, i8** getelementptr inbounds ([10 x i8*], [10 x i8*]* @_ZN6Halide7Runtime8Internal7mem_bufE, i32 0, i32 4), align 4, !tbaa !8
  tail call void @_ZN6Halide7Runtime8Internal12aligned_freeEPv(i8* %5) #12
  %6 = load i8*, i8** getelementptr inbounds ([10 x i8*], [10 x i8*]* @_ZN6Halide7Runtime8Internal7mem_bufE, i32 0, i32 5), align 4, !tbaa !8
  tail call void @_ZN6Halide7Runtime8Internal12aligned_freeEPv(i8* %6) #12
  %7 = load i8*, i8** getelementptr inbounds ([10 x i8*], [10 x i8*]* @_ZN6Halide7Runtime8Internal7mem_bufE, i32 0, i32 6), align 4, !tbaa !8
  tail call void @_ZN6Halide7Runtime8Internal12aligned_freeEPv(i8* %7) #12
  %8 = load i8*, i8** getelementptr inbounds ([10 x i8*], [10 x i8*]* @_ZN6Halide7Runtime8Internal7mem_bufE, i32 0, i32 7), align 4, !tbaa !8
  tail call void @_ZN6Halide7Runtime8Internal12aligned_freeEPv(i8* %8) #12
  %9 = load i8*, i8** getelementptr inbounds ([10 x i8*], [10 x i8*]* @_ZN6Halide7Runtime8Internal7mem_bufE, i32 0, i32 8), align 4, !tbaa !8
  tail call void @_ZN6Halide7Runtime8Internal12aligned_freeEPv(i8* %9) #12
  %10 = load i8*, i8** getelementptr inbounds ([10 x i8*], [10 x i8*]* @_ZN6Halide7Runtime8Internal7mem_bufE, i32 0, i32 9), align 4, !tbaa !8
  tail call void @_ZN6Halide7Runtime8Internal12aligned_freeEPv(i8* %10) #12
  ret void
}

; Function Attrs: nounwind
define weak i8* @halide_default_malloc(i8*, i32) #0 {
  %3 = icmp ult i32 %1, 65537
  br i1 %3, label %4, label %17

; <label>:4:                                      ; preds = %2
  %5 = cmpxchg i32* getelementptr inbounds ([10 x i32], [10 x i32]* @_ZN6Halide7Runtime8Internal11buf_is_usedE, i32 0, i32 0), i32 0, i32 1 seq_cst seq_cst
  %6 = extractvalue { i32, i1 } %5, 1
  br i1 %6, label %7, label %14

; <label>:7:                                      ; preds = %42, %39, %36, %33, %30, %27, %24, %21, %14, %4
  %8 = phi i32 [ 0, %4 ], [ 1, %14 ], [ 2, %21 ], [ 3, %24 ], [ 4, %27 ], [ 5, %30 ], [ 6, %33 ], [ 7, %36 ], [ 8, %39 ], [ 9, %42 ]
  %9 = getelementptr inbounds [10 x i8*], [10 x i8*]* @_ZN6Halide7Runtime8Internal7mem_bufE, i32 0, i32 %8
  %10 = load i8*, i8** %9, align 4, !tbaa !8
  %11 = icmp eq i8* %10, null
  br i1 %11, label %12, label %19

; <label>:12:                                     ; preds = %7
  %13 = tail call i8* @_ZN6Halide7Runtime8Internal14aligned_mallocEjj(i32 128, i32 65536) #12
  store i8* %13, i8** %9, align 4, !tbaa !8
  br label %19

; <label>:14:                                     ; preds = %4
  %15 = cmpxchg i32* getelementptr inbounds ([10 x i32], [10 x i32]* @_ZN6Halide7Runtime8Internal11buf_is_usedE, i32 0, i32 1), i32 0, i32 1 seq_cst seq_cst
  %16 = extractvalue { i32, i1 } %15, 1
  br i1 %16, label %7, label %21

; <label>:17:                                     ; preds = %42, %2
  %18 = tail call i8* @_ZN6Halide7Runtime8Internal14aligned_mallocEjj(i32 128, i32 %1) #12
  br label %19

; <label>:19:                                     ; preds = %17, %12, %7
  %20 = phi i8* [ %18, %17 ], [ %10, %7 ], [ %13, %12 ]
  ret i8* %20

; <label>:21:                                     ; preds = %14
  %22 = cmpxchg i32* getelementptr inbounds ([10 x i32], [10 x i32]* @_ZN6Halide7Runtime8Internal11buf_is_usedE, i32 0, i32 2), i32 0, i32 1 seq_cst seq_cst
  %23 = extractvalue { i32, i1 } %22, 1
  br i1 %23, label %7, label %24

; <label>:24:                                     ; preds = %21
  %25 = cmpxchg i32* getelementptr inbounds ([10 x i32], [10 x i32]* @_ZN6Halide7Runtime8Internal11buf_is_usedE, i32 0, i32 3), i32 0, i32 1 seq_cst seq_cst
  %26 = extractvalue { i32, i1 } %25, 1
  br i1 %26, label %7, label %27

; <label>:27:                                     ; preds = %24
  %28 = cmpxchg i32* getelementptr inbounds ([10 x i32], [10 x i32]* @_ZN6Halide7Runtime8Internal11buf_is_usedE, i32 0, i32 4), i32 0, i32 1 seq_cst seq_cst
  %29 = extractvalue { i32, i1 } %28, 1
  br i1 %29, label %7, label %30

; <label>:30:                                     ; preds = %27
  %31 = cmpxchg i32* getelementptr inbounds ([10 x i32], [10 x i32]* @_ZN6Halide7Runtime8Internal11buf_is_usedE, i32 0, i32 5), i32 0, i32 1 seq_cst seq_cst
  %32 = extractvalue { i32, i1 } %31, 1
  br i1 %32, label %7, label %33

; <label>:33:                                     ; preds = %30
  %34 = cmpxchg i32* getelementptr inbounds ([10 x i32], [10 x i32]* @_ZN6Halide7Runtime8Internal11buf_is_usedE, i32 0, i32 6), i32 0, i32 1 seq_cst seq_cst
  %35 = extractvalue { i32, i1 } %34, 1
  br i1 %35, label %7, label %36

; <label>:36:                                     ; preds = %33
  %37 = cmpxchg i32* getelementptr inbounds ([10 x i32], [10 x i32]* @_ZN6Halide7Runtime8Internal11buf_is_usedE, i32 0, i32 7), i32 0, i32 1 seq_cst seq_cst
  %38 = extractvalue { i32, i1 } %37, 1
  br i1 %38, label %7, label %39

; <label>:39:                                     ; preds = %36
  %40 = cmpxchg i32* getelementptr inbounds ([10 x i32], [10 x i32]* @_ZN6Halide7Runtime8Internal11buf_is_usedE, i32 0, i32 8), i32 0, i32 1 seq_cst seq_cst
  %41 = extractvalue { i32, i1 } %40, 1
  br i1 %41, label %7, label %42

; <label>:42:                                     ; preds = %39
  %43 = cmpxchg i32* getelementptr inbounds ([10 x i32], [10 x i32]* @_ZN6Halide7Runtime8Internal11buf_is_usedE, i32 0, i32 9), i32 0, i32 1 seq_cst seq_cst
  %44 = extractvalue { i32, i1 } %43, 1
  br i1 %44, label %7, label %17
}

; Function Attrs: nounwind
define weak void @halide_default_free(i8*, i8*) #0 {
  %3 = load i8*, i8** getelementptr inbounds ([10 x i8*], [10 x i8*]* @_ZN6Halide7Runtime8Internal7mem_bufE, i32 0, i32 0), align 4, !tbaa !8
  %4 = icmp eq i8* %3, %1
  br i1 %4, label %5, label %8

; <label>:5:                                      ; preds = %33, %30, %27, %24, %21, %18, %15, %12, %8, %2
  %6 = phi i32 [ 0, %2 ], [ 1, %8 ], [ 2, %12 ], [ 3, %15 ], [ 4, %18 ], [ 5, %21 ], [ 6, %24 ], [ 7, %27 ], [ 8, %30 ], [ 9, %33 ]
  %7 = getelementptr inbounds [10 x i32], [10 x i32]* @_ZN6Halide7Runtime8Internal11buf_is_usedE, i32 0, i32 %6
  store i32 0, i32* %7, align 4, !tbaa !12
  br label %11

; <label>:8:                                      ; preds = %2
  %9 = load i8*, i8** getelementptr inbounds ([10 x i8*], [10 x i8*]* @_ZN6Halide7Runtime8Internal7mem_bufE, i32 0, i32 1), align 4, !tbaa !8
  %10 = icmp eq i8* %9, %1
  br i1 %10, label %5, label %12

; <label>:11:                                     ; preds = %36, %5
  ret void

; <label>:12:                                     ; preds = %8
  %13 = load i8*, i8** getelementptr inbounds ([10 x i8*], [10 x i8*]* @_ZN6Halide7Runtime8Internal7mem_bufE, i32 0, i32 2), align 4, !tbaa !8
  %14 = icmp eq i8* %13, %1
  br i1 %14, label %5, label %15

; <label>:15:                                     ; preds = %12
  %16 = load i8*, i8** getelementptr inbounds ([10 x i8*], [10 x i8*]* @_ZN6Halide7Runtime8Internal7mem_bufE, i32 0, i32 3), align 4, !tbaa !8
  %17 = icmp eq i8* %16, %1
  br i1 %17, label %5, label %18

; <label>:18:                                     ; preds = %15
  %19 = load i8*, i8** getelementptr inbounds ([10 x i8*], [10 x i8*]* @_ZN6Halide7Runtime8Internal7mem_bufE, i32 0, i32 4), align 4, !tbaa !8
  %20 = icmp eq i8* %19, %1
  br i1 %20, label %5, label %21

; <label>:21:                                     ; preds = %18
  %22 = load i8*, i8** getelementptr inbounds ([10 x i8*], [10 x i8*]* @_ZN6Halide7Runtime8Internal7mem_bufE, i32 0, i32 5), align 4, !tbaa !8
  %23 = icmp eq i8* %22, %1
  br i1 %23, label %5, label %24

; <label>:24:                                     ; preds = %21
  %25 = load i8*, i8** getelementptr inbounds ([10 x i8*], [10 x i8*]* @_ZN6Halide7Runtime8Internal7mem_bufE, i32 0, i32 6), align 4, !tbaa !8
  %26 = icmp eq i8* %25, %1
  br i1 %26, label %5, label %27

; <label>:27:                                     ; preds = %24
  %28 = load i8*, i8** getelementptr inbounds ([10 x i8*], [10 x i8*]* @_ZN6Halide7Runtime8Internal7mem_bufE, i32 0, i32 7), align 4, !tbaa !8
  %29 = icmp eq i8* %28, %1
  br i1 %29, label %5, label %30

; <label>:30:                                     ; preds = %27
  %31 = load i8*, i8** getelementptr inbounds ([10 x i8*], [10 x i8*]* @_ZN6Halide7Runtime8Internal7mem_bufE, i32 0, i32 8), align 4, !tbaa !8
  %32 = icmp eq i8* %31, %1
  br i1 %32, label %5, label %33

; <label>:33:                                     ; preds = %30
  %34 = load i8*, i8** getelementptr inbounds ([10 x i8*], [10 x i8*]* @_ZN6Halide7Runtime8Internal7mem_bufE, i32 0, i32 9), align 4, !tbaa !8
  %35 = icmp eq i8* %34, %1
  br i1 %35, label %5, label %36

; <label>:36:                                     ; preds = %33
  tail call void @_ZN6Halide7Runtime8Internal12aligned_freeEPv(i8* %1) #12
  br label %11
}

; Function Attrs: nounwind
define weak i8* (i8*, i32)* @halide_set_custom_malloc(i8* (i8*, i32)*) local_unnamed_addr #0 {
  tail call void @halide_print(i8* null, i8* getelementptr inbounds ([45 x i8], [45 x i8]* @.str, i32 0, i32 0)) #11
  %2 = load i8* (i8*, i32)*, i8* (i8*, i32)** @_ZN6Halide7Runtime8Internal13custom_mallocE, align 4, !tbaa !8
  store i8* (i8*, i32)* %0, i8* (i8*, i32)** @_ZN6Halide7Runtime8Internal13custom_mallocE, align 4, !tbaa !8
  ret i8* (i8*, i32)* %2
}

declare extern_weak void @halide_print(i8*, i8*) local_unnamed_addr #2

; Function Attrs: nounwind
define weak void (i8*, i8*)* @halide_set_custom_free(void (i8*, i8*)*) local_unnamed_addr #0 {
  tail call void @halide_print(i8* null, i8* getelementptr inbounds ([45 x i8], [45 x i8]* @.str, i32 0, i32 0)) #11
  %2 = load void (i8*, i8*)*, void (i8*, i8*)** @_ZN6Halide7Runtime8Internal11custom_freeE, align 4, !tbaa !8
  store void (i8*, i8*)* %0, void (i8*, i8*)** @_ZN6Halide7Runtime8Internal11custom_freeE, align 4, !tbaa !8
  ret void (i8*, i8*)* %2
}

; Function Attrs: nounwind
define weak noalias i8* @halide_malloc(i8*, i32) local_unnamed_addr #0 {
  %3 = tail call i8* @halide_default_malloc(i8* %0, i32 %1) #12
  ret i8* %3
}

; Function Attrs: nounwind
define weak void @halide_free(i8*, i8*) local_unnamed_addr #0 {
  tail call void @halide_default_free(i8* %0, i8* %1) #12
  ret void
}

; Function Attrs: nounwind
define weak i32 @halide_default_do_task(i8*, i32 (i8*, i32, i8*)*, i32, i8*) #0 {
  %5 = tail call i32 %1(i8* %0, i32 %2, i8* %3) #11
  ret i32 %5
}

; Function Attrs: nounwind
define weak i32 @halide_default_do_loop_task(i8*, i32 (i8*, i32, i32, i8*, i8*)*, i32, i32, i8*, i8*) #0 {
  %7 = tail call i32 %1(i8* %0, i32 %2, i32 %3, i8* %4, i8* %5) #11
  ret i32 %7
}

; Function Attrs: nounwind
define weak i32 @halide_default_do_par_for(i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*) #0 {
  %6 = add nsw i32 %3, %2
  %7 = icmp sgt i32 %3, 0
  br i1 %7, label %.preheader, label %.loopexit

; <label>:8:                                      ; preds = %.preheader
  %9 = icmp slt i32 %13, %6
  br i1 %9, label %.preheader, label %.loopexit

.preheader:                                       ; preds = %5, %8
  %10 = phi i32 [ %13, %8 ], [ %2, %5 ]
  %11 = tail call i32 @halide_do_task(i8* %0, i32 (i8*, i32, i8*)* %1, i32 %10, i8* %4) #12
  %12 = icmp eq i32 %11, 0
  %13 = add nsw i32 %10, 1
  br i1 %12, label %8, label %.loopexit

.loopexit:                                        ; preds = %.preheader, %8, %5
  %14 = phi i32 [ 0, %5 ], [ %11, %.preheader ], [ 0, %8 ]
  ret i32 %14
}

; Function Attrs: nounwind
define weak i32 @halide_do_task(i8*, i32 (i8*, i32, i8*)*, i32, i8*) local_unnamed_addr #0 {
  %5 = load i32 (i8*, i32 (i8*, i32, i8*)*, i32, i8*)*, i32 (i8*, i32 (i8*, i32, i8*)*, i32, i8*)** @_ZN6Halide7Runtime8Internal14custom_do_taskE, align 4, !tbaa !8
  %6 = tail call i32 %5(i8* %0, i32 (i8*, i32, i8*)* %1, i32 %2, i8* %3) #11
  ret i32 %6
}

; Function Attrs: nounwind
define weak i32 @halide_default_do_parallel_tasks(i8*, i32, %struct.halide_parallel_task_t*, i8*) #0 {
  tail call void @halide_error(i8* null, i8* getelementptr inbounds ([67 x i8], [67 x i8]* @.str.1, i32 0, i32 0)) #11
  ret i32 -1
}

declare extern_weak void @halide_error(i8*, i8*) local_unnamed_addr #2

; Function Attrs: nounwind
define weak i32 @halide_default_semaphore_init(%struct.halide_semaphore_t*, i32) #0 {
  tail call void @halide_error(i8* null, i8* getelementptr inbounds ([64 x i8], [64 x i8]* @.str.1.2, i32 0, i32 0)) #11
  ret i32 0
}

; Function Attrs: nounwind
define weak zeroext i1 @halide_default_semaphore_try_acquire(%struct.halide_semaphore_t*, i32) #0 {
  tail call void @halide_error(i8* null, i8* getelementptr inbounds ([71 x i8], [71 x i8]* @.str.3, i32 0, i32 0)) #11
  ret i1 false
}

; Function Attrs: nounwind
define weak i32 @halide_default_semaphore_release(%struct.halide_semaphore_t*, i32) #0 {
  tail call void @halide_error(i8* null, i8* getelementptr inbounds ([67 x i8], [67 x i8]* @.str.2, i32 0, i32 0)) #11
  ret i32 0
}

; Function Attrs: nounwind
define weak %struct.halide_thread* @halide_spawn_thread(void (i8*)*, i8*) local_unnamed_addr #0 {
  tail call void @halide_error(i8* null, i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.4, i32 0, i32 0)) #11
  ret %struct.halide_thread* null
}

; Function Attrs: nounwind
define weak void @halide_join_thread(%struct.halide_thread*) local_unnamed_addr #0 {
  tail call void @halide_error(i8* null, i8* getelementptr inbounds ([53 x i8], [53 x i8]* @.str.5, i32 0, i32 0)) #11
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @halide_mutex_lock(%struct.halide_mutex*) local_unnamed_addr #3 {
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @halide_mutex_unlock(%struct.halide_mutex*) local_unnamed_addr #3 {
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @halide_shutdown_thread_pool() local_unnamed_addr #3 {
  ret void
}

; Function Attrs: nounwind
define weak i32 @halide_set_num_threads(i32) local_unnamed_addr #0 {
  %2 = icmp eq i32 %0, 1
  br i1 %2, label %4, label %3

; <label>:3:                                      ; preds = %1
  tail call void @halide_error(i8* null, i8* getelementptr inbounds ([69 x i8], [69 x i8]* @.str.6, i32 0, i32 0)) #11
  br label %4

; <label>:4:                                      ; preds = %3, %1
  ret i32 1
}

; Function Attrs: norecurse nounwind
define weak i32 (i8*, i32 (i8*, i32, i8*)*, i32, i8*)* @halide_set_custom_do_task(i32 (i8*, i32 (i8*, i32, i8*)*, i32, i8*)*) local_unnamed_addr #3 {
  %2 = load i32 (i8*, i32 (i8*, i32, i8*)*, i32, i8*)*, i32 (i8*, i32 (i8*, i32, i8*)*, i32, i8*)** @_ZN6Halide7Runtime8Internal14custom_do_taskE, align 4, !tbaa !8
  store i32 (i8*, i32 (i8*, i32, i8*)*, i32, i8*)* %0, i32 (i8*, i32 (i8*, i32, i8*)*, i32, i8*)** @_ZN6Halide7Runtime8Internal14custom_do_taskE, align 4, !tbaa !8
  ret i32 (i8*, i32 (i8*, i32, i8*)*, i32, i8*)* %2
}

; Function Attrs: norecurse nounwind
define weak i32 (i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*)* @halide_set_custom_do_par_for(i32 (i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*)*) local_unnamed_addr #3 {
  %2 = load i32 (i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*)*, i32 (i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*)** @_ZN6Halide7Runtime8Internal17custom_do_par_forE, align 4, !tbaa !8
  store i32 (i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*)* %0, i32 (i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*)** @_ZN6Halide7Runtime8Internal17custom_do_par_forE, align 4, !tbaa !8
  ret i32 (i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*)* %2
}

; Function Attrs: nounwind
define weak i32 @halide_do_par_for(i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*) local_unnamed_addr #0 {
  %6 = load i32 (i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*)*, i32 (i8*, i32 (i8*, i32, i8*)*, i32, i32, i8*)** @_ZN6Halide7Runtime8Internal17custom_do_par_forE, align 4, !tbaa !8
  %7 = tail call i32 %6(i8* %0, i32 (i8*, i32, i8*)* %1, i32 %2, i32 %3, i8* %4) #11
  ret i32 %7
}

; Function Attrs: nounwind
define weak i32 @halide_do_loop_task(i8*, i32 (i8*, i32, i32, i8*, i8*)*, i32, i32, i8*, i8*) local_unnamed_addr #0 {
  %7 = load i32 (i8*, i32 (i8*, i32, i32, i8*, i8*)*, i32, i32, i8*, i8*)*, i32 (i8*, i32 (i8*, i32, i32, i8*, i8*)*, i32, i32, i8*, i8*)** @_ZN6Halide7Runtime8Internal19custom_do_loop_taskE, align 4, !tbaa !8
  %8 = tail call i32 %7(i8* %0, i32 (i8*, i32, i32, i8*, i8*)* %1, i32 %2, i32 %3, i8* %4, i8* %5) #11
  ret i32 %8
}

; Function Attrs: nounwind
define weak i32 @halide_do_parallel_tasks(i8*, i32, %struct.halide_parallel_task_t*, i8*) local_unnamed_addr #0 {
  %5 = load i32 (i8*, i32, %struct.halide_parallel_task_t*, i8*)*, i32 (i8*, i32, %struct.halide_parallel_task_t*, i8*)** @_ZN6Halide7Runtime8Internal24custom_do_parallel_tasksE, align 4, !tbaa !8
  %6 = tail call i32 %5(i8* %0, i32 %1, %struct.halide_parallel_task_t* %2, i8* %3) #11
  ret i32 %6
}

; Function Attrs: nounwind
define weak i32 @halide_semaphore_init(%struct.halide_semaphore_t*, i32) local_unnamed_addr #0 {
  %3 = load i32 (%struct.halide_semaphore_t*, i32)*, i32 (%struct.halide_semaphore_t*, i32)** @_ZN6Halide7Runtime8Internal21custom_semaphore_initE, align 4, !tbaa !8
  %4 = tail call i32 %3(%struct.halide_semaphore_t* %0, i32 %1) #11
  ret i32 %4
}

; Function Attrs: nounwind
define weak i32 @halide_semaphore_release(%struct.halide_semaphore_t*, i32) local_unnamed_addr #0 {
  %3 = load i32 (%struct.halide_semaphore_t*, i32)*, i32 (%struct.halide_semaphore_t*, i32)** @_ZN6Halide7Runtime8Internal24custom_semaphore_releaseE, align 4, !tbaa !8
  %4 = tail call i32 %3(%struct.halide_semaphore_t* %0, i32 %1) #11
  ret i32 %4
}

; Function Attrs: nounwind
define weak zeroext i1 @halide_semaphore_try_acquire(%struct.halide_semaphore_t*, i32) local_unnamed_addr #0 {
  %3 = load i1 (%struct.halide_semaphore_t*, i32)*, i1 (%struct.halide_semaphore_t*, i32)** @_ZN6Halide7Runtime8Internal28custom_semaphore_try_acquireE, align 4, !tbaa !8
  %4 = tail call zeroext i1 %3(%struct.halide_semaphore_t* %0, i32 %1) #11
  ret i1 %4
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64, i8* nocapture) #4

; Function Attrs: nounwind
declare i8* @memcpy(i8* returned, i8* nocapture readonly, i32) local_unnamed_addr #1

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64, i8* nocapture) #4

; Function Attrs: argmemonly nounwind
declare void @llvm.memcpy.p0i8.p0i8.i32(i8* nocapture writeonly, i8* nocapture readonly, i32, i1) #4

; Function Attrs: norecurse nounwind
define weak void @halide_set_gpu_device(i32) local_unnamed_addr #3 {
  store i32 %0, i32* @_ZN6Halide7Runtime8Internal17halide_gpu_deviceE, align 4, !tbaa !12
  store i8 1, i8* @_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE, align 1, !tbaa !14
  ret void
}

; Function Attrs: nounwind
define weak i32 @halide_get_gpu_device(i8*) local_unnamed_addr #0 {
  br label %2

; <label>:2:                                      ; preds = %2, %1
  %3 = atomicrmw xchg i32* @_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE, i32 1 seq_cst
  %4 = icmp eq i32 %3, 0
  br i1 %4, label %5, label %2

; <label>:5:                                      ; preds = %2
  %6 = load i8, i8* @_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE, align 1, !tbaa !14, !range !16
  %7 = icmp eq i8 %6, 0
  br i1 %7, label %10, label %8

; <label>:8:                                      ; preds = %5
  %9 = load i32, i32* @_ZN6Halide7Runtime8Internal17halide_gpu_deviceE, align 4, !tbaa !12
  br label %17

; <label>:10:                                     ; preds = %5
  %11 = tail call i8* @getenv(i8* getelementptr inbounds ([14 x i8], [14 x i8]* @.str.7, i32 0, i32 0)) #11
  %12 = icmp eq i8* %11, null
  br i1 %12, label %15, label %13

; <label>:13:                                     ; preds = %10
  %14 = tail call i32 @atoi(i8* nonnull %11) #11
  br label %15

; <label>:15:                                     ; preds = %13, %10
  %16 = phi i32 [ %14, %13 ], [ -1, %10 ]
  store i32 %16, i32* @_ZN6Halide7Runtime8Internal17halide_gpu_deviceE, align 4, !tbaa !12
  store i8 1, i8* @_ZN6Halide7Runtime8Internal29halide_gpu_device_initializedE, align 1, !tbaa !14
  br label %17

; <label>:17:                                     ; preds = %15, %8
  %18 = phi i32 [ %9, %8 ], [ %16, %15 ]
  store atomic i32 0, i32* @_ZN6Halide7Runtime8Internal22halide_gpu_device_lockE release, align 4
  ret i32 %18
}

; Function Attrs: nounwind readonly
declare i8* @getenv(i8* nocapture) local_unnamed_addr #5

; Function Attrs: nounwind readonly
declare i32 @atoi(i8* nocapture) local_unnamed_addr #5

; Function Attrs: norecurse nounwind
define weak i8* @halide_string_to_string(i8*, i8*, i8*) local_unnamed_addr #3 {
  %4 = icmp ult i8* %0, %1
  br i1 %4, label %.preheader, label %.loopexit

; <label>:5:                                      ; preds = %10
  store i8 0, i8* %7, align 1, !tbaa !17
  br label %.loopexit

.preheader:                                       ; preds = %3, %10
  %6 = phi i8* [ %12, %10 ], [ %2, %3 ]
  %7 = phi i8* [ %11, %10 ], [ %0, %3 ]
  %8 = load i8, i8* %6, align 1, !tbaa !17
  store i8 %8, i8* %7, align 1, !tbaa !17
  %9 = icmp eq i8 %8, 0
  br i1 %9, label %.loopexit, label %10

; <label>:10:                                     ; preds = %.preheader
  %11 = getelementptr inbounds i8, i8* %7, i32 1
  %12 = getelementptr inbounds i8, i8* %6, i32 1
  %13 = icmp eq i8* %11, %1
  br i1 %13, label %5, label %.preheader

.loopexit:                                        ; preds = %.preheader, %5, %3
  %14 = phi i8* [ %1, %5 ], [ %0, %3 ], [ %7, %.preheader ]
  ret i8* %14
}

; Function Attrs: nounwind
define weak i8* @halide_uint64_to_string(i8*, i8*, i64, i32) local_unnamed_addr #0 {
  %5 = alloca [32 x i8], align 1
  %6 = getelementptr inbounds [32 x i8], [32 x i8]* %5, i32 0, i32 0
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %6) #9
  %7 = getelementptr inbounds [32 x i8], [32 x i8]* %5, i32 0, i32 31
  store i8 0, i8* %7, align 1, !tbaa !17
  %8 = getelementptr inbounds [32 x i8], [32 x i8]* %5, i32 0, i32 30
  %9 = icmp sgt i32 %3, 0
  %10 = icmp ne i64 %2, 0
  %11 = or i1 %10, %9
  br i1 %11, label %.preheader, label %.loopexit

.loopexit:                                        ; preds = %.preheader, %4
  %12 = phi i8* [ %8, %4 ], [ %23, %.preheader ]
  %13 = getelementptr inbounds i8, i8* %12, i32 1
  %14 = call i8* @halide_string_to_string(i8* %0, i8* %1, i8* nonnull %13) #12
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %6) #9
  ret i8* %14

.preheader:                                       ; preds = %4, %.preheader
  %15 = phi i64 [ %18, %.preheader ], [ %2, %4 ]
  %16 = phi i8* [ %23, %.preheader ], [ %8, %4 ]
  %17 = phi i32 [ %24, %.preheader ], [ 0, %4 ]
  %18 = udiv i64 %15, 10
  %19 = mul i64 %18, -10
  %20 = add i64 %19, %15
  %21 = trunc i64 %20 to i8
  %22 = add i8 %21, 48
  store i8 %22, i8* %16, align 1, !tbaa !17
  %23 = getelementptr inbounds i8, i8* %16, i32 -1
  %24 = add nuw nsw i32 %17, 1
  %25 = icmp slt i32 %24, %3
  %26 = icmp ugt i64 %15, 9
  %27 = or i1 %26, %25
  br i1 %27, label %.preheader, label %.loopexit
}

; Function Attrs: nounwind
define weak i8* @halide_int64_to_string(i8*, i8*, i64, i32) local_unnamed_addr #0 {
  %5 = icmp slt i64 %2, 0
  %6 = icmp ult i8* %0, %1
  %7 = and i1 %6, %5
  br i1 %7, label %8, label %11

; <label>:8:                                      ; preds = %4
  %9 = getelementptr inbounds i8, i8* %0, i32 1
  store i8 45, i8* %0, align 1, !tbaa !17
  %10 = sub nsw i64 0, %2
  br label %11

; <label>:11:                                     ; preds = %8, %4
  %12 = phi i64 [ %10, %8 ], [ %2, %4 ]
  %13 = phi i8* [ %9, %8 ], [ %0, %4 ]
  %14 = tail call i8* @halide_uint64_to_string(i8* %13, i8* %1, i64 %12, i32 %3) #12
  ret i8* %14
}

; Function Attrs: nounwind
define weak i8* @halide_double_to_string(i8*, i8*, double, i32) local_unnamed_addr #0 {
  %5 = alloca double, align 8
  %6 = alloca i64, align 8
  %7 = alloca [512 x i8], align 1
  store double %2, double* %5, align 8, !tbaa !18
  %8 = bitcast i64* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %8) #9
  store i64 0, i64* %6, align 8, !tbaa !20
  %9 = bitcast double* %5 to i8*
  %10 = call i8* @memcpy(i8* nonnull %8, i8* nonnull %9, i32 8) #11
  %11 = load i64, i64* %6, align 8, !tbaa !20
  %12 = and i64 %11, 4503599627370495
  %13 = lshr i64 %11, 52
  %14 = trunc i64 %13 to i32
  %15 = and i32 %14, 2047
  %16 = lshr i64 %11, 63
  %17 = trunc i64 %16 to i32
  %18 = icmp eq i32 %15, 2047
  br i1 %18, label %19, label %32

; <label>:19:                                     ; preds = %4
  %20 = icmp eq i64 %12, 0
  %21 = icmp ne i32 %17, 0
  br i1 %20, label %27, label %22

; <label>:22:                                     ; preds = %19
  br i1 %21, label %23, label %25

; <label>:23:                                     ; preds = %22
  %24 = call i8* @halide_string_to_string(i8* %0, i8* %1, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.8, i32 0, i32 0)) #12
  br label %192

; <label>:25:                                     ; preds = %22
  %26 = call i8* @halide_string_to_string(i8* %0, i8* %1, i8* getelementptr inbounds ([4 x i8], [4 x i8]* @.str.1.9, i32 0, i32 0)) #12
  br label %192

; <label>:27:                                     ; preds = %19
  br i1 %21, label %28, label %30

; <label>:28:                                     ; preds = %27
  %29 = call i8* @halide_string_to_string(i8* %0, i8* %1, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.2.10, i32 0, i32 0)) #12
  br label %192

; <label>:30:                                     ; preds = %27
  %31 = call i8* @halide_string_to_string(i8* %0, i8* %1, i8* getelementptr inbounds ([4 x i8], [4 x i8]* @.str.3.11, i32 0, i32 0)) #12
  br label %192

; <label>:32:                                     ; preds = %4
  %33 = icmp eq i32 %15, 0
  %34 = icmp eq i64 %12, 0
  %35 = and i1 %34, %33
  br i1 %35, label %36, label %49

; <label>:36:                                     ; preds = %32
  %37 = icmp eq i32 %3, 0
  %38 = icmp ne i32 %17, 0
  br i1 %37, label %44, label %39

; <label>:39:                                     ; preds = %36
  br i1 %38, label %40, label %42

; <label>:40:                                     ; preds = %39
  %41 = call i8* @halide_string_to_string(i8* %0, i8* %1, i8* getelementptr inbounds ([14 x i8], [14 x i8]* @.str.4.12, i32 0, i32 0)) #12
  br label %192

; <label>:42:                                     ; preds = %39
  %43 = call i8* @halide_string_to_string(i8* %0, i8* %1, i8* getelementptr inbounds ([13 x i8], [13 x i8]* @.str.5.13, i32 0, i32 0)) #12
  br label %192

; <label>:44:                                     ; preds = %36
  br i1 %38, label %45, label %47

; <label>:45:                                     ; preds = %44
  %46 = call i8* @halide_string_to_string(i8* %0, i8* %1, i8* getelementptr inbounds ([10 x i8], [10 x i8]* @.str.6.14, i32 0, i32 0)) #12
  br label %192

; <label>:47:                                     ; preds = %44
  %48 = call i8* @halide_string_to_string(i8* %0, i8* %1, i8* getelementptr inbounds ([9 x i8], [9 x i8]* @.str.7.15, i32 0, i32 0)) #12
  br label %192

; <label>:49:                                     ; preds = %32
  %50 = icmp eq i32 %17, 0
  br i1 %50, label %54, label %51

; <label>:51:                                     ; preds = %49
  %52 = call i8* @halide_string_to_string(i8* %0, i8* %1, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.8.16, i32 0, i32 0)) #12
  %53 = fsub double -0.000000e+00, %2
  store double %53, double* %5, align 8, !tbaa !18
  br label %54

; <label>:54:                                     ; preds = %51, %49
  %55 = phi double [ %53, %51 ], [ %2, %49 ]
  %56 = phi i8* [ %52, %51 ], [ %0, %49 ]
  %57 = icmp eq i32 %3, 0
  br i1 %57, label %99, label %58

; <label>:58:                                     ; preds = %54
  %59 = fcmp olt double %55, 1.000000e+00
  br i1 %59, label %.preheader16, label %66

.preheader16:                                     ; preds = %58, %.preheader16
  %60 = phi i32 [ %63, %.preheader16 ], [ 0, %58 ]
  %61 = phi double [ %62, %.preheader16 ], [ %55, %58 ]
  %62 = fmul double %61, 1.000000e+01
  %63 = add nsw i32 %60, -1
  %64 = fcmp olt double %62, 1.000000e+00
  br i1 %64, label %.preheader16, label %65

; <label>:65:                                     ; preds = %.preheader16
  store double %62, double* %5, align 8, !tbaa !18
  br label %66

; <label>:66:                                     ; preds = %65, %58
  %67 = phi double [ %62, %65 ], [ %55, %58 ]
  %68 = phi i32 [ %63, %65 ], [ 0, %58 ]
  %69 = fcmp ult double %67, 1.000000e+01
  br i1 %69, label %76, label %.preheader15

.preheader15:                                     ; preds = %66, %.preheader15
  %70 = phi i32 [ %73, %.preheader15 ], [ %68, %66 ]
  %71 = phi double [ %72, %.preheader15 ], [ %67, %66 ]
  %72 = fdiv double %71, 1.000000e+01
  %73 = add nsw i32 %70, 1
  %74 = fcmp ult double %72, 1.000000e+01
  br i1 %74, label %75, label %.preheader15

; <label>:75:                                     ; preds = %.preheader15
  store double %72, double* %5, align 8, !tbaa !18
  br label %76

; <label>:76:                                     ; preds = %75, %66
  %77 = phi double [ %72, %75 ], [ %67, %66 ]
  %78 = phi i32 [ %73, %75 ], [ %68, %66 ]
  %79 = fmul double %77, 1.000000e+06
  %80 = fadd double %79, 5.000000e-01
  %81 = fptoui double %80 to i64
  %82 = udiv i64 %81, 1000000
  %83 = mul i64 %82, -1000000
  %84 = add i64 %83, %81
  %85 = call i8* @halide_int64_to_string(i8* %56, i8* %1, i64 %82, i32 1) #12
  %86 = call i8* @halide_string_to_string(i8* %85, i8* %1, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.30.67, i32 0, i32 0)) #12
  %87 = call i8* @halide_int64_to_string(i8* %86, i8* %1, i64 %84, i32 6) #12
  %88 = icmp sgt i32 %78, -1
  br i1 %88, label %89, label %91

; <label>:89:                                     ; preds = %76
  %90 = call i8* @halide_string_to_string(i8* %87, i8* %1, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.10, i32 0, i32 0)) #12
  br label %94

; <label>:91:                                     ; preds = %76
  %92 = call i8* @halide_string_to_string(i8* %87, i8* %1, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.11, i32 0, i32 0)) #12
  %93 = sub nsw i32 0, %78
  br label %94

; <label>:94:                                     ; preds = %91, %89
  %95 = phi i32 [ %78, %89 ], [ %93, %91 ]
  %96 = phi i8* [ %90, %89 ], [ %92, %91 ]
  %97 = sext i32 %95 to i64
  %98 = call i8* @halide_int64_to_string(i8* %96, i8* %1, i64 %97, i32 2) #12
  br label %192

; <label>:99:                                     ; preds = %54
  br i1 %33, label %100, label %102

; <label>:100:                                    ; preds = %99
  %101 = call i8* @halide_double_to_string(i8* %56, i8* %1, double 0.000000e+00, i32 0) #12
  br label %192

; <label>:102:                                    ; preds = %99
  %103 = or i64 %12, 4503599627370496
  %104 = add nsw i32 %15, -1075
  %105 = icmp slt i32 %104, 0
  br i1 %105, label %106, label %136

; <label>:106:                                    ; preds = %102
  %107 = icmp slt i32 %104, -52
  br i1 %107, label %114, label %108

; <label>:108:                                    ; preds = %106
  %109 = sub nsw i32 1075, %15
  %110 = zext i32 %109 to i64
  %111 = lshr i64 %103, %110
  %112 = shl i64 %111, %110
  %113 = sub i64 %103, %112
  br label %114

; <label>:114:                                    ; preds = %108, %106
  %115 = phi i64 [ %111, %108 ], [ 0, %106 ]
  %116 = phi i64 [ %113, %108 ], [ %103, %106 ]
  %117 = uitofp i64 %116 to double
  %118 = zext i32 %104 to i64
  %119 = shl i64 %118, 52
  %120 = add i64 %119, 4696837146684686336
  %121 = bitcast i64 %120 to double
  %122 = fmul double %121, %117
  %123 = fadd double %122, 5.000000e-01
  %124 = fptoui double %123 to i64
  %125 = uitofp i64 %124 to double
  %126 = fcmp oeq double %123, %125
  %127 = and i64 %124, 1
  %128 = icmp ne i64 %127, 0
  %129 = and i1 %128, %126
  %130 = sext i1 %129 to i64
  %131 = add i64 %130, %124
  %132 = icmp eq i64 %131, 1000000
  %133 = zext i1 %132 to i64
  %134 = add i64 %115, %133
  %135 = select i1 %132, i64 0, i64 %131
  br label %136

; <label>:136:                                    ; preds = %114, %102
  %137 = phi i64 [ %134, %114 ], [ %103, %102 ]
  %138 = phi i32 [ 0, %114 ], [ %104, %102 ]
  %139 = phi i64 [ %135, %114 ], [ 0, %102 ]
  %140 = getelementptr inbounds [512 x i8], [512 x i8]* %7, i32 0, i32 0
  call void @llvm.lifetime.start.p0i8(i64 512, i8* nonnull %140) #9
  %141 = getelementptr inbounds [512 x i8], [512 x i8]* %7, i32 0, i32 512
  %142 = getelementptr inbounds [512 x i8], [512 x i8]* %7, i32 0, i32 480
  %143 = call i8* @halide_int64_to_string(i8* nonnull %142, i8* nonnull %141, i64 %137, i32 1) #12
  %144 = icmp sgt i32 %138, 0
  br i1 %144, label %.preheader14.preheader, label %.loopexit

.preheader14.preheader:                           ; preds = %136
  %145 = add nsw i32 %138, -1
  %xtraiter = and i32 %138, 7
  %146 = icmp ult i32 %145, 7
  br i1 %146, label %.loopexit.loopexit.unr-lcssa, label %.preheader14.preheader.new

.preheader14.preheader.new:                       ; preds = %.preheader14.preheader
  %unroll_iter = sub nsw i32 %138, %xtraiter
  br label %.preheader14

.preheader14:                                     ; preds = %317, %.preheader14.preheader.new
  %147 = phi i8* [ %142, %.preheader14.preheader.new ], [ %318, %317 ]
  %niter = phi i32 [ %unroll_iter, %.preheader14.preheader.new ], [ %niter.nsub.7, %317 ]
  %148 = getelementptr inbounds i8, i8* %147, i32 -1
  %149 = icmp eq i8* %143, %147
  br i1 %149, label %.preheader14.144, label %.preheader

.loopexit.loopexit.unr-lcssa:                     ; preds = %317, %.preheader14.preheader
  %.lcssa37.ph = phi i8* [ undef, %.preheader14.preheader ], [ %318, %317 ]
  %.unr42 = phi i8* [ %142, %.preheader14.preheader ], [ %318, %317 ]
  %lcmp.mod = icmp eq i32 %xtraiter, 0
  br i1 %lcmp.mod, label %.loopexit, label %.preheader14.epil

.preheader14.epil:                                ; preds = %.loopexit.loopexit.unr-lcssa, %168
  %150 = phi i8* [ %169, %168 ], [ %.unr42, %.loopexit.loopexit.unr-lcssa ]
  %epil.iter = phi i32 [ %epil.iter.sub, %168 ], [ %xtraiter, %.loopexit.loopexit.unr-lcssa ]
  %151 = getelementptr inbounds i8, i8* %150, i32 -1
  %152 = icmp eq i8* %143, %150
  br i1 %152, label %168, label %.preheader.epil

.preheader.epil:                                  ; preds = %.preheader14.epil, %.preheader.epil
  %153 = phi i8* [ %155, %.preheader.epil ], [ %143, %.preheader14.epil ]
  %154 = phi i8 [ %162, %.preheader.epil ], [ 0, %.preheader14.epil ]
  %155 = getelementptr inbounds i8, i8* %153, i32 -1
  %156 = load i8, i8* %155, align 1, !tbaa !17
  %157 = shl i8 %156, 1
  %158 = add i8 %157, -96
  %159 = or i8 %158, %154
  %160 = icmp sgt i8 %159, 9
  %161 = add i8 %159, -10
  %162 = zext i1 %160 to i8
  %163 = select i1 %160, i8 %161, i8 %159
  %164 = add i8 %163, 48
  store i8 %164, i8* %155, align 1, !tbaa !17
  %165 = icmp eq i8* %155, %150
  br i1 %165, label %166, label %.preheader.epil

; <label>:166:                                    ; preds = %.preheader.epil
  br i1 %160, label %167, label %168

; <label>:167:                                    ; preds = %166
  store i8 49, i8* %151, align 1, !tbaa !17
  br label %168

; <label>:168:                                    ; preds = %167, %166, %.preheader14.epil
  %169 = phi i8* [ %151, %167 ], [ %150, %166 ], [ %143, %.preheader14.epil ]
  %epil.iter.sub = add i32 %epil.iter, -1
  %epil.iter.cmp = icmp eq i32 %epil.iter.sub, 0
  br i1 %epil.iter.cmp, label %.loopexit, label %.preheader14.epil, !llvm.loop !22

.loopexit:                                        ; preds = %.loopexit.loopexit.unr-lcssa, %168, %136
  %170 = phi i8* [ %142, %136 ], [ %.lcssa37.ph, %.loopexit.loopexit.unr-lcssa ], [ %169, %168 ]
  %171 = call i8* @halide_string_to_string(i8* %56, i8* %1, i8* %170) #12
  %172 = call i8* @halide_string_to_string(i8* %171, i8* %1, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.30.67, i32 0, i32 0)) #12
  %173 = call i8* @halide_int64_to_string(i8* %172, i8* %1, i64 %139, i32 6) #12
  call void @llvm.lifetime.end.p0i8(i64 512, i8* nonnull %140) #9
  br label %192

; <label>:174:                                    ; preds = %.preheader
  br i1 %182, label %188, label %.preheader14.144

.preheader:                                       ; preds = %.preheader14, %.preheader
  %175 = phi i8* [ %177, %.preheader ], [ %143, %.preheader14 ]
  %176 = phi i8 [ %184, %.preheader ], [ 0, %.preheader14 ]
  %177 = getelementptr inbounds i8, i8* %175, i32 -1
  %178 = load i8, i8* %177, align 1, !tbaa !17
  %179 = shl i8 %178, 1
  %180 = add i8 %179, -96
  %181 = or i8 %180, %176
  %182 = icmp sgt i8 %181, 9
  %183 = add i8 %181, -10
  %184 = zext i1 %182 to i8
  %185 = select i1 %182, i8 %183, i8 %181
  %186 = add i8 %185, 48
  store i8 %186, i8* %177, align 1, !tbaa !17
  %187 = icmp eq i8* %177, %147
  br i1 %187, label %174, label %.preheader

; <label>:188:                                    ; preds = %174
  store i8 49, i8* %148, align 1, !tbaa !17
  br label %.preheader14.144

.preheader14.144:                                 ; preds = %188, %174, %.preheader14
  %189 = phi i8* [ %148, %188 ], [ %147, %174 ], [ %143, %.preheader14 ]
  %190 = getelementptr inbounds i8, i8* %189, i32 -1
  %191 = icmp eq i8* %143, %189
  br i1 %191, label %.preheader14.245, label %.preheader.1

; <label>:192:                                    ; preds = %.loopexit, %100, %94, %47, %45, %42, %40, %30, %28, %25, %23
  %193 = phi i8* [ %24, %23 ], [ %26, %25 ], [ %29, %28 ], [ %31, %30 ], [ %41, %40 ], [ %43, %42 ], [ %46, %45 ], [ %48, %47 ], [ %101, %100 ], [ %98, %94 ], [ %173, %.loopexit ]
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %8) #9
  ret i8* %193

.preheader.1:                                     ; preds = %.preheader14.144, %.preheader.1
  %194 = phi i8* [ %196, %.preheader.1 ], [ %143, %.preheader14.144 ]
  %195 = phi i8 [ %203, %.preheader.1 ], [ 0, %.preheader14.144 ]
  %196 = getelementptr inbounds i8, i8* %194, i32 -1
  %197 = load i8, i8* %196, align 1, !tbaa !17
  %198 = shl i8 %197, 1
  %199 = add i8 %198, -96
  %200 = or i8 %199, %195
  %201 = icmp sgt i8 %200, 9
  %202 = add i8 %200, -10
  %203 = zext i1 %201 to i8
  %204 = select i1 %201, i8 %202, i8 %200
  %205 = add i8 %204, 48
  store i8 %205, i8* %196, align 1, !tbaa !17
  %206 = icmp eq i8* %196, %189
  br i1 %206, label %207, label %.preheader.1

; <label>:207:                                    ; preds = %.preheader.1
  br i1 %201, label %208, label %.preheader14.245

; <label>:208:                                    ; preds = %207
  store i8 49, i8* %190, align 1, !tbaa !17
  br label %.preheader14.245

.preheader14.245:                                 ; preds = %208, %207, %.preheader14.144
  %209 = phi i8* [ %190, %208 ], [ %189, %207 ], [ %143, %.preheader14.144 ]
  %210 = getelementptr inbounds i8, i8* %209, i32 -1
  %211 = icmp eq i8* %143, %209
  br i1 %211, label %.preheader14.346, label %.preheader.2

.preheader.2:                                     ; preds = %.preheader14.245, %.preheader.2
  %212 = phi i8* [ %214, %.preheader.2 ], [ %143, %.preheader14.245 ]
  %213 = phi i8 [ %221, %.preheader.2 ], [ 0, %.preheader14.245 ]
  %214 = getelementptr inbounds i8, i8* %212, i32 -1
  %215 = load i8, i8* %214, align 1, !tbaa !17
  %216 = shl i8 %215, 1
  %217 = add i8 %216, -96
  %218 = or i8 %217, %213
  %219 = icmp sgt i8 %218, 9
  %220 = add i8 %218, -10
  %221 = zext i1 %219 to i8
  %222 = select i1 %219, i8 %220, i8 %218
  %223 = add i8 %222, 48
  store i8 %223, i8* %214, align 1, !tbaa !17
  %224 = icmp eq i8* %214, %209
  br i1 %224, label %225, label %.preheader.2

; <label>:225:                                    ; preds = %.preheader.2
  br i1 %219, label %226, label %.preheader14.346

; <label>:226:                                    ; preds = %225
  store i8 49, i8* %210, align 1, !tbaa !17
  br label %.preheader14.346

.preheader14.346:                                 ; preds = %226, %225, %.preheader14.245
  %227 = phi i8* [ %210, %226 ], [ %209, %225 ], [ %143, %.preheader14.245 ]
  %228 = getelementptr inbounds i8, i8* %227, i32 -1
  %229 = icmp eq i8* %143, %227
  br i1 %229, label %.preheader14.447, label %.preheader.3

.preheader.3:                                     ; preds = %.preheader14.346, %.preheader.3
  %230 = phi i8* [ %232, %.preheader.3 ], [ %143, %.preheader14.346 ]
  %231 = phi i8 [ %239, %.preheader.3 ], [ 0, %.preheader14.346 ]
  %232 = getelementptr inbounds i8, i8* %230, i32 -1
  %233 = load i8, i8* %232, align 1, !tbaa !17
  %234 = shl i8 %233, 1
  %235 = add i8 %234, -96
  %236 = or i8 %235, %231
  %237 = icmp sgt i8 %236, 9
  %238 = add i8 %236, -10
  %239 = zext i1 %237 to i8
  %240 = select i1 %237, i8 %238, i8 %236
  %241 = add i8 %240, 48
  store i8 %241, i8* %232, align 1, !tbaa !17
  %242 = icmp eq i8* %232, %227
  br i1 %242, label %243, label %.preheader.3

; <label>:243:                                    ; preds = %.preheader.3
  br i1 %237, label %244, label %.preheader14.447

; <label>:244:                                    ; preds = %243
  store i8 49, i8* %228, align 1, !tbaa !17
  br label %.preheader14.447

.preheader14.447:                                 ; preds = %244, %243, %.preheader14.346
  %245 = phi i8* [ %228, %244 ], [ %227, %243 ], [ %143, %.preheader14.346 ]
  %246 = getelementptr inbounds i8, i8* %245, i32 -1
  %247 = icmp eq i8* %143, %245
  br i1 %247, label %.preheader14.548, label %.preheader.4

.preheader.4:                                     ; preds = %.preheader14.447, %.preheader.4
  %248 = phi i8* [ %250, %.preheader.4 ], [ %143, %.preheader14.447 ]
  %249 = phi i8 [ %257, %.preheader.4 ], [ 0, %.preheader14.447 ]
  %250 = getelementptr inbounds i8, i8* %248, i32 -1
  %251 = load i8, i8* %250, align 1, !tbaa !17
  %252 = shl i8 %251, 1
  %253 = add i8 %252, -96
  %254 = or i8 %253, %249
  %255 = icmp sgt i8 %254, 9
  %256 = add i8 %254, -10
  %257 = zext i1 %255 to i8
  %258 = select i1 %255, i8 %256, i8 %254
  %259 = add i8 %258, 48
  store i8 %259, i8* %250, align 1, !tbaa !17
  %260 = icmp eq i8* %250, %245
  br i1 %260, label %261, label %.preheader.4

; <label>:261:                                    ; preds = %.preheader.4
  br i1 %255, label %262, label %.preheader14.548

; <label>:262:                                    ; preds = %261
  store i8 49, i8* %246, align 1, !tbaa !17
  br label %.preheader14.548

.preheader14.548:                                 ; preds = %262, %261, %.preheader14.447
  %263 = phi i8* [ %246, %262 ], [ %245, %261 ], [ %143, %.preheader14.447 ]
  %264 = getelementptr inbounds i8, i8* %263, i32 -1
  %265 = icmp eq i8* %143, %263
  br i1 %265, label %.preheader14.649, label %.preheader.5

.preheader.5:                                     ; preds = %.preheader14.548, %.preheader.5
  %266 = phi i8* [ %268, %.preheader.5 ], [ %143, %.preheader14.548 ]
  %267 = phi i8 [ %275, %.preheader.5 ], [ 0, %.preheader14.548 ]
  %268 = getelementptr inbounds i8, i8* %266, i32 -1
  %269 = load i8, i8* %268, align 1, !tbaa !17
  %270 = shl i8 %269, 1
  %271 = add i8 %270, -96
  %272 = or i8 %271, %267
  %273 = icmp sgt i8 %272, 9
  %274 = add i8 %272, -10
  %275 = zext i1 %273 to i8
  %276 = select i1 %273, i8 %274, i8 %272
  %277 = add i8 %276, 48
  store i8 %277, i8* %268, align 1, !tbaa !17
  %278 = icmp eq i8* %268, %263
  br i1 %278, label %279, label %.preheader.5

; <label>:279:                                    ; preds = %.preheader.5
  br i1 %273, label %280, label %.preheader14.649

; <label>:280:                                    ; preds = %279
  store i8 49, i8* %264, align 1, !tbaa !17
  br label %.preheader14.649

.preheader14.649:                                 ; preds = %280, %279, %.preheader14.548
  %281 = phi i8* [ %264, %280 ], [ %263, %279 ], [ %143, %.preheader14.548 ]
  %282 = getelementptr inbounds i8, i8* %281, i32 -1
  %283 = icmp eq i8* %143, %281
  br i1 %283, label %.preheader14.750, label %.preheader.6

.preheader.6:                                     ; preds = %.preheader14.649, %.preheader.6
  %284 = phi i8* [ %286, %.preheader.6 ], [ %143, %.preheader14.649 ]
  %285 = phi i8 [ %293, %.preheader.6 ], [ 0, %.preheader14.649 ]
  %286 = getelementptr inbounds i8, i8* %284, i32 -1
  %287 = load i8, i8* %286, align 1, !tbaa !17
  %288 = shl i8 %287, 1
  %289 = add i8 %288, -96
  %290 = or i8 %289, %285
  %291 = icmp sgt i8 %290, 9
  %292 = add i8 %290, -10
  %293 = zext i1 %291 to i8
  %294 = select i1 %291, i8 %292, i8 %290
  %295 = add i8 %294, 48
  store i8 %295, i8* %286, align 1, !tbaa !17
  %296 = icmp eq i8* %286, %281
  br i1 %296, label %297, label %.preheader.6

; <label>:297:                                    ; preds = %.preheader.6
  br i1 %291, label %298, label %.preheader14.750

; <label>:298:                                    ; preds = %297
  store i8 49, i8* %282, align 1, !tbaa !17
  br label %.preheader14.750

.preheader14.750:                                 ; preds = %298, %297, %.preheader14.649
  %299 = phi i8* [ %282, %298 ], [ %281, %297 ], [ %143, %.preheader14.649 ]
  %300 = getelementptr inbounds i8, i8* %299, i32 -1
  %301 = icmp eq i8* %143, %299
  br i1 %301, label %317, label %.preheader.7

.preheader.7:                                     ; preds = %.preheader14.750, %.preheader.7
  %302 = phi i8* [ %304, %.preheader.7 ], [ %143, %.preheader14.750 ]
  %303 = phi i8 [ %311, %.preheader.7 ], [ 0, %.preheader14.750 ]
  %304 = getelementptr inbounds i8, i8* %302, i32 -1
  %305 = load i8, i8* %304, align 1, !tbaa !17
  %306 = shl i8 %305, 1
  %307 = add i8 %306, -96
  %308 = or i8 %307, %303
  %309 = icmp sgt i8 %308, 9
  %310 = add i8 %308, -10
  %311 = zext i1 %309 to i8
  %312 = select i1 %309, i8 %310, i8 %308
  %313 = add i8 %312, 48
  store i8 %313, i8* %304, align 1, !tbaa !17
  %314 = icmp eq i8* %304, %299
  br i1 %314, label %315, label %.preheader.7

; <label>:315:                                    ; preds = %.preheader.7
  br i1 %309, label %316, label %317

; <label>:316:                                    ; preds = %315
  store i8 49, i8* %300, align 1, !tbaa !17
  br label %317

; <label>:317:                                    ; preds = %316, %315, %.preheader14.750
  %318 = phi i8* [ %300, %316 ], [ %299, %315 ], [ %143, %.preheader14.750 ]
  %niter.nsub.7 = add i32 %niter, -8
  %niter.ncmp.7 = icmp eq i32 %niter.nsub.7, 0
  br i1 %niter.ncmp.7, label %.loopexit.loopexit.unr-lcssa, label %.preheader14
}

; Function Attrs: nounwind
define weak i8* @halide_pointer_to_string(i8*, i8*, i8*) local_unnamed_addr #0 {
  %4 = alloca [20 x i8], align 1
  %5 = getelementptr inbounds [20 x i8], [20 x i8]* %4, i32 0, i32 0
  call void @llvm.lifetime.start.p0i8(i64 20, i8* nonnull %5) #9
  call void @llvm.memset.p0i8.i32(i8* nonnull align 1 %5, i8 0, i32 20, i1 false)
  %6 = getelementptr inbounds [20 x i8], [20 x i8]* %4, i32 0, i32 18
  %7 = ptrtoint i8* %2 to i32
  %8 = zext i32 %7 to i64
  br label %9

; <label>:9:                                      ; preds = %9, %3
  %10 = phi i32 [ 0, %3 ], [ %20, %9 ]
  %11 = phi i64 [ %8, %3 ], [ %18, %9 ]
  %12 = phi i8* [ %6, %3 ], [ %17, %9 ]
  %13 = trunc i64 %11 to i32
  %14 = and i32 %13, 15
  %15 = getelementptr inbounds [17 x i8], [17 x i8]* @.str.12, i32 0, i32 %14
  %16 = load i8, i8* %15, align 1, !tbaa !17
  %17 = getelementptr inbounds i8, i8* %12, i32 -1
  store i8 %16, i8* %12, align 1, !tbaa !17
  %18 = lshr i64 %11, 4
  %19 = icmp ne i64 %18, 0
  %20 = add nuw nsw i32 %10, 1
  %21 = icmp ult i32 %20, 16
  %22 = and i1 %21, %19
  br i1 %22, label %9, label %23

; <label>:23:                                     ; preds = %9
  %24 = getelementptr inbounds i8, i8* %12, i32 -2
  store i8 120, i8* %17, align 1, !tbaa !17
  store i8 48, i8* %24, align 1, !tbaa !17
  %25 = call i8* @halide_string_to_string(i8* %0, i8* %1, i8* nonnull %24) #12
  call void @llvm.lifetime.end.p0i8(i64 20, i8* nonnull %5) #9
  ret i8* %25
}

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i32(i8* nocapture writeonly, i8, i32, i1) #4

; Function Attrs: nounwind
define weak i8* @halide_type_to_string(i8*, i8*, %struct.halide_type_t*) local_unnamed_addr #0 {
  %4 = getelementptr inbounds %struct.halide_type_t, %struct.halide_type_t* %2, i32 0, i32 0
  %5 = load i8, i8* %4, align 2, !tbaa !24
  %6 = icmp ult i8 %5, 4
  br i1 %6, label %switch.lookup, label %8

switch.lookup:                                    ; preds = %3
  %7 = sext i8 %5 to i32
  %switch.gep = getelementptr inbounds [4 x i8*], [4 x i8*]* @switch.table.halide_type_to_string, i32 0, i32 %7
  %switch.load = load i8*, i8** %switch.gep, align 4
  br label %8

; <label>:8:                                      ; preds = %3, %switch.lookup
  %9 = phi i8* [ %switch.load, %switch.lookup ], [ getelementptr inbounds ([14 x i8], [14 x i8]* @.str.17, i32 0, i32 0), %3 ]
  %10 = tail call i8* @halide_string_to_string(i8* %0, i8* %1, i8* nonnull %9) #12
  %11 = getelementptr inbounds %struct.halide_type_t, %struct.halide_type_t* %2, i32 0, i32 1
  %12 = load i8, i8* %11, align 1, !tbaa !27
  %13 = zext i8 %12 to i64
  %14 = tail call i8* @halide_uint64_to_string(i8* %10, i8* %1, i64 %13, i32 1) #12
  %15 = getelementptr inbounds %struct.halide_type_t, %struct.halide_type_t* %2, i32 0, i32 2
  %16 = load i16, i16* %15, align 2, !tbaa !28
  %17 = icmp eq i16 %16, 1
  br i1 %17, label %23, label %18

; <label>:18:                                     ; preds = %8
  %19 = tail call i8* @halide_string_to_string(i8* %14, i8* %1, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.18, i32 0, i32 0)) #12
  %20 = load i16, i16* %15, align 2, !tbaa !28
  %21 = zext i16 %20 to i64
  %22 = tail call i8* @halide_uint64_to_string(i8* %19, i8* %1, i64 %21, i32 1) #12
  br label %23

; <label>:23:                                     ; preds = %18, %8
  %24 = phi i8* [ %22, %18 ], [ %14, %8 ]
  ret i8* %24
}

; Function Attrs: nounwind
define weak i8* @halide_buffer_to_string(i8*, i8*, %struct.halide_buffer_t*) local_unnamed_addr #0 {
  %4 = icmp eq %struct.halide_buffer_t* %2, null
  br i1 %4, label %5, label %7

; <label>:5:                                      ; preds = %3
  %6 = tail call i8* @halide_string_to_string(i8* %0, i8* %1, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.19, i32 0, i32 0)) #12
  br label %60

; <label>:7:                                      ; preds = %3
  %8 = tail call i8* @halide_string_to_string(i8* %0, i8* %1, i8* getelementptr inbounds ([8 x i8], [8 x i8]* @.str.20, i32 0, i32 0)) #12
  %9 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %2, i32 0, i32 0
  %10 = load i64, i64* %9, align 8, !tbaa !29
  %11 = tail call i8* @halide_uint64_to_string(i8* %8, i8* %1, i64 %10, i32 1) #12
  %12 = tail call i8* @halide_string_to_string(i8* %11, i8* %1, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.55, i32 0, i32 0)) #12
  %13 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %2, i32 0, i32 1
  %14 = bitcast %struct.halide_device_interface_t** %13 to i8**
  %15 = load i8*, i8** %14, align 8, !tbaa !31
  %16 = tail call i8* @halide_pointer_to_string(i8* %12, i8* %1, i8* %15) #12
  %17 = tail call i8* @halide_string_to_string(i8* %16, i8* %1, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.55, i32 0, i32 0)) #12
  %18 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %2, i32 0, i32 2
  %19 = load i8*, i8** %18, align 4, !tbaa !32
  %20 = tail call i8* @halide_pointer_to_string(i8* %17, i8* %1, i8* %19) #12
  %21 = tail call i8* @halide_string_to_string(i8* %20, i8* %1, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.55, i32 0, i32 0)) #12
  %22 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %2, i32 0, i32 3
  %23 = load i64, i64* %22, align 8, !tbaa !33
  %24 = tail call i8* @halide_uint64_to_string(i8* %21, i8* %1, i64 %23, i32 1) #12
  %25 = tail call i8* @halide_string_to_string(i8* %24, i8* %1, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.55, i32 0, i32 0)) #12
  %26 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %2, i32 0, i32 4
  %27 = tail call i8* @halide_type_to_string(i8* %25, i8* %1, %struct.halide_type_t* nonnull %26) #12
  %28 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %2, i32 0, i32 5
  %29 = load i32, i32* %28, align 4, !tbaa !34
  %30 = icmp sgt i32 %29, 0
  br i1 %30, label %31, label %.loopexit

; <label>:31:                                     ; preds = %7
  %32 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %2, i32 0, i32 6
  br label %35

.loopexit:                                        ; preds = %35, %7
  %33 = phi i8* [ %27, %7 ], [ %56, %35 ]
  %34 = tail call i8* @halide_string_to_string(i8* %33, i8* %1, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.5.94, i32 0, i32 0)) #12
  br label %60

; <label>:35:                                     ; preds = %35, %31
  %36 = phi i32 [ 0, %31 ], [ %57, %35 ]
  %37 = phi i8* [ %27, %31 ], [ %56, %35 ]
  %38 = tail call i8* @halide_string_to_string(i8* %37, i8* %1, i8* getelementptr inbounds ([4 x i8], [4 x i8]* @.str.22, i32 0, i32 0)) #12
  %39 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %32, align 8, !tbaa !35
  %40 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %39, i32 %36, i32 0
  %41 = load i32, i32* %40, align 4, !tbaa !36
  %42 = sext i32 %41 to i64
  %43 = tail call i8* @halide_int64_to_string(i8* %38, i8* %1, i64 %42, i32 1) #12
  %44 = tail call i8* @halide_string_to_string(i8* %43, i8* %1, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.55, i32 0, i32 0)) #12
  %45 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %32, align 8, !tbaa !35
  %46 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %45, i32 %36, i32 1
  %47 = load i32, i32* %46, align 4, !tbaa !38
  %48 = sext i32 %47 to i64
  %49 = tail call i8* @halide_int64_to_string(i8* %44, i8* %1, i64 %48, i32 1) #12
  %50 = tail call i8* @halide_string_to_string(i8* %49, i8* %1, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.55, i32 0, i32 0)) #12
  %51 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %32, align 8, !tbaa !35
  %52 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %51, i32 %36, i32 2
  %53 = load i32, i32* %52, align 4, !tbaa !39
  %54 = sext i32 %53 to i64
  %55 = tail call i8* @halide_int64_to_string(i8* %50, i8* %1, i64 %54, i32 1) #12
  %56 = tail call i8* @halide_string_to_string(i8* %55, i8* %1, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.23, i32 0, i32 0)) #12
  %57 = add nuw nsw i32 %36, 1
  %58 = load i32, i32* %28, align 4, !tbaa !34
  %59 = icmp slt i32 %57, %58
  br i1 %59, label %35, label %.loopexit

; <label>:60:                                     ; preds = %.loopexit, %5
  %61 = phi i8* [ %6, %5 ], [ %34, %.loopexit ]
  ret i8* %61
}

; Function Attrs: nounwind
define weak i32 @halide_reuse_device_allocations(i8*, i1 zeroext) local_unnamed_addr #0 {
  %3 = zext i1 %1 to i8
  store i8 %3, i8* @_ZN6Halide7Runtime8Internal36halide_reuse_device_allocations_flagE, align 1, !tbaa !14
  br i1 %1, label %18, label %4

; <label>:4:                                      ; preds = %2
  tail call void @halide_mutex_lock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal21allocation_pools_lockE) #11
  %5 = load %struct.halide_device_allocation_pool*, %struct.halide_device_allocation_pool** @_ZN6Halide7Runtime8Internal23device_allocation_poolsE, align 4, !tbaa !8
  %6 = icmp eq %struct.halide_device_allocation_pool* %5, null
  br i1 %6, label %.loopexit, label %.preheader

.loopexit:                                        ; preds = %.preheader, %4
  %7 = phi i32 [ 0, %4 ], [ %14, %.preheader ]
  tail call void @halide_mutex_unlock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal21allocation_pools_lockE) #11
  br label %18

.preheader:                                       ; preds = %4, %.preheader
  %8 = phi %struct.halide_device_allocation_pool* [ %16, %.preheader ], [ %5, %4 ]
  %9 = phi i32 [ %14, %.preheader ], [ 0, %4 ]
  %10 = getelementptr inbounds %struct.halide_device_allocation_pool, %struct.halide_device_allocation_pool* %8, i32 0, i32 0
  %11 = load i32 (i8*)*, i32 (i8*)** %10, align 4, !tbaa !40
  %12 = tail call i32 %11(i8* %0) #11
  %13 = icmp eq i32 %12, 0
  %14 = select i1 %13, i32 %9, i32 %12
  %15 = getelementptr inbounds %struct.halide_device_allocation_pool, %struct.halide_device_allocation_pool* %8, i32 0, i32 1
  %16 = load %struct.halide_device_allocation_pool*, %struct.halide_device_allocation_pool** %15, align 4, !tbaa !8
  %17 = icmp eq %struct.halide_device_allocation_pool* %16, null
  br i1 %17, label %.loopexit, label %.preheader

; <label>:18:                                     ; preds = %.loopexit, %2
  %19 = phi i32 [ 0, %2 ], [ %7, %.loopexit ]
  ret i32 %19
}

; Function Attrs: norecurse nounwind
define weak zeroext i1 @halide_can_reuse_device_allocations(i8*) local_unnamed_addr #3 {
  %2 = load i8, i8* @_ZN6Halide7Runtime8Internal36halide_reuse_device_allocations_flagE, align 1, !tbaa !14, !range !16
  %3 = icmp ne i8 %2, 0
  ret i1 %3
}

; Function Attrs: norecurse nounwind
define weak void @halide_register_device_allocation_pool(%struct.halide_device_allocation_pool*) local_unnamed_addr #3 {
  tail call void @halide_mutex_lock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal21allocation_pools_lockE) #11
  %2 = load i32, i32* bitcast (%struct.halide_device_allocation_pool** @_ZN6Halide7Runtime8Internal23device_allocation_poolsE to i32*), align 4, !tbaa !8
  %3 = getelementptr inbounds %struct.halide_device_allocation_pool, %struct.halide_device_allocation_pool* %0, i32 0, i32 1
  %4 = bitcast %struct.halide_device_allocation_pool** %3 to i32*
  store i32 %2, i32* %4, align 4, !tbaa !42
  store %struct.halide_device_allocation_pool* %0, %struct.halide_device_allocation_pool** @_ZN6Halide7Runtime8Internal23device_allocation_poolsE, align 4, !tbaa !8
  tail call void @halide_mutex_unlock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal21allocation_pools_lockE) #11
  ret void
}

; Function Attrs: nounwind
define linkonce void @_ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx(%"struct.Halide::Runtime::Internal::device_copy"* dereferenceable(416), i32, i64, i64) local_unnamed_addr #0 {
  %5 = icmp sgt i32 %1, -1
  br i1 %5, label %.preheader, label %.loopexit2

.preheader:                                       ; preds = %4, %10
  %6 = phi i32 [ %11, %10 ], [ %1, %4 ]
  %7 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %0, i32 0, i32 3, i32 %6
  %8 = load i64, i64* %7, align 8, !tbaa !20
  %9 = icmp eq i64 %8, 1
  br i1 %9, label %10, label %.loopexit2

; <label>:10:                                     ; preds = %.preheader
  %11 = add nsw i32 %6, -1
  %12 = icmp sgt i32 %6, 0
  br i1 %12, label %.preheader, label %.loopexit2

.loopexit2:                                       ; preds = %10, %.preheader, %4
  %13 = phi i32 [ %1, %4 ], [ %6, %.preheader ], [ %11, %10 ]
  %14 = icmp eq i32 %13, -1
  br i1 %14, label %23, label %15

; <label>:15:                                     ; preds = %.loopexit2
  %16 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %0, i32 0, i32 3, i32 %13
  %17 = load i64, i64* %16, align 8, !tbaa !20
  %18 = icmp eq i64 %17, 0
  br i1 %18, label %.loopexit, label %19

; <label>:19:                                     ; preds = %15
  %20 = add nsw i32 %13, -1
  %21 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %0, i32 0, i32 4, i32 %13
  %22 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %0, i32 0, i32 5, i32 %13
  br label %38

; <label>:23:                                     ; preds = %.loopexit2
  %24 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %0, i32 0, i32 0
  %25 = load i64, i64* %24, align 8, !tbaa !43
  %26 = add i64 %25, %2
  %27 = trunc i64 %26 to i32
  %28 = inttoptr i32 %27 to i8*
  %29 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %0, i32 0, i32 1
  %30 = load i64, i64* %29, align 8, !tbaa !45
  %31 = add i64 %30, %3
  %32 = trunc i64 %31 to i32
  %33 = inttoptr i32 %32 to i8*
  %34 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %0, i32 0, i32 6
  %35 = load i64, i64* %34, align 8, !tbaa !46
  %36 = trunc i64 %35 to i32
  %37 = tail call i8* @memcpy(i8* %33, i8* %28, i32 %36) #11
  br label %.loopexit

; <label>:38:                                     ; preds = %38, %19
  %39 = phi i64 [ 0, %19 ], [ %46, %38 ]
  %40 = phi i64 [ %2, %19 ], [ %43, %38 ]
  %41 = phi i64 [ %3, %19 ], [ %45, %38 ]
  tail call void @_ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx(%"struct.Halide::Runtime::Internal::device_copy"* nonnull dereferenceable(416) %0, i32 %20, i64 %40, i64 %41) #12
  %42 = load i64, i64* %21, align 8, !tbaa !20
  %43 = add i64 %42, %40
  %44 = load i64, i64* %22, align 8, !tbaa !20
  %45 = add i64 %44, %41
  %46 = add nuw i64 %39, 1
  %47 = load i64, i64* %16, align 8, !tbaa !20
  %48 = icmp ult i64 %46, %47
  br i1 %48, label %38, label %.loopexit

.loopexit:                                        ; preds = %38, %23, %15
  ret void
}

; Function Attrs: nounwind
define linkonce void @_ZN6Halide7Runtime8Internal11copy_memoryERKNS1_11device_copyEPv(%"struct.Halide::Runtime::Internal::device_copy"* dereferenceable(416), i8*) local_unnamed_addr #0 {
  %3 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %0, i32 0, i32 0
  %4 = load i64, i64* %3, align 8, !tbaa !43
  %5 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %0, i32 0, i32 1
  %6 = load i64, i64* %5, align 8, !tbaa !45
  %7 = icmp eq i64 %4, %6
  br i1 %7, label %11, label %8

; <label>:8:                                      ; preds = %2
  %9 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %0, i32 0, i32 2
  %10 = load i64, i64* %9, align 8, !tbaa !47
  tail call void @_ZN6Halide7Runtime8Internal18copy_memory_helperERKNS1_11device_copyEixx(%"struct.Halide::Runtime::Internal::device_copy"* nonnull dereferenceable(416) %0, i32 15, i64 %10, i64 0) #12
  br label %11

; <label>:11:                                     ; preds = %8, %2
  ret void
}

; Function Attrs: nounwind
define linkonce void @_ZN6Halide7Runtime8Internal16make_buffer_copyEPK15halide_buffer_tbS4_b(%"struct.Halide::Runtime::Internal::device_copy"* noalias sret, %struct.halide_buffer_t*, i1 zeroext, %struct.halide_buffer_t*, i1 zeroext) local_unnamed_addr #0 {
  %6 = alloca %"struct.Halide::Runtime::Internal::device_copy", align 8
  %7 = bitcast %"struct.Halide::Runtime::Internal::device_copy"* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 416, i8* nonnull %7) #9
  br i1 %2, label %8, label %13

; <label>:8:                                      ; preds = %5
  %9 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 2
  %10 = bitcast i8** %9 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !32
  %12 = zext i32 %11 to i64
  br label %16

; <label>:13:                                     ; preds = %5
  %14 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %15 = load i64, i64* %14, align 8, !tbaa !29
  br label %16

; <label>:16:                                     ; preds = %13, %8
  %17 = phi i64 [ %12, %8 ], [ %15, %13 ]
  %18 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 0
  store i64 %17, i64* %18, align 8, !tbaa !43
  br i1 %4, label %19, label %24

; <label>:19:                                     ; preds = %16
  %20 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 2
  %21 = bitcast i8** %20 to i32*
  %22 = load i32, i32* %21, align 4, !tbaa !32
  %23 = zext i32 %22 to i64
  br label %27

; <label>:24:                                     ; preds = %16
  %25 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 0
  %26 = load i64, i64* %25, align 8, !tbaa !29
  br label %27

; <label>:27:                                     ; preds = %24, %19
  %28 = phi i64 [ %23, %19 ], [ %26, %24 ]
  %29 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 1
  store i64 %28, i64* %29, align 8, !tbaa !45
  %30 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 4, i32 1
  %31 = load i8, i8* %30, align 1, !tbaa !27
  %32 = zext i8 %31 to i32
  %33 = add nuw nsw i32 %32, 7
  %34 = lshr i32 %33, 3
  %35 = zext i32 %34 to i64
  %36 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 6
  store i64 %35, i64* %36, align 8, !tbaa !46
  %37 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 0
  store i64 1, i64* %37, align 8, !tbaa !20
  %38 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 0
  store i64 0, i64* %38, align 8, !tbaa !20
  %39 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 0
  store i64 0, i64* %39, align 8, !tbaa !20
  %40 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 1
  store i64 1, i64* %40, align 8, !tbaa !20
  %41 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 1
  store i64 0, i64* %41, align 8, !tbaa !20
  %42 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 1
  store i64 0, i64* %42, align 8, !tbaa !20
  %43 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 2
  store i64 1, i64* %43, align 8, !tbaa !20
  %44 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 2
  store i64 0, i64* %44, align 8, !tbaa !20
  %45 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 2
  store i64 0, i64* %45, align 8, !tbaa !20
  %46 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 3
  store i64 1, i64* %46, align 8, !tbaa !20
  %47 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 3
  store i64 0, i64* %47, align 8, !tbaa !20
  %48 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 3
  store i64 0, i64* %48, align 8, !tbaa !20
  %49 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 4
  store i64 1, i64* %49, align 8, !tbaa !20
  %50 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 4
  store i64 0, i64* %50, align 8, !tbaa !20
  %51 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 4
  store i64 0, i64* %51, align 8, !tbaa !20
  %52 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 5
  store i64 1, i64* %52, align 8, !tbaa !20
  %53 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 5
  store i64 0, i64* %53, align 8, !tbaa !20
  %54 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 5
  store i64 0, i64* %54, align 8, !tbaa !20
  %55 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 6
  store i64 1, i64* %55, align 8, !tbaa !20
  %56 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 6
  store i64 0, i64* %56, align 8, !tbaa !20
  %57 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 6
  store i64 0, i64* %57, align 8, !tbaa !20
  %58 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 7
  store i64 1, i64* %58, align 8, !tbaa !20
  %59 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 7
  store i64 0, i64* %59, align 8, !tbaa !20
  %60 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 7
  store i64 0, i64* %60, align 8, !tbaa !20
  %61 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 8
  store i64 1, i64* %61, align 8, !tbaa !20
  %62 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 8
  store i64 0, i64* %62, align 8, !tbaa !20
  %63 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 8
  store i64 0, i64* %63, align 8, !tbaa !20
  %64 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 9
  store i64 1, i64* %64, align 8, !tbaa !20
  %65 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 9
  store i64 0, i64* %65, align 8, !tbaa !20
  %66 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 9
  store i64 0, i64* %66, align 8, !tbaa !20
  %67 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 10
  store i64 1, i64* %67, align 8, !tbaa !20
  %68 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 10
  store i64 0, i64* %68, align 8, !tbaa !20
  %69 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 10
  store i64 0, i64* %69, align 8, !tbaa !20
  %70 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 11
  store i64 1, i64* %70, align 8, !tbaa !20
  %71 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 11
  store i64 0, i64* %71, align 8, !tbaa !20
  %72 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 11
  store i64 0, i64* %72, align 8, !tbaa !20
  %73 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 12
  store i64 1, i64* %73, align 8, !tbaa !20
  %74 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 12
  store i64 0, i64* %74, align 8, !tbaa !20
  %75 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 12
  store i64 0, i64* %75, align 8, !tbaa !20
  %76 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 13
  store i64 1, i64* %76, align 8, !tbaa !20
  %77 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 13
  store i64 0, i64* %77, align 8, !tbaa !20
  %78 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 13
  store i64 0, i64* %78, align 8, !tbaa !20
  %79 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 14
  store i64 1, i64* %79, align 8, !tbaa !20
  %80 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 14
  store i64 0, i64* %80, align 8, !tbaa !20
  %81 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 14
  store i64 0, i64* %81, align 8, !tbaa !20
  %82 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 15
  store i64 1, i64* %82, align 8, !tbaa !20
  %83 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 15
  store i64 0, i64* %83, align 8, !tbaa !20
  %84 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 15
  store i64 0, i64* %84, align 8, !tbaa !20
  %85 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 2
  store i64 0, i64* %85, align 8, !tbaa !47
  %86 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 5
  %87 = load i32, i32* %86, align 4, !tbaa !34
  %88 = icmp sgt i32 %87, 0
  br i1 %88, label %89, label %109

; <label>:89:                                     ; preds = %27
  %90 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 6
  %91 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %90, align 8, !tbaa !35
  %92 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 6
  %93 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %92, align 8, !tbaa !35
  %94 = add i32 %87, -1
  %xtraiter20 = and i32 %87, 7
  %95 = icmp ult i32 %94, 7
  br i1 %95, label %.unr-lcssa, label %.new

.new:                                             ; preds = %89
  %unroll_iter = sub i32 %87, %xtraiter20
  br label %115

.unr-lcssa:                                       ; preds = %115, %89
  %.lcssa.ph = phi i64 [ undef, %89 ], [ %204, %115 ]
  %.unr21 = phi i64 [ 0, %89 ], [ %204, %115 ]
  %.unr22 = phi i32 [ 0, %89 ], [ %205, %115 ]
  %lcmp.mod23 = icmp eq i32 %xtraiter20, 0
  br i1 %lcmp.mod23, label %.epilog-lcssa, label %.epil.preheader

.epil.preheader:                                  ; preds = %.unr-lcssa, %.epil.preheader
  %96 = phi i64 [ %107, %.epil.preheader ], [ %.unr21, %.unr-lcssa ]
  %97 = phi i32 [ %108, %.epil.preheader ], [ %.unr22, %.unr-lcssa ]
  %epil.iter = phi i32 [ %epil.iter.sub, %.epil.preheader ], [ %xtraiter20, %.unr-lcssa ]
  %98 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %91, i32 %97, i32 2
  %99 = load i32, i32* %98, align 4, !tbaa !39
  %100 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %93, i32 %97, i32 0
  %101 = load i32, i32* %100, align 4, !tbaa !36
  %102 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %91, i32 %97, i32 0
  %103 = load i32, i32* %102, align 4, !tbaa !36
  %104 = sub nsw i32 %101, %103
  %105 = mul nsw i32 %104, %99
  %106 = sext i32 %105 to i64
  %107 = add i64 %96, %106
  %108 = add nuw nsw i32 %97, 1
  %epil.iter.sub = add i32 %epil.iter, -1
  %epil.iter.cmp = icmp eq i32 %epil.iter.sub, 0
  br i1 %epil.iter.cmp, label %.epilog-lcssa, label %.epil.preheader, !llvm.loop !48

.epilog-lcssa:                                    ; preds = %.epil.preheader, %.unr-lcssa
  %.lcssa = phi i64 [ %.lcssa.ph, %.unr-lcssa ], [ %107, %.epil.preheader ]
  store i64 %.lcssa, i64* %85, align 8, !tbaa !47
  br label %109

; <label>:109:                                    ; preds = %.epilog-lcssa, %27
  %110 = phi i64 [ %.lcssa, %.epilog-lcssa ], [ 0, %27 ]
  %111 = mul i64 %110, %35
  store i64 %111, i64* %85, align 8, !tbaa !47
  %112 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 5
  %113 = load i32, i32* %112, align 4, !tbaa !34
  %114 = icmp eq i32 %87, %113
  br i1 %114, label %206, label %215

; <label>:115:                                    ; preds = %115, %.new
  %116 = phi i64 [ 0, %.new ], [ %204, %115 ]
  %117 = phi i32 [ 0, %.new ], [ %205, %115 ]
  %niter = phi i32 [ %unroll_iter, %.new ], [ %niter.nsub.7, %115 ]
  %118 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %91, i32 %117, i32 2
  %119 = load i32, i32* %118, align 4, !tbaa !39
  %120 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %93, i32 %117, i32 0
  %121 = load i32, i32* %120, align 4, !tbaa !36
  %122 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %91, i32 %117, i32 0
  %123 = load i32, i32* %122, align 4, !tbaa !36
  %124 = sub nsw i32 %121, %123
  %125 = mul nsw i32 %124, %119
  %126 = sext i32 %125 to i64
  %127 = add i64 %116, %126
  %128 = or i32 %117, 1
  %129 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %91, i32 %128, i32 2
  %130 = load i32, i32* %129, align 4, !tbaa !39
  %131 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %93, i32 %128, i32 0
  %132 = load i32, i32* %131, align 4, !tbaa !36
  %133 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %91, i32 %128, i32 0
  %134 = load i32, i32* %133, align 4, !tbaa !36
  %135 = sub nsw i32 %132, %134
  %136 = mul nsw i32 %135, %130
  %137 = sext i32 %136 to i64
  %138 = add i64 %127, %137
  %139 = or i32 %117, 2
  %140 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %91, i32 %139, i32 2
  %141 = load i32, i32* %140, align 4, !tbaa !39
  %142 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %93, i32 %139, i32 0
  %143 = load i32, i32* %142, align 4, !tbaa !36
  %144 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %91, i32 %139, i32 0
  %145 = load i32, i32* %144, align 4, !tbaa !36
  %146 = sub nsw i32 %143, %145
  %147 = mul nsw i32 %146, %141
  %148 = sext i32 %147 to i64
  %149 = add i64 %138, %148
  %150 = or i32 %117, 3
  %151 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %91, i32 %150, i32 2
  %152 = load i32, i32* %151, align 4, !tbaa !39
  %153 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %93, i32 %150, i32 0
  %154 = load i32, i32* %153, align 4, !tbaa !36
  %155 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %91, i32 %150, i32 0
  %156 = load i32, i32* %155, align 4, !tbaa !36
  %157 = sub nsw i32 %154, %156
  %158 = mul nsw i32 %157, %152
  %159 = sext i32 %158 to i64
  %160 = add i64 %149, %159
  %161 = or i32 %117, 4
  %162 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %91, i32 %161, i32 2
  %163 = load i32, i32* %162, align 4, !tbaa !39
  %164 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %93, i32 %161, i32 0
  %165 = load i32, i32* %164, align 4, !tbaa !36
  %166 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %91, i32 %161, i32 0
  %167 = load i32, i32* %166, align 4, !tbaa !36
  %168 = sub nsw i32 %165, %167
  %169 = mul nsw i32 %168, %163
  %170 = sext i32 %169 to i64
  %171 = add i64 %160, %170
  %172 = or i32 %117, 5
  %173 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %91, i32 %172, i32 2
  %174 = load i32, i32* %173, align 4, !tbaa !39
  %175 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %93, i32 %172, i32 0
  %176 = load i32, i32* %175, align 4, !tbaa !36
  %177 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %91, i32 %172, i32 0
  %178 = load i32, i32* %177, align 4, !tbaa !36
  %179 = sub nsw i32 %176, %178
  %180 = mul nsw i32 %179, %174
  %181 = sext i32 %180 to i64
  %182 = add i64 %171, %181
  %183 = or i32 %117, 6
  %184 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %91, i32 %183, i32 2
  %185 = load i32, i32* %184, align 4, !tbaa !39
  %186 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %93, i32 %183, i32 0
  %187 = load i32, i32* %186, align 4, !tbaa !36
  %188 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %91, i32 %183, i32 0
  %189 = load i32, i32* %188, align 4, !tbaa !36
  %190 = sub nsw i32 %187, %189
  %191 = mul nsw i32 %190, %185
  %192 = sext i32 %191 to i64
  %193 = add i64 %182, %192
  %194 = or i32 %117, 7
  %195 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %91, i32 %194, i32 2
  %196 = load i32, i32* %195, align 4, !tbaa !39
  %197 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %93, i32 %194, i32 0
  %198 = load i32, i32* %197, align 4, !tbaa !36
  %199 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %91, i32 %194, i32 0
  %200 = load i32, i32* %199, align 4, !tbaa !36
  %201 = sub nsw i32 %198, %200
  %202 = mul nsw i32 %201, %196
  %203 = sext i32 %202 to i64
  %204 = add i64 %193, %203
  %205 = add nuw nsw i32 %117, 8
  %niter.nsub.7 = add i32 %niter, -8
  %niter.ncmp.7 = icmp eq i32 %niter.nsub.7, 0
  br i1 %niter.ncmp.7, label %.unr-lcssa, label %115

; <label>:206:                                    ; preds = %109
  %207 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 4, i32 1
  %208 = load i8, i8* %207, align 1, !tbaa !27
  %209 = zext i8 %208 to i32
  %210 = add nuw nsw i32 %209, 7
  %211 = lshr i32 %210, 3
  %212 = icmp ne i32 %34, %211
  %213 = icmp sgt i32 %87, 16
  %214 = or i1 %213, %212
  br i1 %214, label %215, label %217

; <label>:215:                                    ; preds = %206, %109
  %216 = bitcast %"struct.Halide::Runtime::Internal::device_copy"* %0 to i8*
  tail call void @llvm.memset.p0i8.i32(i8* align 8 %216, i8 0, i32 416, i1 false)
  br label %413

; <label>:217:                                    ; preds = %206
  %218 = icmp eq i32 %34, 0
  br i1 %218, label %225, label %219

; <label>:219:                                    ; preds = %217
  br i1 %88, label %220, label %.loopexit

; <label>:220:                                    ; preds = %219
  %221 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 6
  %222 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %221, align 8, !tbaa !35
  %223 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 6
  %224 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %223, align 8, !tbaa !35
  br label %233

; <label>:225:                                    ; preds = %217
  %226 = bitcast %"struct.Halide::Runtime::Internal::device_copy"* %0 to i8*
  tail call void @llvm.memset.p0i8.i32(i8* align 8 %226, i8 0, i32 416, i1 false)
  br label %413

; <label>:227:                                    ; preds = %.loopexit10
  %228 = load i64, i64* %36, align 8, !tbaa !46
  %229 = load i64, i64* %38, align 8, !tbaa !20
  %230 = icmp eq i64 %228, %229
  br i1 %230, label %231, label %.loopexit

; <label>:231:                                    ; preds = %227
  %232 = load i64, i64* %39, align 8, !tbaa !20
  br label %359

; <label>:233:                                    ; preds = %.loopexit10, %220
  %234 = phi i32 [ 0, %220 ], [ %276, %.loopexit10 ]
  %235 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %222, i32 %234, i32 2
  %236 = load i32, i32* %235, align 4, !tbaa !39
  %237 = mul nsw i32 %236, %34
  %238 = sext i32 %237 to i64
  %239 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %224, i32 %234, i32 2
  %240 = load i32, i32* %239, align 4, !tbaa !39
  %241 = mul nsw i32 %240, %34
  %242 = sext i32 %241 to i64
  %243 = icmp eq i32 %234, 0
  br i1 %243, label %.loopexit12, label %244

; <label>:244:                                    ; preds = %233
  %245 = icmp eq i32 %237, 0
  br i1 %245, label %.loopexit12, label %.preheader11

.preheader11:                                     ; preds = %244, %250
  %246 = phi i32 [ %251, %250 ], [ 0, %244 ]
  %247 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 %246
  %248 = load i64, i64* %247, align 8, !tbaa !20
  %249 = icmp ugt i64 %248, %238
  br i1 %249, label %.loopexit12, label %250

; <label>:250:                                    ; preds = %.preheader11
  %251 = add nuw nsw i32 %246, 1
  %252 = icmp ult i32 %251, %234
  br i1 %252, label %.preheader11, label %.loopexit12

.loopexit12:                                      ; preds = %250, %.preheader11, %244, %233
  %253 = phi i32 [ 0, %233 ], [ %234, %244 ], [ %251, %250 ], [ %246, %.preheader11 ]
  %254 = icmp ugt i32 %234, %253
  br i1 %254, label %.preheader.preheader, label %.loopexit10

.preheader.preheader:                             ; preds = %.loopexit12
  %255 = sub i32 %234, %253
  %256 = xor i32 %253, -1
  %257 = add i32 %234, %256
  %xtraiter = and i32 %255, 7
  %lcmp.mod = icmp eq i32 %xtraiter, 0
  br i1 %lcmp.mod, label %.preheader.prol.loopexit, label %.preheader.prol

.preheader.prol:                                  ; preds = %.preheader.preheader, %.preheader.prol
  %258 = phi i32 [ %259, %.preheader.prol ], [ %234, %.preheader.preheader ]
  %prol.iter = phi i32 [ %prol.iter.sub, %.preheader.prol ], [ %xtraiter, %.preheader.preheader ]
  %259 = add nsw i32 %258, -1
  %260 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 %259
  %261 = load i64, i64* %260, align 8, !tbaa !20
  %262 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 %258
  store i64 %261, i64* %262, align 8, !tbaa !20
  %263 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 %259
  %264 = load i64, i64* %263, align 8, !tbaa !20
  %265 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 %258
  store i64 %264, i64* %265, align 8, !tbaa !20
  %266 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 %259
  %267 = load i64, i64* %266, align 8, !tbaa !20
  %268 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 %258
  store i64 %267, i64* %268, align 8, !tbaa !20
  %prol.iter.sub = add i32 %prol.iter, -1
  %prol.iter.cmp = icmp eq i32 %prol.iter.sub, 0
  br i1 %prol.iter.cmp, label %.preheader.prol.loopexit, label %.preheader.prol, !llvm.loop !49

.preheader.prol.loopexit:                         ; preds = %.preheader.prol, %.preheader.preheader
  %.unr = phi i32 [ %234, %.preheader.preheader ], [ %259, %.preheader.prol ]
  %269 = icmp ult i32 %257, 7
  br i1 %269, label %.loopexit10, label %.preheader

.loopexit10:                                      ; preds = %.preheader.prol.loopexit, %.preheader, %.loopexit12
  %270 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %222, i32 %234, i32 1
  %271 = load i32, i32* %270, align 4, !tbaa !38
  %272 = sext i32 %271 to i64
  %273 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 %253
  store i64 %272, i64* %273, align 8, !tbaa !20
  %274 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 %253
  store i64 %238, i64* %274, align 8, !tbaa !20
  %275 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 %253
  store i64 %242, i64* %275, align 8, !tbaa !20
  %276 = add nuw nsw i32 %234, 1
  %exitcond = icmp eq i32 %276, %87
  br i1 %exitcond, label %227, label %233

.preheader:                                       ; preds = %.preheader.prol.loopexit, %.preheader
  %277 = phi i32 [ %348, %.preheader ], [ %.unr, %.preheader.prol.loopexit ]
  %278 = add nsw i32 %277, -1
  %279 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 %278
  %280 = load i64, i64* %279, align 8, !tbaa !20
  %281 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 %277
  store i64 %280, i64* %281, align 8, !tbaa !20
  %282 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 %278
  %283 = load i64, i64* %282, align 8, !tbaa !20
  %284 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 %277
  store i64 %283, i64* %284, align 8, !tbaa !20
  %285 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 %278
  %286 = load i64, i64* %285, align 8, !tbaa !20
  %287 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 %277
  store i64 %286, i64* %287, align 8, !tbaa !20
  %288 = add nsw i32 %277, -2
  %289 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 %288
  %290 = load i64, i64* %289, align 8, !tbaa !20
  %291 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 %278
  store i64 %290, i64* %291, align 8, !tbaa !20
  %292 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 %288
  %293 = load i64, i64* %292, align 8, !tbaa !20
  %294 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 %278
  store i64 %293, i64* %294, align 8, !tbaa !20
  %295 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 %288
  %296 = load i64, i64* %295, align 8, !tbaa !20
  %297 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 %278
  store i64 %296, i64* %297, align 8, !tbaa !20
  %298 = add nsw i32 %277, -3
  %299 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 %298
  %300 = load i64, i64* %299, align 8, !tbaa !20
  %301 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 %288
  store i64 %300, i64* %301, align 8, !tbaa !20
  %302 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 %298
  %303 = load i64, i64* %302, align 8, !tbaa !20
  %304 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 %288
  store i64 %303, i64* %304, align 8, !tbaa !20
  %305 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 %298
  %306 = load i64, i64* %305, align 8, !tbaa !20
  %307 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 %288
  store i64 %306, i64* %307, align 8, !tbaa !20
  %308 = add nsw i32 %277, -4
  %309 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 %308
  %310 = load i64, i64* %309, align 8, !tbaa !20
  %311 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 %298
  store i64 %310, i64* %311, align 8, !tbaa !20
  %312 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 %308
  %313 = load i64, i64* %312, align 8, !tbaa !20
  %314 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 %298
  store i64 %313, i64* %314, align 8, !tbaa !20
  %315 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 %308
  %316 = load i64, i64* %315, align 8, !tbaa !20
  %317 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 %298
  store i64 %316, i64* %317, align 8, !tbaa !20
  %318 = add nsw i32 %277, -5
  %319 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 %318
  %320 = load i64, i64* %319, align 8, !tbaa !20
  %321 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 %308
  store i64 %320, i64* %321, align 8, !tbaa !20
  %322 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 %318
  %323 = load i64, i64* %322, align 8, !tbaa !20
  %324 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 %308
  store i64 %323, i64* %324, align 8, !tbaa !20
  %325 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 %318
  %326 = load i64, i64* %325, align 8, !tbaa !20
  %327 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 %308
  store i64 %326, i64* %327, align 8, !tbaa !20
  %328 = add nsw i32 %277, -6
  %329 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 %328
  %330 = load i64, i64* %329, align 8, !tbaa !20
  %331 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 %318
  store i64 %330, i64* %331, align 8, !tbaa !20
  %332 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 %328
  %333 = load i64, i64* %332, align 8, !tbaa !20
  %334 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 %318
  store i64 %333, i64* %334, align 8, !tbaa !20
  %335 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 %328
  %336 = load i64, i64* %335, align 8, !tbaa !20
  %337 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 %318
  store i64 %336, i64* %337, align 8, !tbaa !20
  %338 = add nsw i32 %277, -7
  %339 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 %338
  %340 = load i64, i64* %339, align 8, !tbaa !20
  %341 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 %328
  store i64 %340, i64* %341, align 8, !tbaa !20
  %342 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 %338
  %343 = load i64, i64* %342, align 8, !tbaa !20
  %344 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 %328
  store i64 %343, i64* %344, align 8, !tbaa !20
  %345 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 %338
  %346 = load i64, i64* %345, align 8, !tbaa !20
  %347 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 %328
  store i64 %346, i64* %347, align 8, !tbaa !20
  %348 = add nsw i32 %277, -8
  %349 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 %348
  %350 = load i64, i64* %349, align 8, !tbaa !20
  %351 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 3, i32 %338
  store i64 %350, i64* %351, align 8, !tbaa !20
  %352 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 %348
  %353 = load i64, i64* %352, align 8, !tbaa !20
  %354 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 5, i32 %338
  store i64 %353, i64* %354, align 8, !tbaa !20
  %355 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 %348
  %356 = load i64, i64* %355, align 8, !tbaa !20
  %357 = getelementptr inbounds %"struct.Halide::Runtime::Internal::device_copy", %"struct.Halide::Runtime::Internal::device_copy"* %6, i32 0, i32 4, i32 %338
  store i64 %356, i64* %357, align 8, !tbaa !20
  %358 = icmp sgt i32 %348, %253
  br i1 %358, label %.preheader, label %.loopexit10

; <label>:359:                                    ; preds = %363, %231
  %360 = phi i64 [ %232, %231 ], [ %368, %363 ]
  %361 = phi i64 [ %228, %231 ], [ %365, %363 ]
  %362 = icmp eq i64 %361, %360
  br i1 %362, label %363, label %.loopexit

; <label>:363:                                    ; preds = %359
  %364 = load i64, i64* %37, align 8, !tbaa !20
  %365 = mul i64 %364, %360
  store i64 %365, i64* %36, align 8, !tbaa !46
  %366 = load i64, i64* %40, align 8, !tbaa !20
  store i64 %366, i64* %37, align 8, !tbaa !20
  %367 = load i64, i64* %41, align 8, !tbaa !20
  store i64 %367, i64* %38, align 8, !tbaa !20
  %368 = load i64, i64* %42, align 8, !tbaa !20
  store i64 %368, i64* %39, align 8, !tbaa !20
  %369 = load i64, i64* %43, align 8, !tbaa !20
  store i64 %369, i64* %40, align 8, !tbaa !20
  %370 = load i64, i64* %44, align 8, !tbaa !20
  store i64 %370, i64* %41, align 8, !tbaa !20
  %371 = load i64, i64* %45, align 8, !tbaa !20
  store i64 %371, i64* %42, align 8, !tbaa !20
  %372 = load i64, i64* %46, align 8, !tbaa !20
  store i64 %372, i64* %43, align 8, !tbaa !20
  %373 = load i64, i64* %47, align 8, !tbaa !20
  store i64 %373, i64* %44, align 8, !tbaa !20
  %374 = load i64, i64* %48, align 8, !tbaa !20
  store i64 %374, i64* %45, align 8, !tbaa !20
  %375 = load i64, i64* %49, align 8, !tbaa !20
  store i64 %375, i64* %46, align 8, !tbaa !20
  %376 = load i64, i64* %50, align 8, !tbaa !20
  store i64 %376, i64* %47, align 8, !tbaa !20
  %377 = load i64, i64* %51, align 8, !tbaa !20
  store i64 %377, i64* %48, align 8, !tbaa !20
  %378 = load i64, i64* %52, align 8, !tbaa !20
  store i64 %378, i64* %49, align 8, !tbaa !20
  %379 = load i64, i64* %53, align 8, !tbaa !20
  store i64 %379, i64* %50, align 8, !tbaa !20
  %380 = load i64, i64* %54, align 8, !tbaa !20
  store i64 %380, i64* %51, align 8, !tbaa !20
  %381 = load i64, i64* %55, align 8, !tbaa !20
  store i64 %381, i64* %52, align 8, !tbaa !20
  %382 = load i64, i64* %56, align 8, !tbaa !20
  store i64 %382, i64* %53, align 8, !tbaa !20
  %383 = load i64, i64* %57, align 8, !tbaa !20
  store i64 %383, i64* %54, align 8, !tbaa !20
  %384 = load i64, i64* %58, align 8, !tbaa !20
  store i64 %384, i64* %55, align 8, !tbaa !20
  %385 = load i64, i64* %59, align 8, !tbaa !20
  store i64 %385, i64* %56, align 8, !tbaa !20
  %386 = load i64, i64* %60, align 8, !tbaa !20
  store i64 %386, i64* %57, align 8, !tbaa !20
  %387 = load i64, i64* %61, align 8, !tbaa !20
  store i64 %387, i64* %58, align 8, !tbaa !20
  %388 = load i64, i64* %62, align 8, !tbaa !20
  store i64 %388, i64* %59, align 8, !tbaa !20
  %389 = load i64, i64* %63, align 8, !tbaa !20
  store i64 %389, i64* %60, align 8, !tbaa !20
  %390 = load i64, i64* %64, align 8, !tbaa !20
  store i64 %390, i64* %61, align 8, !tbaa !20
  %391 = load i64, i64* %65, align 8, !tbaa !20
  store i64 %391, i64* %62, align 8, !tbaa !20
  %392 = load i64, i64* %66, align 8, !tbaa !20
  store i64 %392, i64* %63, align 8, !tbaa !20
  %393 = load i64, i64* %67, align 8, !tbaa !20
  store i64 %393, i64* %64, align 8, !tbaa !20
  %394 = load i64, i64* %68, align 8, !tbaa !20
  store i64 %394, i64* %65, align 8, !tbaa !20
  %395 = load i64, i64* %69, align 8, !tbaa !20
  store i64 %395, i64* %66, align 8, !tbaa !20
  %396 = load i64, i64* %70, align 8, !tbaa !20
  store i64 %396, i64* %67, align 8, !tbaa !20
  %397 = load i64, i64* %71, align 8, !tbaa !20
  store i64 %397, i64* %68, align 8, !tbaa !20
  %398 = load i64, i64* %72, align 8, !tbaa !20
  store i64 %398, i64* %69, align 8, !tbaa !20
  %399 = load i64, i64* %73, align 8, !tbaa !20
  store i64 %399, i64* %70, align 8, !tbaa !20
  %400 = load i64, i64* %74, align 8, !tbaa !20
  store i64 %400, i64* %71, align 8, !tbaa !20
  %401 = load i64, i64* %75, align 8, !tbaa !20
  store i64 %401, i64* %72, align 8, !tbaa !20
  %402 = load i64, i64* %76, align 8, !tbaa !20
  store i64 %402, i64* %73, align 8, !tbaa !20
  %403 = load i64, i64* %77, align 8, !tbaa !20
  store i64 %403, i64* %74, align 8, !tbaa !20
  %404 = load i64, i64* %78, align 8, !tbaa !20
  store i64 %404, i64* %75, align 8, !tbaa !20
  %405 = load i64, i64* %79, align 8, !tbaa !20
  store i64 %405, i64* %76, align 8, !tbaa !20
  %406 = load i64, i64* %80, align 8, !tbaa !20
  store i64 %406, i64* %77, align 8, !tbaa !20
  %407 = load i64, i64* %81, align 8, !tbaa !20
  store i64 %407, i64* %78, align 8, !tbaa !20
  %408 = load i64, i64* %82, align 8, !tbaa !20
  store i64 %408, i64* %79, align 8, !tbaa !20
  %409 = load i64, i64* %83, align 8, !tbaa !20
  store i64 %409, i64* %80, align 8, !tbaa !20
  %410 = load i64, i64* %84, align 8, !tbaa !20
  store i64 %410, i64* %81, align 8, !tbaa !20
  store i64 1, i64* %82, align 8, !tbaa !20
  store i64 0, i64* %83, align 8, !tbaa !20
  store i64 0, i64* %84, align 8, !tbaa !20
  %411 = icmp eq i64 %365, %367
  br i1 %411, label %359, label %.loopexit

.loopexit:                                        ; preds = %363, %359, %227, %219
  %412 = bitcast %"struct.Halide::Runtime::Internal::device_copy"* %0 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i32(i8* align 8 %412, i8* nonnull align 8 %7, i32 416, i1 false), !tbaa.struct !50
  br label %413

; <label>:413:                                    ; preds = %.loopexit, %225, %215
  call void @llvm.lifetime.end.p0i8(i64 416, i8* nonnull %7) #9
  ret void
}

; Function Attrs: nounwind
define linkonce i32 @_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t(i8*, %struct.halide_buffer_t*) local_unnamed_addr #0 {
  %3 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 3
  %4 = load i64, i64* %3, align 8, !tbaa !33
  %5 = and i64 %4, 2
  %6 = icmp eq i64 %5, 0
  br i1 %6, label %26, label %7

; <label>:7:                                      ; preds = %2
  %8 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %9 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %8, align 8, !tbaa !31
  %10 = and i64 %4, 1
  %11 = icmp eq i64 %10, 0
  br i1 %11, label %12, label %26

; <label>:12:                                     ; preds = %7
  %13 = icmp eq %struct.halide_device_interface_t* %9, null
  br i1 %13, label %26, label %14

; <label>:14:                                     ; preds = %12
  %15 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %9, i32 0, i32 15
  %16 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %15, align 4, !tbaa !51
  %17 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %16, i32 0, i32 6
  %18 = load i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)** %17, align 4, !tbaa !53
  %19 = tail call i32 %18(i8* %0, %struct.halide_buffer_t* nonnull %1) #11
  %20 = icmp eq i32 %19, 0
  br i1 %20, label %21, label %26

; <label>:21:                                     ; preds = %14
  %22 = load i64, i64* %3, align 8, !tbaa !33
  %23 = and i64 %22, -3
  store i64 %23, i64* %3, align 8, !tbaa !33
  %24 = bitcast %struct.halide_buffer_t* %1 to %struct.halide_buffer_t.23*
  %25 = tail call i32 @halide_msan_annotate_buffer_is_initialized(i8* %0, %struct.halide_buffer_t.23* nonnull %24) #11
  br label %26

; <label>:26:                                     ; preds = %21, %14, %12, %7, %2
  %27 = phi i32 [ 0, %2 ], [ 0, %21 ], [ -14, %7 ], [ -19, %12 ], [ -14, %14 ]
  ret i32 %27
}

; Function Attrs: nounwind
define weak void @halide_device_release(i8*, %struct.halide_device_interface_t*) local_unnamed_addr #0 {
  %3 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %1, i32 0, i32 15
  %4 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %3, align 4, !tbaa !51
  %5 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %4, i32 0, i32 5
  %6 = load i32 (i8*)*, i32 (i8*)** %5, align 4, !tbaa !55
  %7 = tail call i32 %6(i8* %0) #11
  ret void
}

; Function Attrs: nounwind
define weak i32 @halide_copy_to_host(i8*, %struct.halide_buffer_t*) local_unnamed_addr #0 {
  tail call void @halide_mutex_lock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #11
  %3 = icmp eq %struct.halide_buffer_t* %1, null
  br i1 %3, label %4, label %6

; <label>:4:                                      ; preds = %2
  %5 = tail call i32 @halide_error_buffer_is_null(i8* %0, i8* getelementptr inbounds ([20 x i8], [20 x i8]* @.str.9.17, i32 0, i32 0)) #11
  br label %30

; <label>:6:                                      ; preds = %2
  %7 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %8 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %7, align 8, !tbaa !31
  %9 = icmp ne %struct.halide_device_interface_t* %8, null
  %10 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %11 = load i64, i64* %10, align 8, !tbaa !29
  %12 = icmp ne i64 %11, 0
  %13 = xor i1 %12, true
  %14 = or i1 %9, %13
  br i1 %14, label %17, label %15

; <label>:15:                                     ; preds = %6
  %16 = tail call i32 @halide_error_no_device_interface(i8* %0) #11
  br label %30

; <label>:17:                                     ; preds = %6
  %18 = xor i1 %9, true
  %19 = or i1 %12, %18
  br i1 %19, label %22, label %20

; <label>:20:                                     ; preds = %17
  %21 = tail call i32 @halide_error_device_interface_no_device(i8* %0) #11
  br label %30

; <label>:22:                                     ; preds = %17
  %23 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 3
  %24 = load i64, i64* %23, align 8, !tbaa !33
  %25 = and i64 %24, 3
  %26 = icmp eq i64 %25, 3
  br i1 %26, label %28, label %.split

.split:                                           ; preds = %22
  %27 = tail call i32 @_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t(i8* %0, %struct.halide_buffer_t* nonnull %1) #12
  br label %34

; <label>:28:                                     ; preds = %22
  %29 = tail call i32 @halide_error_host_and_device_dirty(i8* %0) #11
  br label %30

; <label>:30:                                     ; preds = %28, %20, %15, %4
  %31 = phi i32 [ %5, %4 ], [ %21, %20 ], [ %16, %15 ], [ %29, %28 ]
  %32 = icmp eq i32 %31, 0
  br i1 %32, label %.split1, label %34

.split1:                                          ; preds = %30
  %33 = tail call i32 @_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t(i8* %0, %struct.halide_buffer_t* %1) #12
  br label %34

; <label>:34:                                     ; preds = %.split, %.split1, %30
  %35 = phi i32 [ %31, %30 ], [ %27, %.split ], [ %33, %.split1 ]
  tail call void @halide_mutex_unlock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #11
  ret i32 %35
}

; Function Attrs: nounwind
define linkonce i32 @copy_to_device_already_locked(i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*) local_unnamed_addr #0 {
  %4 = icmp eq %struct.halide_buffer_t* %1, null
  br i1 %4, label %5, label %7

; <label>:5:                                      ; preds = %3
  %6 = tail call i32 @halide_error_buffer_is_null(i8* %0, i8* getelementptr inbounds ([22 x i8], [22 x i8]* @.str.10.18, i32 0, i32 0)) #11
  br label %30

; <label>:7:                                      ; preds = %3
  %8 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %9 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %8, align 8, !tbaa !31
  %10 = icmp ne %struct.halide_device_interface_t* %9, null
  %11 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %12 = load i64, i64* %11, align 8, !tbaa !29
  %13 = icmp ne i64 %12, 0
  %14 = xor i1 %13, true
  %15 = or i1 %10, %14
  br i1 %15, label %18, label %16

; <label>:16:                                     ; preds = %7
  %17 = tail call i32 @halide_error_no_device_interface(i8* %0) #11
  br label %30

; <label>:18:                                     ; preds = %7
  %19 = xor i1 %10, true
  %20 = or i1 %13, %19
  br i1 %20, label %23, label %21

; <label>:21:                                     ; preds = %18
  %22 = tail call i32 @halide_error_device_interface_no_device(i8* %0) #11
  br label %30

; <label>:23:                                     ; preds = %18
  %24 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 3
  %25 = load i64, i64* %24, align 8, !tbaa !33
  %26 = and i64 %25, 3
  %27 = icmp eq i64 %26, 3
  br i1 %27, label %28, label %33

; <label>:28:                                     ; preds = %23
  %29 = tail call i32 @halide_error_host_and_device_dirty(i8* %0) #11
  br label %30

; <label>:30:                                     ; preds = %28, %21, %16, %5
  %31 = phi i32 [ %6, %5 ], [ %22, %21 ], [ %17, %16 ], [ %29, %28 ]
  %32 = icmp eq i32 %31, 0
  br i1 %32, label %33, label %72

; <label>:33:                                     ; preds = %30, %23
  %34 = icmp eq %struct.halide_device_interface_t* %2, null
  br i1 %34, label %35, label %41

; <label>:35:                                     ; preds = %33
  %36 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %37 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %36, align 8, !tbaa !31
  %38 = icmp eq %struct.halide_device_interface_t* %37, null
  br i1 %38, label %39, label %41

; <label>:39:                                     ; preds = %35
  %40 = tail call i32 @halide_error_no_device_interface(i8* %0) #11
  br label %72

; <label>:41:                                     ; preds = %35, %33
  %42 = phi %struct.halide_device_interface_t* [ %2, %33 ], [ %37, %35 ]
  %43 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %44 = load i64, i64* %43, align 8, !tbaa !29
  %45 = icmp eq i64 %44, 0
  br i1 %45, label %51, label %46

; <label>:46:                                     ; preds = %41
  %47 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %48 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %47, align 8, !tbaa !31
  %49 = icmp eq %struct.halide_device_interface_t* %48, %42
  br i1 %49, label %54, label %50

; <label>:50:                                     ; preds = %46
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([61 x i8], [61 x i8]* @.str.12.19, i32 0, i32 0)) #11
  br label %72

; <label>:51:                                     ; preds = %41
  %52 = tail call i32 @halide_device_malloc(i8* %0, %struct.halide_buffer_t* nonnull %1, %struct.halide_device_interface_t* nonnull %42) #12
  %53 = icmp eq i32 %52, 0
  br i1 %53, label %54, label %72

; <label>:54:                                     ; preds = %51, %46
  %55 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 3
  %56 = load i64, i64* %55, align 8, !tbaa !33
  %57 = and i64 %56, 1
  %58 = icmp eq i64 %57, 0
  br i1 %58, label %72, label %59

; <label>:59:                                     ; preds = %54
  %60 = and i64 %56, 2
  %61 = icmp eq i64 %60, 0
  br i1 %61, label %62, label %72

; <label>:62:                                     ; preds = %59
  %63 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %42, i32 0, i32 15
  %64 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %63, align 4, !tbaa !51
  %65 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %64, i32 0, i32 7
  %66 = load i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)** %65, align 4, !tbaa !56
  %67 = tail call i32 %66(i8* %0, %struct.halide_buffer_t* nonnull %1) #11
  %68 = icmp eq i32 %67, 0
  br i1 %68, label %69, label %72

; <label>:69:                                     ; preds = %62
  %70 = load i64, i64* %55, align 8, !tbaa !33
  %71 = and i64 %70, -2
  store i64 %71, i64* %55, align 8, !tbaa !33
  br label %72

; <label>:72:                                     ; preds = %69, %62, %59, %54, %51, %50, %39, %30
  %73 = phi i32 [ %40, %39 ], [ -42, %50 ], [ %31, %30 ], [ 0, %69 ], [ 0, %54 ], [ %52, %51 ], [ -15, %59 ], [ -15, %62 ]
  ret i32 %73
}

; Function Attrs: nounwind
define weak i32 @halide_device_malloc(i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*) local_unnamed_addr #0 {
  %4 = icmp eq %struct.halide_buffer_t* %1, null
  br i1 %4, label %5, label %7

; <label>:5:                                      ; preds = %3
  %6 = tail call i32 @halide_error_buffer_is_null(i8* %0, i8* getelementptr inbounds ([21 x i8], [21 x i8]* @.str.18.20, i32 0, i32 0)) #11
  br label %30

; <label>:7:                                      ; preds = %3
  %8 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %9 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %8, align 8, !tbaa !31
  %10 = icmp ne %struct.halide_device_interface_t* %9, null
  %11 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %12 = load i64, i64* %11, align 8, !tbaa !29
  %13 = icmp ne i64 %12, 0
  %14 = xor i1 %13, true
  %15 = or i1 %10, %14
  br i1 %15, label %18, label %16

; <label>:16:                                     ; preds = %7
  %17 = tail call i32 @halide_error_no_device_interface(i8* %0) #11
  br label %30

; <label>:18:                                     ; preds = %7
  %19 = xor i1 %10, true
  %20 = or i1 %13, %19
  br i1 %20, label %23, label %21

; <label>:21:                                     ; preds = %18
  %22 = tail call i32 @halide_error_device_interface_no_device(i8* %0) #11
  br label %30

; <label>:23:                                     ; preds = %18
  %24 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 3
  %25 = load i64, i64* %24, align 8, !tbaa !33
  %26 = and i64 %25, 3
  %27 = icmp eq i64 %26, 3
  br i1 %27, label %28, label %36

; <label>:28:                                     ; preds = %23
  %29 = tail call i32 @halide_error_host_and_device_dirty(i8* %0) #11
  br label %30

; <label>:30:                                     ; preds = %28, %21, %16, %5
  %31 = phi i32 [ %6, %5 ], [ %22, %21 ], [ %17, %16 ], [ %29, %28 ]
  %32 = icmp eq i32 %31, 0
  br i1 %32, label %33, label %56

; <label>:33:                                     ; preds = %30
  %34 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %35 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %34, align 8, !tbaa !31
  br label %36

; <label>:36:                                     ; preds = %33, %23
  %37 = phi %struct.halide_device_interface_t* [ %35, %33 ], [ %9, %23 ]
  %38 = icmp eq %struct.halide_device_interface_t* %37, null
  %39 = icmp eq %struct.halide_device_interface_t* %37, %2
  %40 = or i1 %38, %39
  br i1 %40, label %42, label %41

; <label>:41:                                     ; preds = %36
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([59 x i8], [59 x i8]* @.str.20.21, i32 0, i32 0)) #11
  br label %56

; <label>:42:                                     ; preds = %36
  %43 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %2, i32 0, i32 15
  %44 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %43, align 4, !tbaa !51
  %45 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %44, i32 0, i32 0
  %46 = load void ()*, void ()** %45, align 4, !tbaa !57
  tail call void %46() #11
  %47 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %43, align 4, !tbaa !51
  %48 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %47, i32 0, i32 2
  %49 = load i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)** %48, align 4, !tbaa !58
  %50 = tail call i32 %49(i8* %0, %struct.halide_buffer_t* nonnull %1) #11
  %51 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %43, align 4, !tbaa !51
  %52 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %51, i32 0, i32 1
  %53 = load void ()*, void ()** %52, align 4, !tbaa !59
  tail call void %53() #11
  %54 = icmp eq i32 %50, 0
  %55 = select i1 %54, i32 0, i32 -16
  br label %56

; <label>:56:                                     ; preds = %42, %41, %30
  %57 = phi i32 [ %31, %30 ], [ -42, %41 ], [ %55, %42 ]
  ret i32 %57
}

; Function Attrs: nounwind
define weak i32 @halide_copy_to_device(i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*) local_unnamed_addr #0 {
  tail call void @halide_mutex_lock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #11
  %4 = tail call i32 @copy_to_device_already_locked(i8* %0, %struct.halide_buffer_t* %1, %struct.halide_device_interface_t* %2) #12
  tail call void @halide_mutex_unlock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #11
  ret i32 %4
}

; Function Attrs: nounwind
define weak i32 @halide_device_sync(i8*, %struct.halide_buffer_t*) local_unnamed_addr #0 {
  %3 = icmp eq %struct.halide_buffer_t* %1, null
  br i1 %3, label %4, label %6

; <label>:4:                                      ; preds = %2
  %5 = tail call i32 @halide_error_buffer_is_null(i8* %0, i8* getelementptr inbounds ([19 x i8], [19 x i8]* @.str.17.22, i32 0, i32 0)) #11
  br label %29

; <label>:6:                                      ; preds = %2
  %7 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %8 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %7, align 8, !tbaa !31
  %9 = icmp ne %struct.halide_device_interface_t* %8, null
  %10 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %11 = load i64, i64* %10, align 8, !tbaa !29
  %12 = icmp ne i64 %11, 0
  %13 = xor i1 %12, true
  %14 = or i1 %9, %13
  br i1 %14, label %17, label %15

; <label>:15:                                     ; preds = %6
  %16 = tail call i32 @halide_error_no_device_interface(i8* %0) #11
  br label %29

; <label>:17:                                     ; preds = %6
  %18 = xor i1 %9, true
  %19 = or i1 %12, %18
  br i1 %19, label %22, label %20

; <label>:20:                                     ; preds = %17
  %21 = tail call i32 @halide_error_device_interface_no_device(i8* %0) #11
  br label %29

; <label>:22:                                     ; preds = %17
  %23 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 3
  %24 = load i64, i64* %23, align 8, !tbaa !33
  %25 = and i64 %24, 3
  %26 = icmp eq i64 %25, 3
  br i1 %26, label %27, label %35

; <label>:27:                                     ; preds = %22
  %28 = tail call i32 @halide_error_host_and_device_dirty(i8* %0) #11
  br label %29

; <label>:29:                                     ; preds = %27, %20, %15, %4
  %30 = phi i32 [ %5, %4 ], [ %21, %20 ], [ %16, %15 ], [ %28, %27 ]
  %31 = icmp eq i32 %30, 0
  br i1 %31, label %32, label %48

; <label>:32:                                     ; preds = %29
  %33 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %34 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %33, align 8, !tbaa !31
  br label %35

; <label>:35:                                     ; preds = %32, %22
  %36 = phi %struct.halide_device_interface_t* [ %34, %32 ], [ %8, %22 ]
  %37 = icmp eq %struct.halide_device_interface_t* %36, null
  br i1 %37, label %38, label %40

; <label>:38:                                     ; preds = %35
  %39 = tail call i32 @halide_error_no_device_interface(i8* %0) #11
  br label %48

; <label>:40:                                     ; preds = %35
  %41 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %36, i32 0, i32 15
  %42 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %41, align 4, !tbaa !51
  %43 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %42, i32 0, i32 4
  %44 = load i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)** %43, align 4, !tbaa !60
  %45 = tail call i32 %44(i8* %0, %struct.halide_buffer_t* nonnull %1) #11
  %46 = icmp eq i32 %45, 0
  %47 = select i1 %46, i32 0, i32 -17
  br label %48

; <label>:48:                                     ; preds = %40, %38, %29
  %49 = phi i32 [ %30, %29 ], [ %39, %38 ], [ %47, %40 ]
  ret i32 %49
}

; Function Attrs: nounwind
define weak i32 @halide_device_free(i8*, %struct.halide_buffer_t*) local_unnamed_addr #0 {
  %3 = icmp eq %struct.halide_buffer_t* %1, null
  br i1 %3, label %4, label %6

; <label>:4:                                      ; preds = %2
  %5 = tail call i32 @halide_error_buffer_is_null(i8* %0, i8* getelementptr inbounds ([19 x i8], [19 x i8]* @.str.21.23, i32 0, i32 0)) #11
  br label %29

; <label>:6:                                      ; preds = %2
  %7 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %8 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %7, align 8, !tbaa !31
  %9 = icmp ne %struct.halide_device_interface_t* %8, null
  %10 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %11 = load i64, i64* %10, align 8, !tbaa !29
  %12 = icmp ne i64 %11, 0
  %13 = xor i1 %12, true
  %14 = or i1 %9, %13
  br i1 %14, label %17, label %15

; <label>:15:                                     ; preds = %6
  %16 = tail call i32 @halide_error_no_device_interface(i8* %0) #11
  br label %29

; <label>:17:                                     ; preds = %6
  %18 = xor i1 %9, true
  %19 = or i1 %12, %18
  br i1 %19, label %22, label %20

; <label>:20:                                     ; preds = %17
  %21 = tail call i32 @halide_error_device_interface_no_device(i8* %0) #11
  br label %29

; <label>:22:                                     ; preds = %17
  %23 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 3
  %24 = load i64, i64* %23, align 8, !tbaa !33
  %25 = and i64 %24, 3
  %26 = icmp eq i64 %25, 3
  br i1 %26, label %27, label %35

; <label>:27:                                     ; preds = %22
  %28 = tail call i32 @halide_error_host_and_device_dirty(i8* %0) #11
  br label %29

; <label>:29:                                     ; preds = %27, %20, %15, %4
  %30 = phi i32 [ %5, %4 ], [ %21, %20 ], [ %16, %15 ], [ %28, %27 ]
  %31 = icmp eq i32 %30, 0
  br i1 %31, label %32, label %61

; <label>:32:                                     ; preds = %29
  %33 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %34 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %33, align 8, !tbaa !31
  br label %35

; <label>:35:                                     ; preds = %32, %22
  %36 = phi %struct.halide_device_interface_t* [ %34, %32 ], [ %8, %22 ]
  %37 = icmp eq %struct.halide_device_interface_t* %36, null
  br i1 %37, label %57, label %38

; <label>:38:                                     ; preds = %35
  %39 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %36, i32 0, i32 15
  %40 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %39, align 4, !tbaa !51
  %41 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %40, i32 0, i32 0
  %42 = load void ()*, void ()** %41, align 4, !tbaa !57
  tail call void %42() #11
  %43 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %39, align 4, !tbaa !51
  %44 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %43, i32 0, i32 3
  %45 = load i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)** %44, align 4, !tbaa !61
  %46 = tail call i32 %45(i8* %0, %struct.halide_buffer_t* nonnull %1) #11
  %47 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %39, align 4, !tbaa !51
  %48 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %47, i32 0, i32 1
  %49 = load void ()*, void ()** %48, align 4, !tbaa !59
  tail call void %49() #11
  %50 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %51 = load i64, i64* %50, align 8, !tbaa !29
  %52 = icmp eq i64 %51, 0
  br i1 %52, label %54, label %53

; <label>:53:                                     ; preds = %38
  tail call void @halide_print(i8* %0, i8* getelementptr inbounds ([110 x i8], [110 x i8]* @.str.22.24, i32 0, i32 0)) #11
  tail call void @abort() #11
  br label %54

; <label>:54:                                     ; preds = %53, %38
  %55 = icmp eq i32 %46, 0
  %56 = select i1 %55, i32 0, i32 -18
  br label %61

; <label>:57:                                     ; preds = %35
  %58 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 3
  %59 = load i64, i64* %58, align 8, !tbaa !33
  %60 = and i64 %59, -3
  store i64 %60, i64* %58, align 8, !tbaa !33
  br label %61

; <label>:61:                                     ; preds = %57, %54, %29
  %62 = phi i32 [ %30, %29 ], [ 0, %57 ], [ %56, %54 ]
  ret i32 %62
}

; Function Attrs: nounwind
define weak void @halide_device_free_as_destructor(i8*, i8*) local_unnamed_addr #0 {
  %3 = bitcast i8* %1 to %struct.halide_buffer_t*
  %4 = tail call i32 @halide_device_free(i8* %0, %struct.halide_buffer_t* %3) #12
  ret void
}

; Function Attrs: nounwind
define weak i32 @halide_device_and_host_malloc(i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*) local_unnamed_addr #0 {
  %4 = icmp eq %struct.halide_buffer_t* %1, null
  br i1 %4, label %5, label %7

; <label>:5:                                      ; preds = %3
  %6 = tail call i32 @halide_error_buffer_is_null(i8* %0, i8* getelementptr inbounds ([30 x i8], [30 x i8]* @.str.23.25, i32 0, i32 0)) #11
  br label %30

; <label>:7:                                      ; preds = %3
  %8 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %9 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %8, align 8, !tbaa !31
  %10 = icmp ne %struct.halide_device_interface_t* %9, null
  %11 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %12 = load i64, i64* %11, align 8, !tbaa !29
  %13 = icmp ne i64 %12, 0
  %14 = xor i1 %13, true
  %15 = or i1 %10, %14
  br i1 %15, label %18, label %16

; <label>:16:                                     ; preds = %7
  %17 = tail call i32 @halide_error_no_device_interface(i8* %0) #11
  br label %30

; <label>:18:                                     ; preds = %7
  %19 = xor i1 %10, true
  %20 = or i1 %13, %19
  br i1 %20, label %23, label %21

; <label>:21:                                     ; preds = %18
  %22 = tail call i32 @halide_error_device_interface_no_device(i8* %0) #11
  br label %30

; <label>:23:                                     ; preds = %18
  %24 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 3
  %25 = load i64, i64* %24, align 8, !tbaa !33
  %26 = and i64 %25, 3
  %27 = icmp eq i64 %26, 3
  br i1 %27, label %28, label %36

; <label>:28:                                     ; preds = %23
  %29 = tail call i32 @halide_error_host_and_device_dirty(i8* %0) #11
  br label %30

; <label>:30:                                     ; preds = %28, %21, %16, %5
  %31 = phi i32 [ %6, %5 ], [ %22, %21 ], [ %17, %16 ], [ %29, %28 ]
  %32 = icmp eq i32 %31, 0
  br i1 %32, label %33, label %56

; <label>:33:                                     ; preds = %30
  %34 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %35 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %34, align 8, !tbaa !31
  br label %36

; <label>:36:                                     ; preds = %33, %23
  %37 = phi %struct.halide_device_interface_t* [ %35, %33 ], [ %9, %23 ]
  %38 = icmp eq %struct.halide_device_interface_t* %37, null
  %39 = icmp eq %struct.halide_device_interface_t* %37, %2
  %40 = or i1 %38, %39
  br i1 %40, label %42, label %41

; <label>:41:                                     ; preds = %36
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([68 x i8], [68 x i8]* @.str.25, i32 0, i32 0)) #11
  br label %56

; <label>:42:                                     ; preds = %36
  %43 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %2, i32 0, i32 15
  %44 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %43, align 4, !tbaa !51
  %45 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %44, i32 0, i32 0
  %46 = load void ()*, void ()** %45, align 4, !tbaa !57
  tail call void %46() #11
  %47 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %43, align 4, !tbaa !51
  %48 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %47, i32 0, i32 8
  %49 = load i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)** %48, align 4, !tbaa !62
  %50 = tail call i32 %49(i8* %0, %struct.halide_buffer_t* nonnull %1) #11
  %51 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %43, align 4, !tbaa !51
  %52 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %51, i32 0, i32 1
  %53 = load void ()*, void ()** %52, align 4, !tbaa !59
  tail call void %53() #11
  %54 = icmp eq i32 %50, 0
  br i1 %54, label %56, label %55

; <label>:55:                                     ; preds = %42
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([42 x i8], [42 x i8]* @.str.26, i32 0, i32 0)) #11
  br label %56

; <label>:56:                                     ; preds = %55, %42, %41, %30
  %57 = phi i32 [ %31, %30 ], [ -42, %41 ], [ -16, %55 ], [ 0, %42 ]
  ret i32 %57
}

; Function Attrs: nounwind
define weak i32 @halide_device_and_host_free(i8*, %struct.halide_buffer_t*) local_unnamed_addr #0 {
  %3 = icmp eq %struct.halide_buffer_t* %1, null
  br i1 %3, label %4, label %6

; <label>:4:                                      ; preds = %2
  %5 = tail call i32 @halide_error_buffer_is_null(i8* %0, i8* getelementptr inbounds ([28 x i8], [28 x i8]* @.str.27, i32 0, i32 0)) #11
  br label %29

; <label>:6:                                      ; preds = %2
  %7 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %8 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %7, align 8, !tbaa !31
  %9 = icmp ne %struct.halide_device_interface_t* %8, null
  %10 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %11 = load i64, i64* %10, align 8, !tbaa !29
  %12 = icmp ne i64 %11, 0
  %13 = xor i1 %12, true
  %14 = or i1 %9, %13
  br i1 %14, label %17, label %15

; <label>:15:                                     ; preds = %6
  %16 = tail call i32 @halide_error_no_device_interface(i8* %0) #11
  br label %29

; <label>:17:                                     ; preds = %6
  %18 = xor i1 %9, true
  %19 = or i1 %12, %18
  br i1 %19, label %22, label %20

; <label>:20:                                     ; preds = %17
  %21 = tail call i32 @halide_error_device_interface_no_device(i8* %0) #11
  br label %29

; <label>:22:                                     ; preds = %17
  %23 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 3
  %24 = load i64, i64* %23, align 8, !tbaa !33
  %25 = and i64 %24, 3
  %26 = icmp eq i64 %25, 3
  br i1 %26, label %27, label %35

; <label>:27:                                     ; preds = %22
  %28 = tail call i32 @halide_error_host_and_device_dirty(i8* %0) #11
  br label %29

; <label>:29:                                     ; preds = %27, %20, %15, %4
  %30 = phi i32 [ %5, %4 ], [ %21, %20 ], [ %16, %15 ], [ %28, %27 ]
  %31 = icmp eq i32 %30, 0
  br i1 %31, label %32, label %66

; <label>:32:                                     ; preds = %29
  %33 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %34 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %33, align 8, !tbaa !31
  br label %35

; <label>:35:                                     ; preds = %32, %22
  %36 = phi %struct.halide_device_interface_t* [ %34, %32 ], [ %8, %22 ]
  %37 = icmp eq %struct.halide_device_interface_t* %36, null
  br i1 %37, label %57, label %38

; <label>:38:                                     ; preds = %35
  %39 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %36, i32 0, i32 15
  %40 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %39, align 4, !tbaa !51
  %41 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %40, i32 0, i32 0
  %42 = load void ()*, void ()** %41, align 4, !tbaa !57
  tail call void %42() #11
  %43 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %39, align 4, !tbaa !51
  %44 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %43, i32 0, i32 9
  %45 = load i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)** %44, align 4, !tbaa !63
  %46 = tail call i32 %45(i8* %0, %struct.halide_buffer_t* nonnull %1) #11
  %47 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %39, align 4, !tbaa !51
  %48 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %47, i32 0, i32 1
  %49 = load void ()*, void ()** %48, align 4, !tbaa !59
  tail call void %49() #11
  %50 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %51 = load i64, i64* %50, align 8, !tbaa !29
  %52 = icmp eq i64 %51, 0
  br i1 %52, label %54, label %53

; <label>:53:                                     ; preds = %38
  tail call void @halide_print(i8* %0, i8* getelementptr inbounds ([110 x i8], [110 x i8]* @.str.28, i32 0, i32 0)) #11
  tail call void @abort() #11
  br label %54

; <label>:54:                                     ; preds = %53, %38
  %55 = icmp eq i32 %46, 0
  %56 = select i1 %55, i32 0, i32 -18
  br label %66

; <label>:57:                                     ; preds = %35
  %58 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 2
  %59 = load i8*, i8** %58, align 4, !tbaa !32
  %60 = icmp eq i8* %59, null
  br i1 %60, label %62, label %61

; <label>:61:                                     ; preds = %57
  tail call void @halide_free(i8* %0, i8* nonnull %59) #11
  store i8* null, i8** %58, align 4, !tbaa !32
  br label %62

; <label>:62:                                     ; preds = %61, %57
  %63 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 3
  %64 = load i64, i64* %63, align 8, !tbaa !33
  %65 = and i64 %64, -3
  store i64 %65, i64* %63, align 8, !tbaa !33
  br label %66

; <label>:66:                                     ; preds = %62, %54, %29
  %67 = phi i32 [ %30, %29 ], [ 0, %62 ], [ %56, %54 ]
  ret i32 %67
}

; Function Attrs: nounwind
define weak i32 @halide_default_device_and_host_malloc(i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*) local_unnamed_addr #0 {
  %4 = icmp eq %struct.halide_buffer_t* %1, null
  br i1 %4, label %5, label %7

; <label>:5:                                      ; preds = %3
  %6 = tail call i32 @halide_error_buffer_is_null(i8* %0, i8* getelementptr inbounds ([38 x i8], [38 x i8]* @.str.29, i32 0, i32 0)) #11
  br label %30

; <label>:7:                                      ; preds = %3
  %8 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %9 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %8, align 8, !tbaa !31
  %10 = icmp ne %struct.halide_device_interface_t* %9, null
  %11 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %12 = load i64, i64* %11, align 8, !tbaa !29
  %13 = icmp ne i64 %12, 0
  %14 = xor i1 %13, true
  %15 = or i1 %10, %14
  br i1 %15, label %18, label %16

; <label>:16:                                     ; preds = %7
  %17 = tail call i32 @halide_error_no_device_interface(i8* %0) #11
  br label %30

; <label>:18:                                     ; preds = %7
  %19 = xor i1 %10, true
  %20 = or i1 %13, %19
  br i1 %20, label %23, label %21

; <label>:21:                                     ; preds = %18
  %22 = tail call i32 @halide_error_device_interface_no_device(i8* %0) #11
  br label %30

; <label>:23:                                     ; preds = %18
  %24 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 3
  %25 = load i64, i64* %24, align 8, !tbaa !33
  %26 = and i64 %25, 3
  %27 = icmp eq i64 %26, 3
  br i1 %27, label %28, label %33

; <label>:28:                                     ; preds = %23
  %29 = tail call i32 @halide_error_host_and_device_dirty(i8* %0) #11
  br label %30

; <label>:30:                                     ; preds = %28, %21, %16, %5
  %31 = phi i32 [ %6, %5 ], [ %22, %21 ], [ %17, %16 ], [ %29, %28 ]
  %32 = icmp eq i32 %31, 0
  br i1 %32, label %33, label %131

; <label>:33:                                     ; preds = %30, %23
  %34 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 5
  %35 = load i32, i32* %34, align 4, !tbaa !34
  %36 = icmp sgt i32 %35, 0
  br i1 %36, label %40, label %37

; <label>:37:                                     ; preds = %33
  %38 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 4, i32 1
  %39 = load i8, i8* %38, align 1, !tbaa !27
  br label %114

; <label>:40:                                     ; preds = %33
  %41 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 6
  %42 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %41, align 8, !tbaa !35
  %43 = add i32 %35, -1
  %xtraiter19 = and i32 %35, 7
  %44 = icmp ult i32 %43, 7
  br i1 %44, label %.unr-lcssa17, label %.new16

.new16:                                           ; preds = %40
  %unroll_iter26 = sub i32 %35, %xtraiter19
  br label %45

; <label>:45:                                     ; preds = %292, %.new16
  %46 = phi i32 [ 0, %.new16 ], [ %294, %292 ]
  %47 = phi i32 [ 0, %.new16 ], [ %293, %292 ]
  %niter27 = phi i32 [ %unroll_iter26, %.new16 ], [ %niter27.nsub.7, %292 ]
  %48 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %46, i32 2
  %49 = load i32, i32* %48, align 4, !tbaa !39
  %50 = icmp sgt i32 %49, 0
  br i1 %50, label %51, label %57

; <label>:51:                                     ; preds = %45
  %52 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %46, i32 1
  %53 = load i32, i32* %52, align 4, !tbaa !38
  %54 = add nsw i32 %53, -1
  %55 = mul nsw i32 %54, %49
  %56 = add nsw i32 %55, %47
  br label %57

; <label>:57:                                     ; preds = %51, %45
  %58 = phi i32 [ %56, %51 ], [ %47, %45 ]
  %59 = or i32 %46, 1
  %60 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %59, i32 2
  %61 = load i32, i32* %60, align 4, !tbaa !39
  %62 = icmp sgt i32 %61, 0
  br i1 %62, label %214, label %220

.unr-lcssa17:                                     ; preds = %292, %40
  %.lcssa13.ph = phi i32 [ undef, %40 ], [ %293, %292 ]
  %.unr21 = phi i32 [ 0, %40 ], [ %294, %292 ]
  %.unr22 = phi i32 [ 0, %40 ], [ %293, %292 ]
  %lcmp.mod23 = icmp eq i32 %xtraiter19, 0
  br i1 %lcmp.mod23, label %.epilog-lcssa24, label %.epil.preheader18

.epil.preheader18:                                ; preds = %.unr-lcssa17, %74
  %63 = phi i32 [ %76, %74 ], [ %.unr21, %.unr-lcssa17 ]
  %64 = phi i32 [ %75, %74 ], [ %.unr22, %.unr-lcssa17 ]
  %epil.iter20 = phi i32 [ %epil.iter20.sub, %74 ], [ %xtraiter19, %.unr-lcssa17 ]
  %65 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %63, i32 2
  %66 = load i32, i32* %65, align 4, !tbaa !39
  %67 = icmp sgt i32 %66, 0
  br i1 %67, label %68, label %74

; <label>:68:                                     ; preds = %.epil.preheader18
  %69 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %63, i32 1
  %70 = load i32, i32* %69, align 4, !tbaa !38
  %71 = add nsw i32 %70, -1
  %72 = mul nsw i32 %71, %66
  %73 = add nsw i32 %72, %64
  br label %74

; <label>:74:                                     ; preds = %68, %.epil.preheader18
  %75 = phi i32 [ %73, %68 ], [ %64, %.epil.preheader18 ]
  %76 = add nuw nsw i32 %63, 1
  %epil.iter20.sub = add i32 %epil.iter20, -1
  %epil.iter20.cmp = icmp eq i32 %epil.iter20.sub, 0
  br i1 %epil.iter20.cmp, label %.epilog-lcssa24, label %.epil.preheader18, !llvm.loop !64

.epilog-lcssa24:                                  ; preds = %74, %.unr-lcssa17
  %.lcssa13 = phi i32 [ %.lcssa13.ph, %.unr-lcssa17 ], [ %75, %74 ]
  %77 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 4, i32 1
  %78 = load i8, i8* %77, align 1, !tbaa !27
  %79 = add i32 %35, -1
  %xtraiter = and i32 %35, 7
  %80 = icmp ult i32 %79, 7
  br i1 %80, label %.unr-lcssa, label %.new

.new:                                             ; preds = %.epilog-lcssa24
  %unroll_iter = sub i32 %35, %xtraiter
  br label %81

; <label>:81:                                     ; preds = %211, %.new
  %82 = phi i32 [ 0, %.new ], [ %213, %211 ]
  %83 = phi i32 [ 0, %.new ], [ %212, %211 ]
  %niter = phi i32 [ %unroll_iter, %.new ], [ %niter.nsub.7, %211 ]
  %84 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %82, i32 2
  %85 = load i32, i32* %84, align 4, !tbaa !39
  %86 = icmp slt i32 %85, 0
  br i1 %86, label %87, label %93

; <label>:87:                                     ; preds = %81
  %88 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %82, i32 1
  %89 = load i32, i32* %88, align 4, !tbaa !38
  %90 = add nsw i32 %89, -1
  %91 = mul nsw i32 %90, %85
  %92 = add nsw i32 %91, %83
  br label %93

; <label>:93:                                     ; preds = %87, %81
  %94 = phi i32 [ %92, %87 ], [ %83, %81 ]
  %95 = or i32 %82, 1
  %96 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %95, i32 2
  %97 = load i32, i32* %96, align 4, !tbaa !39
  %98 = icmp slt i32 %97, 0
  br i1 %98, label %133, label %139

.unr-lcssa:                                       ; preds = %211, %.epilog-lcssa24
  %.lcssa.ph = phi i32 [ undef, %.epilog-lcssa24 ], [ %212, %211 ]
  %.unr = phi i32 [ 0, %.epilog-lcssa24 ], [ %213, %211 ]
  %.unr14 = phi i32 [ 0, %.epilog-lcssa24 ], [ %212, %211 ]
  %lcmp.mod = icmp eq i32 %xtraiter, 0
  br i1 %lcmp.mod, label %.epilog-lcssa, label %.epil.preheader

.epil.preheader:                                  ; preds = %.unr-lcssa, %110
  %99 = phi i32 [ %112, %110 ], [ %.unr, %.unr-lcssa ]
  %100 = phi i32 [ %111, %110 ], [ %.unr14, %.unr-lcssa ]
  %epil.iter = phi i32 [ %epil.iter.sub, %110 ], [ %xtraiter, %.unr-lcssa ]
  %101 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %99, i32 2
  %102 = load i32, i32* %101, align 4, !tbaa !39
  %103 = icmp slt i32 %102, 0
  br i1 %103, label %104, label %110

; <label>:104:                                    ; preds = %.epil.preheader
  %105 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %99, i32 1
  %106 = load i32, i32* %105, align 4, !tbaa !38
  %107 = add nsw i32 %106, -1
  %108 = mul nsw i32 %107, %102
  %109 = add nsw i32 %108, %100
  br label %110

; <label>:110:                                    ; preds = %104, %.epil.preheader
  %111 = phi i32 [ %109, %104 ], [ %100, %.epil.preheader ]
  %112 = add nuw nsw i32 %99, 1
  %epil.iter.sub = add i32 %epil.iter, -1
  %epil.iter.cmp = icmp eq i32 %epil.iter.sub, 0
  br i1 %epil.iter.cmp, label %.epilog-lcssa, label %.epil.preheader, !llvm.loop !65

.epilog-lcssa:                                    ; preds = %110, %.unr-lcssa
  %.lcssa = phi i32 [ %.lcssa.ph, %.unr-lcssa ], [ %111, %110 ]
  %113 = add nsw i32 %.lcssa13, 1
  br label %114

; <label>:114:                                    ; preds = %.epilog-lcssa, %37
  %115 = phi i8 [ %39, %37 ], [ %78, %.epilog-lcssa ]
  %116 = phi i32 [ 1, %37 ], [ %113, %.epilog-lcssa ]
  %117 = phi i32 [ 0, %37 ], [ %.lcssa, %.epilog-lcssa ]
  %118 = zext i8 %115 to i32
  %119 = add nuw nsw i32 %118, 7
  %120 = lshr i32 %119, 3
  %121 = sub i32 %116, %117
  %122 = mul i32 %121, %120
  %123 = tail call i8* @halide_malloc(i8* %0, i32 %122) #11
  %124 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 2
  store i8* %123, i8** %124, align 4, !tbaa !32
  %125 = icmp eq i8* %123, null
  br i1 %125, label %131, label %126

; <label>:126:                                    ; preds = %114
  %127 = tail call i32 @halide_device_malloc(i8* %0, %struct.halide_buffer_t* nonnull %1, %struct.halide_device_interface_t* %2) #12
  %128 = icmp eq i32 %127, 0
  br i1 %128, label %131, label %129

; <label>:129:                                    ; preds = %126
  %130 = load i8*, i8** %124, align 4, !tbaa !32
  tail call void @halide_free(i8* %0, i8* %130) #11
  store i8* null, i8** %124, align 4, !tbaa !32
  br label %131

; <label>:131:                                    ; preds = %129, %126, %114, %30
  %132 = phi i32 [ %31, %30 ], [ -1, %114 ], [ 0, %126 ], [ %127, %129 ]
  ret i32 %132

; <label>:133:                                    ; preds = %93
  %134 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %95, i32 1
  %135 = load i32, i32* %134, align 4, !tbaa !38
  %136 = add nsw i32 %135, -1
  %137 = mul nsw i32 %136, %97
  %138 = add nsw i32 %137, %94
  br label %139

; <label>:139:                                    ; preds = %133, %93
  %140 = phi i32 [ %138, %133 ], [ %94, %93 ]
  %141 = or i32 %82, 2
  %142 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %141, i32 2
  %143 = load i32, i32* %142, align 4, !tbaa !39
  %144 = icmp slt i32 %143, 0
  br i1 %144, label %145, label %151

; <label>:145:                                    ; preds = %139
  %146 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %141, i32 1
  %147 = load i32, i32* %146, align 4, !tbaa !38
  %148 = add nsw i32 %147, -1
  %149 = mul nsw i32 %148, %143
  %150 = add nsw i32 %149, %140
  br label %151

; <label>:151:                                    ; preds = %145, %139
  %152 = phi i32 [ %150, %145 ], [ %140, %139 ]
  %153 = or i32 %82, 3
  %154 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %153, i32 2
  %155 = load i32, i32* %154, align 4, !tbaa !39
  %156 = icmp slt i32 %155, 0
  br i1 %156, label %157, label %163

; <label>:157:                                    ; preds = %151
  %158 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %153, i32 1
  %159 = load i32, i32* %158, align 4, !tbaa !38
  %160 = add nsw i32 %159, -1
  %161 = mul nsw i32 %160, %155
  %162 = add nsw i32 %161, %152
  br label %163

; <label>:163:                                    ; preds = %157, %151
  %164 = phi i32 [ %162, %157 ], [ %152, %151 ]
  %165 = or i32 %82, 4
  %166 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %165, i32 2
  %167 = load i32, i32* %166, align 4, !tbaa !39
  %168 = icmp slt i32 %167, 0
  br i1 %168, label %169, label %175

; <label>:169:                                    ; preds = %163
  %170 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %165, i32 1
  %171 = load i32, i32* %170, align 4, !tbaa !38
  %172 = add nsw i32 %171, -1
  %173 = mul nsw i32 %172, %167
  %174 = add nsw i32 %173, %164
  br label %175

; <label>:175:                                    ; preds = %169, %163
  %176 = phi i32 [ %174, %169 ], [ %164, %163 ]
  %177 = or i32 %82, 5
  %178 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %177, i32 2
  %179 = load i32, i32* %178, align 4, !tbaa !39
  %180 = icmp slt i32 %179, 0
  br i1 %180, label %181, label %187

; <label>:181:                                    ; preds = %175
  %182 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %177, i32 1
  %183 = load i32, i32* %182, align 4, !tbaa !38
  %184 = add nsw i32 %183, -1
  %185 = mul nsw i32 %184, %179
  %186 = add nsw i32 %185, %176
  br label %187

; <label>:187:                                    ; preds = %181, %175
  %188 = phi i32 [ %186, %181 ], [ %176, %175 ]
  %189 = or i32 %82, 6
  %190 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %189, i32 2
  %191 = load i32, i32* %190, align 4, !tbaa !39
  %192 = icmp slt i32 %191, 0
  br i1 %192, label %193, label %199

; <label>:193:                                    ; preds = %187
  %194 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %189, i32 1
  %195 = load i32, i32* %194, align 4, !tbaa !38
  %196 = add nsw i32 %195, -1
  %197 = mul nsw i32 %196, %191
  %198 = add nsw i32 %197, %188
  br label %199

; <label>:199:                                    ; preds = %193, %187
  %200 = phi i32 [ %198, %193 ], [ %188, %187 ]
  %201 = or i32 %82, 7
  %202 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %201, i32 2
  %203 = load i32, i32* %202, align 4, !tbaa !39
  %204 = icmp slt i32 %203, 0
  br i1 %204, label %205, label %211

; <label>:205:                                    ; preds = %199
  %206 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %201, i32 1
  %207 = load i32, i32* %206, align 4, !tbaa !38
  %208 = add nsw i32 %207, -1
  %209 = mul nsw i32 %208, %203
  %210 = add nsw i32 %209, %200
  br label %211

; <label>:211:                                    ; preds = %205, %199
  %212 = phi i32 [ %210, %205 ], [ %200, %199 ]
  %213 = add nuw nsw i32 %82, 8
  %niter.nsub.7 = add i32 %niter, -8
  %niter.ncmp.7 = icmp eq i32 %niter.nsub.7, 0
  br i1 %niter.ncmp.7, label %.unr-lcssa, label %81

; <label>:214:                                    ; preds = %57
  %215 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %59, i32 1
  %216 = load i32, i32* %215, align 4, !tbaa !38
  %217 = add nsw i32 %216, -1
  %218 = mul nsw i32 %217, %61
  %219 = add nsw i32 %218, %58
  br label %220

; <label>:220:                                    ; preds = %214, %57
  %221 = phi i32 [ %219, %214 ], [ %58, %57 ]
  %222 = or i32 %46, 2
  %223 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %222, i32 2
  %224 = load i32, i32* %223, align 4, !tbaa !39
  %225 = icmp sgt i32 %224, 0
  br i1 %225, label %226, label %232

; <label>:226:                                    ; preds = %220
  %227 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %222, i32 1
  %228 = load i32, i32* %227, align 4, !tbaa !38
  %229 = add nsw i32 %228, -1
  %230 = mul nsw i32 %229, %224
  %231 = add nsw i32 %230, %221
  br label %232

; <label>:232:                                    ; preds = %226, %220
  %233 = phi i32 [ %231, %226 ], [ %221, %220 ]
  %234 = or i32 %46, 3
  %235 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %234, i32 2
  %236 = load i32, i32* %235, align 4, !tbaa !39
  %237 = icmp sgt i32 %236, 0
  br i1 %237, label %238, label %244

; <label>:238:                                    ; preds = %232
  %239 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %234, i32 1
  %240 = load i32, i32* %239, align 4, !tbaa !38
  %241 = add nsw i32 %240, -1
  %242 = mul nsw i32 %241, %236
  %243 = add nsw i32 %242, %233
  br label %244

; <label>:244:                                    ; preds = %238, %232
  %245 = phi i32 [ %243, %238 ], [ %233, %232 ]
  %246 = or i32 %46, 4
  %247 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %246, i32 2
  %248 = load i32, i32* %247, align 4, !tbaa !39
  %249 = icmp sgt i32 %248, 0
  br i1 %249, label %250, label %256

; <label>:250:                                    ; preds = %244
  %251 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %246, i32 1
  %252 = load i32, i32* %251, align 4, !tbaa !38
  %253 = add nsw i32 %252, -1
  %254 = mul nsw i32 %253, %248
  %255 = add nsw i32 %254, %245
  br label %256

; <label>:256:                                    ; preds = %250, %244
  %257 = phi i32 [ %255, %250 ], [ %245, %244 ]
  %258 = or i32 %46, 5
  %259 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %258, i32 2
  %260 = load i32, i32* %259, align 4, !tbaa !39
  %261 = icmp sgt i32 %260, 0
  br i1 %261, label %262, label %268

; <label>:262:                                    ; preds = %256
  %263 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %258, i32 1
  %264 = load i32, i32* %263, align 4, !tbaa !38
  %265 = add nsw i32 %264, -1
  %266 = mul nsw i32 %265, %260
  %267 = add nsw i32 %266, %257
  br label %268

; <label>:268:                                    ; preds = %262, %256
  %269 = phi i32 [ %267, %262 ], [ %257, %256 ]
  %270 = or i32 %46, 6
  %271 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %270, i32 2
  %272 = load i32, i32* %271, align 4, !tbaa !39
  %273 = icmp sgt i32 %272, 0
  br i1 %273, label %274, label %280

; <label>:274:                                    ; preds = %268
  %275 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %270, i32 1
  %276 = load i32, i32* %275, align 4, !tbaa !38
  %277 = add nsw i32 %276, -1
  %278 = mul nsw i32 %277, %272
  %279 = add nsw i32 %278, %269
  br label %280

; <label>:280:                                    ; preds = %274, %268
  %281 = phi i32 [ %279, %274 ], [ %269, %268 ]
  %282 = or i32 %46, 7
  %283 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %282, i32 2
  %284 = load i32, i32* %283, align 4, !tbaa !39
  %285 = icmp sgt i32 %284, 0
  br i1 %285, label %286, label %292

; <label>:286:                                    ; preds = %280
  %287 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %42, i32 %282, i32 1
  %288 = load i32, i32* %287, align 4, !tbaa !38
  %289 = add nsw i32 %288, -1
  %290 = mul nsw i32 %289, %284
  %291 = add nsw i32 %290, %281
  br label %292

; <label>:292:                                    ; preds = %286, %280
  %293 = phi i32 [ %291, %286 ], [ %281, %280 ]
  %294 = add nuw nsw i32 %46, 8
  %niter27.nsub.7 = add i32 %niter27, -8
  %niter27.ncmp.7 = icmp eq i32 %niter27.nsub.7, 0
  br i1 %niter27.ncmp.7, label %.unr-lcssa17, label %45
}

; Function Attrs: nounwind
define weak i32 @halide_default_device_and_host_free(i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*) local_unnamed_addr #0 {
  %4 = icmp eq %struct.halide_buffer_t* %1, null
  br i1 %4, label %5, label %7

; <label>:5:                                      ; preds = %3
  %6 = tail call i32 @halide_error_buffer_is_null(i8* %0, i8* getelementptr inbounds ([36 x i8], [36 x i8]* @.str.30, i32 0, i32 0)) #11
  br label %31

; <label>:7:                                      ; preds = %3
  %8 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %9 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %8, align 8, !tbaa !31
  %10 = icmp ne %struct.halide_device_interface_t* %9, null
  %11 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %12 = load i64, i64* %11, align 8, !tbaa !29
  %13 = icmp ne i64 %12, 0
  %14 = xor i1 %13, true
  %15 = or i1 %10, %14
  br i1 %15, label %18, label %16

; <label>:16:                                     ; preds = %7
  %17 = tail call i32 @halide_error_no_device_interface(i8* %0) #11
  br label %31

; <label>:18:                                     ; preds = %7
  %19 = xor i1 %10, true
  %20 = or i1 %13, %19
  br i1 %20, label %23, label %21

; <label>:21:                                     ; preds = %18
  %22 = tail call i32 @halide_error_device_interface_no_device(i8* %0) #11
  br label %31

; <label>:23:                                     ; preds = %18
  %24 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 3
  %25 = load i64, i64* %24, align 8, !tbaa !33
  %26 = and i64 %25, 3
  %27 = icmp eq i64 %26, 3
  br i1 %27, label %29, label %.split

.split:                                           ; preds = %23
  %28 = tail call i32 @halide_device_free(i8* %0, %struct.halide_buffer_t* nonnull %1) #12
  br label %35

; <label>:29:                                     ; preds = %23
  %30 = tail call i32 @halide_error_host_and_device_dirty(i8* %0) #11
  br label %31

; <label>:31:                                     ; preds = %29, %21, %16, %5
  %32 = phi i32 [ %6, %5 ], [ %22, %21 ], [ %17, %16 ], [ %30, %29 ]
  %33 = icmp eq i32 %32, 0
  br i1 %33, label %.split1, label %44

.split1:                                          ; preds = %31
  %34 = tail call i32 @halide_device_free(i8* %0, %struct.halide_buffer_t* %1) #12
  br label %35

; <label>:35:                                     ; preds = %.split1, %.split
  %phi.call = phi i32 [ %28, %.split ], [ %34, %.split1 ]
  %36 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 2
  %37 = load i8*, i8** %36, align 4, !tbaa !32
  %38 = icmp eq i8* %37, null
  br i1 %38, label %40, label %39

; <label>:39:                                     ; preds = %35
  tail call void @halide_free(i8* %0, i8* nonnull %37) #11
  store i8* null, i8** %36, align 4, !tbaa !32
  br label %40

; <label>:40:                                     ; preds = %39, %35
  %41 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 3
  %42 = load i64, i64* %41, align 8, !tbaa !33
  %43 = and i64 %42, -4
  store i64 %43, i64* %41, align 8, !tbaa !33
  br label %44

; <label>:44:                                     ; preds = %40, %31
  %45 = phi i32 [ %phi.call, %40 ], [ %32, %31 ]
  ret i32 %45
}

; Function Attrs: nounwind
define weak i32 @halide_device_wrap_native(i8*, %struct.halide_buffer_t*, i64, %struct.halide_device_interface_t*) local_unnamed_addr #0 {
  %5 = icmp eq %struct.halide_buffer_t* %1, null
  br i1 %5, label %6, label %8

; <label>:6:                                      ; preds = %4
  %7 = tail call i32 @halide_error_buffer_is_null(i8* %0, i8* getelementptr inbounds ([26 x i8], [26 x i8]* @.str.31, i32 0, i32 0)) #11
  br label %31

; <label>:8:                                      ; preds = %4
  %9 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %10 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %9, align 8, !tbaa !31
  %11 = icmp ne %struct.halide_device_interface_t* %10, null
  %12 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %13 = load i64, i64* %12, align 8, !tbaa !29
  %14 = icmp ne i64 %13, 0
  %15 = xor i1 %14, true
  %16 = or i1 %11, %15
  br i1 %16, label %19, label %17

; <label>:17:                                     ; preds = %8
  %18 = tail call i32 @halide_error_no_device_interface(i8* %0) #11
  br label %31

; <label>:19:                                     ; preds = %8
  %20 = xor i1 %11, true
  %21 = or i1 %14, %20
  br i1 %21, label %24, label %22

; <label>:22:                                     ; preds = %19
  %23 = tail call i32 @halide_error_device_interface_no_device(i8* %0) #11
  br label %31

; <label>:24:                                     ; preds = %19
  %25 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 3
  %26 = load i64, i64* %25, align 8, !tbaa !33
  %27 = and i64 %26, 3
  %28 = icmp eq i64 %27, 3
  br i1 %28, label %29, label %37

; <label>:29:                                     ; preds = %24
  %30 = tail call i32 @halide_error_host_and_device_dirty(i8* %0) #11
  br label %31

; <label>:31:                                     ; preds = %29, %22, %17, %6
  %32 = phi i32 [ %7, %6 ], [ %23, %22 ], [ %18, %17 ], [ %30, %29 ]
  %33 = icmp eq i32 %32, 0
  br i1 %33, label %34, label %58

; <label>:34:                                     ; preds = %31
  %35 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %36 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %35, align 8, !tbaa !31
  br label %37

; <label>:37:                                     ; preds = %34, %24
  %38 = phi %struct.halide_device_interface_t* [ %36, %34 ], [ %10, %24 ]
  %39 = icmp eq %struct.halide_device_interface_t* %38, null
  %40 = icmp eq %struct.halide_device_interface_t* %38, %3
  %41 = or i1 %39, %40
  br i1 %41, label %43, label %42

; <label>:42:                                     ; preds = %37
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([64 x i8], [64 x i8]* @.str.32, i32 0, i32 0)) #11
  br label %58

; <label>:43:                                     ; preds = %37
  %44 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %45 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %3, i32 0, i32 15
  %46 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %45, align 4, !tbaa !51
  %47 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %46, i32 0, i32 0
  %48 = load void ()*, void ()** %47, align 4, !tbaa !57
  tail call void %48() #11
  store %struct.halide_device_interface_t* %3, %struct.halide_device_interface_t** %44, align 8, !tbaa !31
  %49 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %45, align 4, !tbaa !51
  %50 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %49, i32 0, i32 14
  %51 = load i32 (i8*, %struct.halide_buffer_t*, i64)*, i32 (i8*, %struct.halide_buffer_t*, i64)** %50, align 4, !tbaa !66
  %52 = tail call i32 %51(i8* %0, %struct.halide_buffer_t* nonnull %1, i64 %2) #11
  %53 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %45, align 4, !tbaa !51
  %54 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %53, i32 0, i32 1
  %55 = load void ()*, void ()** %54, align 4, !tbaa !59
  tail call void %55() #11
  %56 = icmp eq i32 %52, 0
  %57 = select i1 %56, i32 0, i32 -16
  br label %58

; <label>:58:                                     ; preds = %43, %42, %31
  %59 = phi i32 [ %32, %31 ], [ -42, %42 ], [ %57, %43 ]
  ret i32 %59
}

; Function Attrs: nounwind
define weak i32 @halide_device_detach_native(i8*, %struct.halide_buffer_t*) local_unnamed_addr #0 {
  %3 = icmp eq %struct.halide_buffer_t* %1, null
  br i1 %3, label %4, label %6

; <label>:4:                                      ; preds = %2
  %5 = tail call i32 @halide_error_buffer_is_null(i8* %0, i8* getelementptr inbounds ([28 x i8], [28 x i8]* @.str.33, i32 0, i32 0)) #11
  br label %29

; <label>:6:                                      ; preds = %2
  %7 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %8 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %7, align 8, !tbaa !31
  %9 = icmp ne %struct.halide_device_interface_t* %8, null
  %10 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %11 = load i64, i64* %10, align 8, !tbaa !29
  %12 = icmp ne i64 %11, 0
  %13 = xor i1 %12, true
  %14 = or i1 %9, %13
  br i1 %14, label %17, label %15

; <label>:15:                                     ; preds = %6
  %16 = tail call i32 @halide_error_no_device_interface(i8* %0) #11
  br label %29

; <label>:17:                                     ; preds = %6
  %18 = xor i1 %9, true
  %19 = or i1 %12, %18
  br i1 %19, label %22, label %20

; <label>:20:                                     ; preds = %17
  %21 = tail call i32 @halide_error_device_interface_no_device(i8* %0) #11
  br label %29

; <label>:22:                                     ; preds = %17
  %23 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 3
  %24 = load i64, i64* %23, align 8, !tbaa !33
  %25 = and i64 %24, 3
  %26 = icmp eq i64 %25, 3
  br i1 %26, label %27, label %35

; <label>:27:                                     ; preds = %22
  %28 = tail call i32 @halide_error_host_and_device_dirty(i8* %0) #11
  br label %29

; <label>:29:                                     ; preds = %27, %20, %15, %4
  %30 = phi i32 [ %5, %4 ], [ %21, %20 ], [ %16, %15 ], [ %28, %27 ]
  %31 = icmp eq i32 %30, 0
  br i1 %31, label %32, label %57

; <label>:32:                                     ; preds = %29
  %33 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %34 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %33, align 8, !tbaa !31
  br label %35

; <label>:35:                                     ; preds = %32, %22
  %36 = phi %struct.halide_device_interface_t* [ %34, %32 ], [ %8, %22 ]
  %37 = icmp eq %struct.halide_device_interface_t* %36, null
  br i1 %37, label %57, label %38

; <label>:38:                                     ; preds = %35
  %39 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %36, i32 0, i32 15
  %40 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %39, align 4, !tbaa !51
  %41 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %40, i32 0, i32 0
  %42 = load void ()*, void ()** %41, align 4, !tbaa !57
  tail call void %42() #11
  %43 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %39, align 4, !tbaa !51
  %44 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %43, i32 0, i32 15
  %45 = load i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)** %44, align 4, !tbaa !67
  %46 = tail call i32 %45(i8* %0, %struct.halide_buffer_t* nonnull %1) #11
  %47 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %39, align 4, !tbaa !51
  %48 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %47, i32 0, i32 1
  %49 = load void ()*, void ()** %48, align 4, !tbaa !59
  tail call void %49() #11
  %50 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %51 = load i64, i64* %50, align 8, !tbaa !29
  %52 = icmp eq i64 %51, 0
  br i1 %52, label %54, label %53

; <label>:53:                                     ; preds = %38
  tail call void @halide_print(i8* %0, i8* getelementptr inbounds ([110 x i8], [110 x i8]* @.str.34, i32 0, i32 0)) #11
  tail call void @abort() #11
  br label %54

; <label>:54:                                     ; preds = %53, %38
  %55 = icmp eq i32 %46, 0
  %56 = select i1 %55, i32 0, i32 -33
  ret i32 %56

; <label>:57:                                     ; preds = %35, %29
  %58 = phi i32 [ %30, %29 ], [ 0, %35 ]
  ret i32 %58
}

; Function Attrs: nounwind
define weak i32 @halide_default_device_wrap_native(i8*, %struct.halide_buffer_t*, i64) local_unnamed_addr #0 {
  %4 = icmp eq %struct.halide_buffer_t* %1, null
  br i1 %4, label %5, label %7

; <label>:5:                                      ; preds = %3
  %6 = tail call i32 @halide_error_buffer_is_null(i8* %0, i8* getelementptr inbounds ([34 x i8], [34 x i8]* @.str.35, i32 0, i32 0)) #11
  br label %30

; <label>:7:                                      ; preds = %3
  %8 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %9 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %8, align 8, !tbaa !31
  %10 = icmp ne %struct.halide_device_interface_t* %9, null
  %11 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %12 = load i64, i64* %11, align 8, !tbaa !29
  %13 = icmp ne i64 %12, 0
  %14 = xor i1 %13, true
  %15 = or i1 %10, %14
  br i1 %15, label %18, label %16

; <label>:16:                                     ; preds = %7
  %17 = tail call i32 @halide_error_no_device_interface(i8* %0) #11
  br label %30

; <label>:18:                                     ; preds = %7
  %19 = xor i1 %10, true
  %20 = or i1 %13, %19
  br i1 %20, label %23, label %21

; <label>:21:                                     ; preds = %18
  %22 = tail call i32 @halide_error_device_interface_no_device(i8* %0) #11
  br label %30

; <label>:23:                                     ; preds = %18
  %24 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 3
  %25 = load i64, i64* %24, align 8, !tbaa !33
  %26 = and i64 %25, 3
  %27 = icmp eq i64 %26, 3
  br i1 %27, label %28, label %36

; <label>:28:                                     ; preds = %23
  %29 = tail call i32 @halide_error_host_and_device_dirty(i8* %0) #11
  br label %30

; <label>:30:                                     ; preds = %28, %21, %16, %5
  %31 = phi i32 [ %6, %5 ], [ %22, %21 ], [ %17, %16 ], [ %29, %28 ]
  %32 = icmp eq i32 %31, 0
  br i1 %32, label %33, label %43

; <label>:33:                                     ; preds = %30
  %34 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %35 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %34, align 8, !tbaa !31
  br label %36

; <label>:36:                                     ; preds = %33, %23
  %37 = phi %struct.halide_device_interface_t* [ %35, %33 ], [ %9, %23 ]
  %38 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %37, i32 0, i32 15
  %39 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %38, align 4, !tbaa !51
  %40 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %39, i32 0, i32 0
  %41 = load void ()*, void ()** %40, align 4, !tbaa !57
  tail call void %41() #11
  %42 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  store i64 %2, i64* %42, align 8, !tbaa !29
  br label %43

; <label>:43:                                     ; preds = %36, %30
  %44 = phi i32 [ 0, %36 ], [ %31, %30 ]
  ret i32 %44
}

; Function Attrs: nounwind
define weak i32 @halide_default_device_detach_native(i8*, %struct.halide_buffer_t*) local_unnamed_addr #0 {
  %3 = icmp eq %struct.halide_buffer_t* %1, null
  br i1 %3, label %4, label %6

; <label>:4:                                      ; preds = %2
  %5 = tail call i32 @halide_error_buffer_is_null(i8* %0, i8* getelementptr inbounds ([36 x i8], [36 x i8]* @.str.36, i32 0, i32 0)) #11
  br label %29

; <label>:6:                                      ; preds = %2
  %7 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %8 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %7, align 8, !tbaa !31
  %9 = icmp ne %struct.halide_device_interface_t* %8, null
  %10 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %11 = load i64, i64* %10, align 8, !tbaa !29
  %12 = icmp ne i64 %11, 0
  %13 = xor i1 %12, true
  %14 = or i1 %9, %13
  br i1 %14, label %17, label %15

; <label>:15:                                     ; preds = %6
  %16 = tail call i32 @halide_error_no_device_interface(i8* %0) #11
  br label %29

; <label>:17:                                     ; preds = %6
  %18 = xor i1 %9, true
  %19 = or i1 %12, %18
  br i1 %19, label %22, label %20

; <label>:20:                                     ; preds = %17
  %21 = tail call i32 @halide_error_device_interface_no_device(i8* %0) #11
  br label %29

; <label>:22:                                     ; preds = %17
  %23 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 3
  %24 = load i64, i64* %23, align 8, !tbaa !33
  %25 = and i64 %24, 3
  %26 = icmp eq i64 %25, 3
  br i1 %26, label %27, label %35

; <label>:27:                                     ; preds = %22
  %28 = tail call i32 @halide_error_host_and_device_dirty(i8* %0) #11
  br label %29

; <label>:29:                                     ; preds = %27, %20, %15, %4
  %30 = phi i32 [ %5, %4 ], [ %21, %20 ], [ %16, %15 ], [ %28, %27 ]
  %31 = icmp eq i32 %30, 0
  br i1 %31, label %32, label %46

; <label>:32:                                     ; preds = %29
  %33 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %34 = load i64, i64* %33, align 8, !tbaa !29
  br label %35

; <label>:35:                                     ; preds = %32, %22
  %36 = phi i64 [ %34, %32 ], [ %11, %22 ]
  %37 = icmp eq i64 %36, 0
  br i1 %37, label %46, label %38

; <label>:38:                                     ; preds = %35
  %39 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %40 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %41 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %40, align 8, !tbaa !31
  %42 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %41, i32 0, i32 15
  %43 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %42, align 4, !tbaa !51
  %44 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %43, i32 0, i32 1
  %45 = load void ()*, void ()** %44, align 4, !tbaa !59
  tail call void %45() #11
  store i64 0, i64* %39, align 8, !tbaa !29
  store %struct.halide_device_interface_t* null, %struct.halide_device_interface_t** %40, align 8, !tbaa !31
  br label %46

; <label>:46:                                     ; preds = %38, %35, %29
  %47 = phi i32 [ 0, %38 ], [ %30, %29 ], [ 0, %35 ]
  ret i32 %47
}

; Function Attrs: nounwind
define weak void @halide_device_and_host_free_as_destructor(i8*, i8*) local_unnamed_addr #0 {
  %3 = bitcast i8* %1 to %struct.halide_buffer_t*
  %4 = tail call i32 @halide_device_and_host_free(i8* %0, %struct.halide_buffer_t* %3) #12
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @halide_device_host_nop_free(i8*, i8*) local_unnamed_addr #3 {
  ret void
}

; Function Attrs: norecurse nounwind
define weak i32 @halide_default_buffer_copy(i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*, %struct.halide_buffer_t*) local_unnamed_addr #3 {
  ret i32 -39
}

; Function Attrs: nounwind
define weak i32 @halide_buffer_copy_already_locked(i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*, %struct.halide_buffer_t*) local_unnamed_addr #0 {
  %5 = alloca %"struct.Halide::Runtime::Internal::device_copy", align 8
  %6 = icmp ne %struct.halide_device_interface_t* %2, null
  br i1 %6, label %7, label %21

; <label>:7:                                      ; preds = %4
  %8 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 1
  %9 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %8, align 8, !tbaa !31
  %10 = icmp eq %struct.halide_device_interface_t* %9, null
  %11 = icmp eq %struct.halide_device_interface_t* %9, %2
  %12 = or i1 %10, %11
  br i1 %12, label %14, label %13

; <label>:13:                                     ; preds = %7
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([64 x i8], [64 x i8]* @.str.42, i32 0, i32 0)) #11
  br label %130

; <label>:14:                                     ; preds = %7
  %15 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 0
  %16 = load i64, i64* %15, align 8, !tbaa !29
  %17 = icmp eq i64 %16, 0
  br i1 %17, label %18, label %21

; <label>:18:                                     ; preds = %14
  %19 = tail call i32 @halide_device_malloc(i8* %0, %struct.halide_buffer_t* nonnull %3, %struct.halide_device_interface_t* nonnull %2) #12
  %20 = icmp eq i32 %19, 0
  br i1 %20, label %21, label %130

; <label>:21:                                     ; preds = %18, %14, %4
  %22 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %23 = load i64, i64* %22, align 8, !tbaa !29
  %24 = icmp eq i64 %23, 0
  %25 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 2
  %26 = load i8*, i8** %25, align 4, !tbaa !32
  br i1 %24, label %37, label %27

; <label>:27:                                     ; preds = %21
  %28 = icmp eq i8* %26, null
  br i1 %28, label %29, label %31

; <label>:29:                                     ; preds = %27
  %30 = icmp eq %struct.halide_device_interface_t* %2, null
  br label %50

; <label>:31:                                     ; preds = %27
  %32 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 3
  %33 = load i64, i64* %32, align 8, !tbaa !33
  %34 = and i64 %33, 1
  %35 = icmp ne i64 %34, 0
  %36 = icmp eq %struct.halide_device_interface_t* %2, null
  br label %40

; <label>:37:                                     ; preds = %21
  %38 = icmp eq %struct.halide_device_interface_t* %2, null
  %39 = icmp eq i8* %26, null
  br i1 %39, label %50, label %._crit_edge

._crit_edge:                                      ; preds = %37
  %.phi.trans.insert = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 3
  %.pre = load i64, i64* %.phi.trans.insert, align 8, !tbaa !33
  br label %40

; <label>:40:                                     ; preds = %._crit_edge, %31
  %41 = phi i64 [ %33, %31 ], [ %.pre, %._crit_edge ]
  %42 = phi i1 [ %36, %31 ], [ %38, %._crit_edge ]
  %43 = phi i1 [ %35, %31 ], [ true, %._crit_edge ]
  %44 = and i64 %41, 2
  %45 = icmp eq i64 %44, 0
  br i1 %45, label %50, label %46

; <label>:46:                                     ; preds = %40
  %47 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %48 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %47, align 8, !tbaa !31
  %49 = icmp ne %struct.halide_device_interface_t* %48, null
  br label %50

; <label>:50:                                     ; preds = %46, %40, %37, %29
  %51 = phi i1 [ false, %37 ], [ true, %40 ], [ true, %46 ], [ false, %29 ]
  %52 = phi i1 [ %38, %37 ], [ %42, %40 ], [ %42, %46 ], [ %30, %29 ]
  %53 = phi i1 [ true, %37 ], [ %43, %40 ], [ %43, %46 ], [ false, %29 ]
  %54 = phi i1 [ true, %37 ], [ false, %40 ], [ %49, %46 ], [ true, %29 ]
  %55 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 2
  %56 = load i8*, i8** %55, align 4, !tbaa !32
  %57 = icmp ne i8* %56, null
  %58 = xor i1 %52, true
  %59 = or i1 %57, %58
  br i1 %59, label %60, label %130

; <label>:60:                                     ; preds = %50
  %61 = xor i1 %6, true
  %62 = or i1 %53, %61
  br i1 %62, label %70, label %63

; <label>:63:                                     ; preds = %60
  %64 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %2, i32 0, i32 15
  %65 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %64, align 4, !tbaa !51
  %66 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %65, i32 0, i32 10
  %67 = load i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*, %struct.halide_buffer_t*)** %66, align 4, !tbaa !68
  %68 = tail call i32 %67(i8* %0, %struct.halide_buffer_t* nonnull %1, %struct.halide_device_interface_t* nonnull %2, %struct.halide_buffer_t* nonnull %3) #11
  %69 = icmp eq i32 %68, -42
  br i1 %69, label %70, label %117

; <label>:70:                                     ; preds = %63, %60
  %71 = or i1 %51, %57
  br i1 %71, label %72, label %130

; <label>:72:                                     ; preds = %70
  %73 = or i1 %54, %58
  br i1 %73, label %76, label %74

; <label>:74:                                     ; preds = %72
  %75 = bitcast %"struct.Halide::Runtime::Internal::device_copy"* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 416, i8* nonnull %75) #9
  call void @_ZN6Halide7Runtime8Internal16make_buffer_copyEPK15halide_buffer_tbS4_b(%"struct.Halide::Runtime::Internal::device_copy"* nonnull sret %5, %struct.halide_buffer_t* nonnull %1, i1 zeroext true, %struct.halide_buffer_t* nonnull %3, i1 zeroext true) #12
  call void @_ZN6Halide7Runtime8Internal11copy_memoryERKNS1_11device_copyEPv(%"struct.Halide::Runtime::Internal::device_copy"* nonnull dereferenceable(416) %5, i8* %0) #12
  call void @llvm.lifetime.end.p0i8(i64 416, i8* nonnull %75) #9
  br label %120

; <label>:76:                                     ; preds = %72
  br i1 %52, label %77, label %91

; <label>:77:                                     ; preds = %76
  %78 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %79 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %78, align 8, !tbaa !31
  %80 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %79, i32 0, i32 15
  %81 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %80, align 4, !tbaa !51
  %82 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %81, i32 0, i32 10
  %83 = load i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*, %struct.halide_buffer_t*)** %82, align 4, !tbaa !68
  %84 = tail call i32 %83(i8* %0, %struct.halide_buffer_t* nonnull %1, %struct.halide_device_interface_t* null, %struct.halide_buffer_t* nonnull %3) #11
  %85 = icmp eq i32 %84, -42
  br i1 %85, label %86, label %117

; <label>:86:                                     ; preds = %77
  %87 = tail call i32 @_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t(i8* %0, %struct.halide_buffer_t* nonnull %1) #12
  %88 = icmp eq i32 %87, 0
  br i1 %88, label %89, label %130

; <label>:89:                                     ; preds = %86
  %90 = tail call i32 @halide_buffer_copy_already_locked(i8* %0, %struct.halide_buffer_t* nonnull %1, %struct.halide_device_interface_t* null, %struct.halide_buffer_t* nonnull %3) #12
  br label %117

; <label>:91:                                     ; preds = %76
  %92 = xor i1 %57, true
  %93 = or i1 %53, %92
  br i1 %93, label %108, label %94

; <label>:94:                                     ; preds = %91
  %95 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %96 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %95, align 8, !tbaa !31
  %97 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %96, i32 0, i32 15
  %98 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %97, align 4, !tbaa !51
  %99 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %98, i32 0, i32 10
  %100 = load i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*, %struct.halide_buffer_t*)** %99, align 4, !tbaa !68
  %101 = tail call i32 %100(i8* %0, %struct.halide_buffer_t* nonnull %1, %struct.halide_device_interface_t* null, %struct.halide_buffer_t* nonnull %3) #11
  %102 = icmp eq i32 %101, 0
  br i1 %102, label %103, label %130

; <label>:103:                                    ; preds = %94
  %104 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 3
  %105 = load i64, i64* %104, align 8, !tbaa !33
  %106 = or i64 %105, 1
  store i64 %106, i64* %104, align 8, !tbaa !33
  %107 = tail call i32 @copy_to_device_already_locked(i8* %0, %struct.halide_buffer_t* nonnull %3, %struct.halide_device_interface_t* nonnull %2) #12
  br label %117

; <label>:108:                                    ; preds = %91
  %109 = tail call i32 @_ZN6Halide7Runtime8Internal27copy_to_host_already_lockedEPvP15halide_buffer_t(i8* %0, %struct.halide_buffer_t* nonnull %1) #12
  %110 = icmp eq i32 %109, 0
  br i1 %110, label %111, label %130

; <label>:111:                                    ; preds = %108
  %112 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %2, i32 0, i32 15
  %113 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %112, align 4, !tbaa !51
  %114 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %113, i32 0, i32 10
  %115 = load i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*, %struct.halide_buffer_t*)** %114, align 4, !tbaa !68
  %116 = tail call i32 %115(i8* %0, %struct.halide_buffer_t* nonnull %1, %struct.halide_device_interface_t* nonnull %2, %struct.halide_buffer_t* nonnull %3) #11
  br label %117

; <label>:117:                                    ; preds = %111, %103, %89, %77, %63
  %118 = phi i32 [ %90, %89 ], [ %84, %77 ], [ %107, %103 ], [ %116, %111 ], [ %68, %63 ]
  %119 = icmp eq i32 %118, 0
  br i1 %119, label %120, label %130

; <label>:120:                                    ; preds = %117, %74
  %121 = icmp eq %struct.halide_buffer_t* %3, %1
  br i1 %121, label %130, label %122

; <label>:122:                                    ; preds = %120
  %123 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 3
  %124 = load i64, i64* %123, align 8, !tbaa !33
  %125 = and i64 %124, -4
  br i1 %6, label %126, label %128

; <label>:126:                                    ; preds = %122
  %127 = or i64 %125, 2
  store i64 %127, i64* %123, align 8, !tbaa !33
  br label %130

; <label>:128:                                    ; preds = %122
  %129 = or i64 %125, 1
  store i64 %129, i64* %123, align 8, !tbaa !33
  br label %130

; <label>:130:                                    ; preds = %128, %126, %120, %117, %108, %94, %86, %70, %50, %18, %13
  %131 = phi i32 [ -42, %13 ], [ %19, %18 ], [ -34, %50 ], [ 0, %120 ], [ 0, %126 ], [ 0, %128 ], [ -42, %70 ], [ %118, %117 ], [ %109, %108 ], [ %101, %94 ], [ %87, %86 ]
  ret i32 %131
}

; Function Attrs: nounwind
define weak i32 @halide_buffer_copy(i8*, %struct.halide_buffer_t*, %struct.halide_device_interface_t*, %struct.halide_buffer_t*) local_unnamed_addr #0 {
  tail call void @halide_mutex_lock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #11
  %5 = icmp ne %struct.halide_device_interface_t* %2, null
  br i1 %5, label %6, label %11

; <label>:6:                                      ; preds = %4
  %7 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %2, i32 0, i32 15
  %8 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %7, align 4, !tbaa !51
  %9 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %8, i32 0, i32 0
  %10 = load void ()*, void ()** %9, align 4, !tbaa !57
  tail call void %10() #11
  br label %11

; <label>:11:                                     ; preds = %6, %4
  %12 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %13 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %12, align 8, !tbaa !31
  %14 = icmp eq %struct.halide_device_interface_t* %13, null
  br i1 %14, label %20, label %15

; <label>:15:                                     ; preds = %11
  %16 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %13, i32 0, i32 15
  %17 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %16, align 4, !tbaa !51
  %18 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %17, i32 0, i32 0
  %19 = load void ()*, void ()** %18, align 4, !tbaa !57
  tail call void %19() #11
  br label %20

; <label>:20:                                     ; preds = %15, %11
  %21 = tail call i32 @halide_buffer_copy_already_locked(i8* %0, %struct.halide_buffer_t* nonnull %1, %struct.halide_device_interface_t* %2, %struct.halide_buffer_t* %3) #12
  br i1 %5, label %22, label %27

; <label>:22:                                     ; preds = %20
  %23 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %2, i32 0, i32 15
  %24 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %23, align 4, !tbaa !51
  %25 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %24, i32 0, i32 1
  %26 = load void ()*, void ()** %25, align 4, !tbaa !59
  tail call void %26() #11
  br label %27

; <label>:27:                                     ; preds = %22, %20
  %28 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %12, align 8, !tbaa !31
  %29 = icmp eq %struct.halide_device_interface_t* %28, null
  br i1 %29, label %35, label %30

; <label>:30:                                     ; preds = %27
  %31 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %28, i32 0, i32 15
  %32 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %31, align 4, !tbaa !51
  %33 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %32, i32 0, i32 1
  %34 = load void ()*, void ()** %33, align 4, !tbaa !59
  tail call void %34() #11
  br label %35

; <label>:35:                                     ; preds = %30, %27
  tail call void @halide_mutex_unlock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #11
  ret i32 %21
}

; Function Attrs: nounwind
define weak i32 @halide_default_device_crop(i8*, %struct.halide_buffer_t*, %struct.halide_buffer_t*) local_unnamed_addr #0 {
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([44 x i8], [44 x i8]* @.str.58, i32 0, i32 0)) #11
  ret i32 -40
}

; Function Attrs: nounwind
define weak i32 @halide_default_device_slice(i8*, %struct.halide_buffer_t*, i32, i32, %struct.halide_buffer_t*) local_unnamed_addr #0 {
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([43 x i8], [43 x i8]* @.str.59, i32 0, i32 0)) #11
  ret i32 -40
}

; Function Attrs: nounwind
define weak i32 @halide_device_crop(i8*, %struct.halide_buffer_t*, %struct.halide_buffer_t*) local_unnamed_addr #0 {
  tail call void @halide_mutex_lock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #11
  %4 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %5 = load i64, i64* %4, align 8, !tbaa !29
  %6 = icmp eq i64 %5, 0
  br i1 %6, label %32, label %7

; <label>:7:                                      ; preds = %3
  %8 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %2, i32 0, i32 0
  %9 = load i64, i64* %8, align 8, !tbaa !29
  %10 = icmp eq i64 %9, 0
  br i1 %10, label %12, label %11

; <label>:11:                                     ; preds = %7
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([52 x i8], [52 x i8]* @.str.60, i32 0, i32 0)) #11
  br label %32

; <label>:12:                                     ; preds = %7
  %13 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 5
  %14 = load i32, i32* %13, align 4, !tbaa !34
  %15 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %2, i32 0, i32 5
  %16 = load i32, i32* %15, align 4, !tbaa !34
  %17 = icmp eq i32 %14, %16
  br i1 %17, label %19, label %18

; <label>:18:                                     ; preds = %12
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([48 x i8], [48 x i8]* @.str.61, i32 0, i32 0)) #11
  br label %32

; <label>:19:                                     ; preds = %12
  %20 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %21 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %20, align 8, !tbaa !31
  %22 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %21, i32 0, i32 15
  %23 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %22, align 4, !tbaa !51
  %24 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %23, i32 0, i32 0
  %25 = load void ()*, void ()** %24, align 4, !tbaa !57
  tail call void %25() #11
  %26 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %20, align 8, !tbaa !31
  %27 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %26, i32 0, i32 15
  %28 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %27, align 4, !tbaa !51
  %29 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %28, i32 0, i32 11
  %30 = load i32 (i8*, %struct.halide_buffer_t*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, %struct.halide_buffer_t*)** %29, align 4, !tbaa !69
  %31 = tail call i32 %30(i8* %0, %struct.halide_buffer_t* nonnull %1, %struct.halide_buffer_t* nonnull %2) #11
  br label %32

; <label>:32:                                     ; preds = %19, %18, %11, %3
  %33 = phi i32 [ -41, %11 ], [ -41, %18 ], [ %31, %19 ], [ 0, %3 ]
  tail call void @halide_mutex_unlock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #11
  ret i32 %33
}

; Function Attrs: nounwind
define weak i32 @halide_device_slice(i8*, %struct.halide_buffer_t*, i32, i32, %struct.halide_buffer_t*) local_unnamed_addr #0 {
  tail call void @halide_mutex_lock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #11
  %6 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %7 = load i64, i64* %6, align 8, !tbaa !29
  %8 = icmp eq i64 %7, 0
  br i1 %8, label %35, label %9

; <label>:9:                                      ; preds = %5
  %10 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %4, i32 0, i32 0
  %11 = load i64, i64* %10, align 8, !tbaa !29
  %12 = icmp eq i64 %11, 0
  br i1 %12, label %14, label %13

; <label>:13:                                     ; preds = %9
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([52 x i8], [52 x i8]* @.str.60, i32 0, i32 0)) #11
  br label %35

; <label>:14:                                     ; preds = %9
  %15 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 5
  %16 = load i32, i32* %15, align 4, !tbaa !34
  %17 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %4, i32 0, i32 5
  %18 = load i32, i32* %17, align 4, !tbaa !34
  %19 = add nsw i32 %18, 1
  %20 = icmp eq i32 %16, %19
  br i1 %20, label %22, label %21

; <label>:21:                                     ; preds = %14
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([52 x i8], [52 x i8]* @.str.64, i32 0, i32 0)) #11
  br label %35

; <label>:22:                                     ; preds = %14
  %23 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %24 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %23, align 8, !tbaa !31
  %25 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %24, i32 0, i32 15
  %26 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %25, align 4, !tbaa !51
  %27 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %26, i32 0, i32 0
  %28 = load void ()*, void ()** %27, align 4, !tbaa !57
  tail call void %28() #11
  %29 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %23, align 8, !tbaa !31
  %30 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %29, i32 0, i32 15
  %31 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %30, align 4, !tbaa !51
  %32 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %31, i32 0, i32 12
  %33 = load i32 (i8*, %struct.halide_buffer_t*, i32, i32, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*, i32, i32, %struct.halide_buffer_t*)** %32, align 4, !tbaa !70
  %34 = tail call i32 %33(i8* %0, %struct.halide_buffer_t* nonnull %1, i32 %2, i32 %3, %struct.halide_buffer_t* nonnull %4) #11
  br label %35

; <label>:35:                                     ; preds = %22, %21, %13, %5
  %36 = phi i32 [ -41, %13 ], [ -41, %21 ], [ %34, %22 ], [ 0, %5 ]
  tail call void @halide_mutex_unlock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #11
  ret i32 %36
}

; Function Attrs: nounwind
define weak i32 @halide_default_device_release_crop(i8*, %struct.halide_buffer_t*) local_unnamed_addr #0 {
  %3 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %4 = load i64, i64* %3, align 8, !tbaa !29
  %5 = icmp eq i64 %4, 0
  br i1 %5, label %7, label %6

; <label>:6:                                      ; preds = %2
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([44 x i8], [44 x i8]* @.str.58, i32 0, i32 0)) #11
  br label %7

; <label>:7:                                      ; preds = %6, %2
  %8 = phi i32 [ -40, %6 ], [ 0, %2 ]
  ret i32 %8
}

; Function Attrs: nounwind
define weak i32 @halide_device_release_crop(i8*, %struct.halide_buffer_t*) local_unnamed_addr #0 {
  %3 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 0
  %4 = load i64, i64* %3, align 8, !tbaa !29
  %5 = icmp eq i64 %4, 0
  br i1 %5, label %17, label %6

; <label>:6:                                      ; preds = %2
  tail call void @halide_mutex_lock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #11
  %7 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %1, i32 0, i32 1
  %8 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %7, align 8, !tbaa !31
  %9 = getelementptr inbounds %struct.halide_device_interface_t, %struct.halide_device_interface_t* %8, i32 0, i32 15
  %10 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %9, align 4, !tbaa !51
  %11 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %10, i32 0, i32 13
  %12 = load i32 (i8*, %struct.halide_buffer_t*)*, i32 (i8*, %struct.halide_buffer_t*)** %11, align 4, !tbaa !71
  %13 = tail call i32 %12(i8* %0, %struct.halide_buffer_t* nonnull %1) #11
  store i64 0, i64* %3, align 8, !tbaa !29
  %14 = load %struct.halide_device_interface_impl_t*, %struct.halide_device_interface_impl_t** %9, align 4, !tbaa !51
  %15 = getelementptr inbounds %struct.halide_device_interface_impl_t, %struct.halide_device_interface_impl_t* %14, i32 0, i32 1
  %16 = load void ()*, void ()** %15, align 4, !tbaa !59
  tail call void %16() #11
  store %struct.halide_device_interface_t* null, %struct.halide_device_interface_t** %7, align 8, !tbaa !31
  tail call void @halide_mutex_unlock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal17device_copy_mutexE) #11
  br label %17

; <label>:17:                                     ; preds = %6, %2
  %18 = phi i32 [ %13, %6 ], [ 0, %2 ]
  ret i32 %18
}

; Function Attrs: nounwind
define weak float @halide_float16_bits_to_float(i16 zeroext) local_unnamed_addr #0 {
  %2 = zext i16 %0 to i32
  %3 = shl nuw i32 %2, 16
  %4 = and i32 %3, -2147483648
  %5 = and i32 %2, 1023
  %6 = lshr i32 %2, 10
  %7 = and i32 %6, 31
  %8 = icmp eq i32 %7, 0
  %9 = icmp ne i32 %5, 0
  %10 = and i1 %9, %8
  br i1 %10, label %11, label %21

; <label>:11:                                     ; preds = %1
  %12 = tail call i32 @llvm.ctlz.i32(i32 %5, i1 true), !range !72
  %13 = xor i32 %12, 31
  %14 = shl i32 1, %13
  %15 = xor i32 %14, -1
  %16 = and i32 %5, %15
  %17 = sub nsw i32 23, %13
  %18 = shl i32 %16, %17
  %19 = shl nuw nsw i32 %13, 23
  %20 = add nuw nsw i32 %19, 864026624
  br label %28

; <label>:21:                                     ; preds = %1
  %22 = shl nuw nsw i32 %5, 13
  br i1 %8, label %28, label %23

; <label>:23:                                     ; preds = %21
  %24 = icmp eq i32 %7, 31
  br i1 %24, label %28, label %25

; <label>:25:                                     ; preds = %23
  %26 = shl nuw nsw i32 %7, 23
  %27 = add nuw nsw i32 %26, 939524096
  br label %28

; <label>:28:                                     ; preds = %25, %23, %21, %11
  %29 = phi i32 [ %20, %11 ], [ %22, %23 ], [ %22, %21 ], [ %22, %25 ]
  %30 = phi i32 [ %18, %11 ], [ 2139095040, %23 ], [ 0, %21 ], [ %27, %25 ]
  %31 = or i32 %29, %4
  %32 = or i32 %31, %30
  %33 = bitcast i32 %32 to float
  ret float %33
}

; Function Attrs: nounwind readnone speculatable
declare i32 @llvm.ctlz.i32(i32, i1) #6

; Function Attrs: nounwind
define weak double @halide_float16_bits_to_double(i16 zeroext) local_unnamed_addr #0 {
  %2 = tail call float @halide_float16_bits_to_float(i16 zeroext %0) #12
  %3 = fpext float %2 to double
  ret double %3
}

; Function Attrs: nounwind
define weak i32 @halide_error_bounds_inference_call_failed(i8*, i8*, i32) local_unnamed_addr #0 {
  %4 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %5 = ptrtoint i8* %4 to i32
  %6 = icmp eq i8* %4, null
  br i1 %6, label %.split, label %.split2

.split:                                           ; preds = %3
  %7 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([41 x i8], [41 x i8]* @.str.37, i32 0, i32 0)) #11
  br label %10

.split2:                                          ; preds = %3
  %8 = getelementptr inbounds i8, i8* %4, i32 1023
  store i8 0, i8* %8, align 1, !tbaa !17
  %9 = tail call i8* @halide_string_to_string(i8* nonnull %4, i8* nonnull %8, i8* nonnull getelementptr inbounds ([41 x i8], [41 x i8]* @.str.37, i32 0, i32 0)) #11
  br label %10

; <label>:10:                                     ; preds = %.split, %.split2
  %phi.call = phi i8* [ %7, %.split ], [ %9, %.split2 ]
  %11 = phi i8* [ null, %.split ], [ %8, %.split2 ]
  %12 = icmp eq i8* %1, null
  br i1 %12, label %13, label %15

; <label>:13:                                     ; preds = %10
  %14 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %11, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %17

; <label>:15:                                     ; preds = %10
  %16 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %11, i8* nonnull %1) #11
  br label %17

; <label>:17:                                     ; preds = %15, %13
  %18 = phi i8* [ %16, %15 ], [ %14, %13 ]
  %19 = tail call i8* @halide_string_to_string(i8* %18, i8* %11, i8* nonnull getelementptr inbounds ([27 x i8], [27 x i8]* @.str.1.38, i32 0, i32 0)) #11
  %20 = sext i32 %2 to i64
  %21 = tail call i8* @halide_int64_to_string(i8* %19, i8* %11, i64 %20, i32 1) #11
  br i1 %6, label %.split4, label %.split6

.split4:                                          ; preds = %17
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %27

.split6:                                          ; preds = %17
  %22 = ptrtoint i8* %21 to i32
  %23 = sub i32 1, %5
  %24 = add i32 %23, %22
  %25 = sext i32 %24 to i64
  %26 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %4, i64 %25) #11
  tail call void @halide_error(i8* %0, i8* nonnull %4) #11
  tail call void @halide_free(i8* %0, i8* nonnull %4) #11
  br label %27

; <label>:27:                                     ; preds = %.split6, %.split4
  ret i32 %2
}

; Function Attrs: nounwind
define weak i32 @halide_error_extern_stage_failed(i8*, i8*, i32) local_unnamed_addr #0 {
  %4 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %5 = ptrtoint i8* %4 to i32
  %6 = icmp eq i8* %4, null
  br i1 %6, label %.split, label %.split2

.split:                                           ; preds = %3
  %7 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([24 x i8], [24 x i8]* @.str.2.39, i32 0, i32 0)) #11
  br label %10

.split2:                                          ; preds = %3
  %8 = getelementptr inbounds i8, i8* %4, i32 1023
  store i8 0, i8* %8, align 1, !tbaa !17
  %9 = tail call i8* @halide_string_to_string(i8* nonnull %4, i8* nonnull %8, i8* nonnull getelementptr inbounds ([24 x i8], [24 x i8]* @.str.2.39, i32 0, i32 0)) #11
  br label %10

; <label>:10:                                     ; preds = %.split, %.split2
  %phi.call = phi i8* [ %7, %.split ], [ %9, %.split2 ]
  %11 = phi i8* [ null, %.split ], [ %8, %.split2 ]
  %12 = icmp eq i8* %1, null
  br i1 %12, label %13, label %15

; <label>:13:                                     ; preds = %10
  %14 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %11, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %17

; <label>:15:                                     ; preds = %10
  %16 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %11, i8* nonnull %1) #11
  br label %17

; <label>:17:                                     ; preds = %15, %13
  %18 = phi i8* [ %16, %15 ], [ %14, %13 ]
  %19 = tail call i8* @halide_string_to_string(i8* %18, i8* %11, i8* nonnull getelementptr inbounds ([27 x i8], [27 x i8]* @.str.1.38, i32 0, i32 0)) #11
  %20 = sext i32 %2 to i64
  %21 = tail call i8* @halide_int64_to_string(i8* %19, i8* %11, i64 %20, i32 1) #11
  br i1 %6, label %.split4, label %.split6

.split4:                                          ; preds = %17
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %27

.split6:                                          ; preds = %17
  %22 = ptrtoint i8* %21 to i32
  %23 = sub i32 1, %5
  %24 = add i32 %23, %22
  %25 = sext i32 %24 to i64
  %26 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %4, i64 %25) #11
  tail call void @halide_error(i8* %0, i8* nonnull %4) #11
  tail call void @halide_free(i8* %0, i8* nonnull %4) #11
  br label %27

; <label>:27:                                     ; preds = %.split6, %.split4
  ret i32 %2
}

; Function Attrs: nounwind
define weak i32 @halide_error_explicit_bounds_too_small(i8*, i8*, i8*, i32, i32, i32, i32) local_unnamed_addr #0 {
  %8 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %9 = ptrtoint i8* %8 to i32
  %10 = icmp eq i8* %8, null
  br i1 %10, label %.split, label %.split2

.split:                                           ; preds = %7
  %11 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([18 x i8], [18 x i8]* @.str.3.40, i32 0, i32 0)) #11
  br label %14

.split2:                                          ; preds = %7
  %12 = getelementptr inbounds i8, i8* %8, i32 1023
  store i8 0, i8* %12, align 1, !tbaa !17
  %13 = tail call i8* @halide_string_to_string(i8* nonnull %8, i8* nonnull %12, i8* nonnull getelementptr inbounds ([18 x i8], [18 x i8]* @.str.3.40, i32 0, i32 0)) #11
  br label %14

; <label>:14:                                     ; preds = %.split, %.split2
  %phi.call = phi i8* [ %11, %.split ], [ %13, %.split2 ]
  %15 = phi i8* [ null, %.split ], [ %12, %.split2 ]
  %16 = icmp eq i8* %2, null
  br i1 %16, label %17, label %19

; <label>:17:                                     ; preds = %14
  %18 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %15, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %21

; <label>:19:                                     ; preds = %14
  %20 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %15, i8* nonnull %2) #11
  br label %21

; <label>:21:                                     ; preds = %19, %17
  %22 = phi i8* [ %20, %19 ], [ %18, %17 ]
  %23 = tail call i8* @halide_string_to_string(i8* %22, i8* %15, i8* nonnull getelementptr inbounds ([5 x i8], [5 x i8]* @.str.4.41, i32 0, i32 0)) #11
  %24 = icmp eq i8* %1, null
  br i1 %24, label %25, label %27

; <label>:25:                                     ; preds = %21
  %26 = tail call i8* @halide_string_to_string(i8* %23, i8* %15, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %29

; <label>:27:                                     ; preds = %21
  %28 = tail call i8* @halide_string_to_string(i8* %23, i8* %15, i8* nonnull %1) #11
  br label %29

; <label>:29:                                     ; preds = %27, %25
  %30 = phi i8* [ %28, %27 ], [ %26, %25 ]
  %31 = tail call i8* @halide_string_to_string(i8* %30, i8* %15, i8* nonnull getelementptr inbounds ([8 x i8], [8 x i8]* @.str.5.42, i32 0, i32 0)) #11
  %32 = sext i32 %3 to i64
  %33 = tail call i8* @halide_int64_to_string(i8* %31, i8* %15, i64 %32, i32 1) #11
  %34 = tail call i8* @halide_string_to_string(i8* %33, i8* %15, i8* nonnull getelementptr inbounds ([5 x i8], [5 x i8]* @.str.6.43, i32 0, i32 0)) #11
  %35 = sext i32 %4 to i64
  %36 = tail call i8* @halide_int64_to_string(i8* %34, i8* %15, i64 %35, i32 1) #11
  %37 = tail call i8* @halide_string_to_string(i8* %36, i8* %15, i8* nonnull getelementptr inbounds ([38 x i8], [38 x i8]* @.str.7.44, i32 0, i32 0)) #11
  %38 = sext i32 %5 to i64
  %39 = tail call i8* @halide_int64_to_string(i8* %37, i8* %15, i64 %38, i32 1) #11
  %40 = tail call i8* @halide_string_to_string(i8* %39, i8* %15, i8* nonnull getelementptr inbounds ([5 x i8], [5 x i8]* @.str.6.43, i32 0, i32 0)) #11
  %41 = sext i32 %6 to i64
  %42 = tail call i8* @halide_int64_to_string(i8* %40, i8* %15, i64 %41, i32 1) #11
  %43 = tail call i8* @halide_string_to_string(i8* %42, i8* %15, i8* nonnull getelementptr inbounds ([2 x i8], [2 x i8]* @.str.5.94, i32 0, i32 0)) #11
  br i1 %10, label %.split4, label %.split6

.split4:                                          ; preds = %29
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %49

.split6:                                          ; preds = %29
  %44 = ptrtoint i8* %43 to i32
  %45 = sub i32 1, %9
  %46 = add i32 %45, %44
  %47 = sext i32 %46 to i64
  %48 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %8, i64 %47) #11
  tail call void @halide_error(i8* %0, i8* nonnull %8) #11
  tail call void @halide_free(i8* %0, i8* nonnull %8) #11
  br label %49

; <label>:49:                                     ; preds = %.split6, %.split4
  ret i32 -2
}

; Function Attrs: nounwind
define weak i32 @halide_error_bad_type(i8*, i8*, i32, i32) local_unnamed_addr #0 {
  %5 = alloca i32, align 4
  %6 = alloca i32, align 4
  %7 = alloca i32, align 4
  %tmpcast = bitcast i32* %7 to %struct.halide_type_t*
  %8 = alloca i32, align 4
  %tmpcast4 = bitcast i32* %8 to %struct.halide_type_t*
  store i32 %2, i32* %5, align 4, !tbaa !12
  store i32 %3, i32* %6, align 4, !tbaa !12
  %9 = bitcast i32* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #9
  %10 = bitcast i32* %8 to i8*
  store i32 0, i32* %7, align 4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #9
  %11 = bitcast i32* %6 to i8*
  store i32 0, i32* %8, align 4
  %12 = call i8* @memcpy(i8* nonnull %9, i8* nonnull %11, i32 4) #11
  %13 = bitcast i32* %5 to i8*
  %14 = call i8* @memcpy(i8* nonnull %10, i8* nonnull %13, i32 4) #11
  %15 = call i8* @halide_malloc(i8* %0, i32 1024) #11
  %16 = ptrtoint i8* %15 to i32
  %17 = icmp eq i8* %15, null
  br i1 %17, label %20, label %18

; <label>:18:                                     ; preds = %4
  %19 = getelementptr inbounds i8, i8* %15, i32 1023
  store i8 0, i8* %19, align 1, !tbaa !17
  br label %20

; <label>:20:                                     ; preds = %18, %4
  %21 = phi i8* [ %19, %18 ], [ null, %4 ]
  %22 = icmp eq i8* %1, null
  br i1 %22, label %23, label %25

; <label>:23:                                     ; preds = %20
  %24 = call i8* @halide_string_to_string(i8* %15, i8* %21, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %27

; <label>:25:                                     ; preds = %20
  %26 = call i8* @halide_string_to_string(i8* %15, i8* %21, i8* nonnull %1) #11
  br label %27

; <label>:27:                                     ; preds = %25, %23
  %28 = phi i8* [ %26, %25 ], [ %24, %23 ]
  %29 = call i8* @halide_string_to_string(i8* %28, i8* %21, i8* nonnull getelementptr inbounds ([11 x i8], [11 x i8]* @.str.9.46, i32 0, i32 0)) #11
  %30 = call i8* @halide_type_to_string(i8* %29, i8* %21, %struct.halide_type_t* nonnull %tmpcast) #11
  %31 = call i8* @halide_string_to_string(i8* %30, i8* %21, i8* nonnull getelementptr inbounds ([38 x i8], [38 x i8]* @.str.10.47, i32 0, i32 0)) #11
  %32 = call i8* @halide_type_to_string(i8* %31, i8* %21, %struct.halide_type_t* nonnull %tmpcast4) #11
  br i1 %17, label %.split, label %.split2

.split:                                           ; preds = %27
  call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  call void @halide_free(i8* %0, i8* null) #11
  br label %38

.split2:                                          ; preds = %27
  %33 = ptrtoint i8* %32 to i32
  %34 = sub i32 1, %16
  %35 = add i32 %34, %33
  %36 = sext i32 %35 to i64
  %37 = call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %15, i64 %36) #11
  call void @halide_error(i8* %0, i8* nonnull %15) #11
  call void @halide_free(i8* %0, i8* nonnull %15) #11
  br label %38

; <label>:38:                                     ; preds = %.split2, %.split
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #9
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #9
  ret i32 -3
}

; Function Attrs: nounwind
define weak i32 @halide_error_bad_dimensions(i8*, i8*, i32, i32) local_unnamed_addr #0 {
  %5 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %6 = ptrtoint i8* %5 to i32
  %7 = icmp eq i8* %5, null
  br i1 %7, label %10, label %8

; <label>:8:                                      ; preds = %4
  %9 = getelementptr inbounds i8, i8* %5, i32 1023
  store i8 0, i8* %9, align 1, !tbaa !17
  br label %10

; <label>:10:                                     ; preds = %8, %4
  %11 = phi i8* [ %9, %8 ], [ null, %4 ]
  %12 = icmp eq i8* %1, null
  br i1 %12, label %13, label %15

; <label>:13:                                     ; preds = %10
  %14 = tail call i8* @halide_string_to_string(i8* %5, i8* %11, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %17

; <label>:15:                                     ; preds = %10
  %16 = tail call i8* @halide_string_to_string(i8* %5, i8* %11, i8* nonnull %1) #11
  br label %17

; <label>:17:                                     ; preds = %15, %13
  %18 = phi i8* [ %16, %15 ], [ %14, %13 ]
  %19 = tail call i8* @halide_string_to_string(i8* %18, i8* %11, i8* nonnull getelementptr inbounds ([31 x i8], [31 x i8]* @.str.11.48, i32 0, i32 0)) #11
  %20 = sext i32 %3 to i64
  %21 = tail call i8* @halide_int64_to_string(i8* %19, i8* %11, i64 %20, i32 1) #11
  %22 = tail call i8* @halide_string_to_string(i8* %21, i8* %11, i8* nonnull getelementptr inbounds ([43 x i8], [43 x i8]* @.str.12.49, i32 0, i32 0)) #11
  %23 = sext i32 %2 to i64
  %24 = tail call i8* @halide_int64_to_string(i8* %22, i8* %11, i64 %23, i32 1) #11
  %25 = tail call i8* @halide_string_to_string(i8* %24, i8* %11, i8* nonnull getelementptr inbounds ([12 x i8], [12 x i8]* @.str.13.50, i32 0, i32 0)) #11
  br i1 %7, label %.split, label %.split2

.split:                                           ; preds = %17
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %31

.split2:                                          ; preds = %17
  %26 = ptrtoint i8* %25 to i32
  %27 = sub i32 1, %6
  %28 = add i32 %27, %26
  %29 = sext i32 %28 to i64
  %30 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %5, i64 %29) #11
  tail call void @halide_error(i8* %0, i8* nonnull %5) #11
  tail call void @halide_free(i8* %0, i8* nonnull %5) #11
  br label %31

; <label>:31:                                     ; preds = %.split2, %.split
  ret i32 -43
}

; Function Attrs: nounwind
define weak i32 @halide_error_access_out_of_bounds(i8*, i8*, i32, i32, i32, i32, i32) local_unnamed_addr #0 {
  %8 = icmp slt i32 %3, %5
  br i1 %8, label %9, label %38

; <label>:9:                                      ; preds = %7
  %10 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %11 = ptrtoint i8* %10 to i32
  %12 = icmp eq i8* %10, null
  br i1 %12, label %15, label %13

; <label>:13:                                     ; preds = %9
  %14 = getelementptr inbounds i8, i8* %10, i32 1023
  store i8 0, i8* %14, align 1, !tbaa !17
  br label %15

; <label>:15:                                     ; preds = %13, %9
  %16 = phi i8* [ %14, %13 ], [ null, %9 ]
  %17 = icmp eq i8* %1, null
  br i1 %17, label %18, label %20

; <label>:18:                                     ; preds = %15
  %19 = tail call i8* @halide_string_to_string(i8* %10, i8* %16, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %22

; <label>:20:                                     ; preds = %15
  %21 = tail call i8* @halide_string_to_string(i8* %10, i8* %16, i8* nonnull %1) #11
  br label %22

; <label>:22:                                     ; preds = %20, %18
  %23 = phi i8* [ %21, %20 ], [ %19, %18 ]
  %24 = tail call i8* @halide_string_to_string(i8* %23, i8* %16, i8* nonnull getelementptr inbounds ([17 x i8], [17 x i8]* @.str.14.51, i32 0, i32 0)) #11
  %25 = sext i32 %3 to i64
  %26 = tail call i8* @halide_int64_to_string(i8* %24, i8* %16, i64 %25, i32 1) #11
  %27 = tail call i8* @halide_string_to_string(i8* %26, i8* %16, i8* nonnull getelementptr inbounds ([28 x i8], [28 x i8]* @.str.15.52, i32 0, i32 0)) #11
  %28 = sext i32 %5 to i64
  %29 = tail call i8* @halide_int64_to_string(i8* %27, i8* %16, i64 %28, i32 1) #11
  %30 = tail call i8* @halide_string_to_string(i8* %29, i8* %16, i8* nonnull getelementptr inbounds ([16 x i8], [16 x i8]* @.str.16.53, i32 0, i32 0)) #11
  %31 = sext i32 %2 to i64
  %32 = tail call i8* @halide_int64_to_string(i8* %30, i8* %16, i64 %31, i32 1) #11
  br i1 %12, label %.split, label %.split2

.split:                                           ; preds = %22
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %69

.split2:                                          ; preds = %22
  %33 = ptrtoint i8* %32 to i32
  %34 = sub i32 1, %11
  %35 = add i32 %34, %33
  %36 = sext i32 %35 to i64
  %37 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %10, i64 %36) #11
  tail call void @halide_error(i8* %0, i8* nonnull %10) #11
  tail call void @halide_free(i8* %0, i8* nonnull %10) #11
  br label %69

; <label>:38:                                     ; preds = %7
  %39 = icmp sgt i32 %4, %6
  br i1 %39, label %40, label %69

; <label>:40:                                     ; preds = %38
  %41 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %42 = ptrtoint i8* %41 to i32
  %43 = icmp eq i8* %41, null
  br i1 %43, label %46, label %44

; <label>:44:                                     ; preds = %40
  %45 = getelementptr inbounds i8, i8* %41, i32 1023
  store i8 0, i8* %45, align 1, !tbaa !17
  br label %46

; <label>:46:                                     ; preds = %44, %40
  %47 = phi i8* [ %45, %44 ], [ null, %40 ]
  %48 = icmp eq i8* %1, null
  br i1 %48, label %49, label %51

; <label>:49:                                     ; preds = %46
  %50 = tail call i8* @halide_string_to_string(i8* %41, i8* %47, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %53

; <label>:51:                                     ; preds = %46
  %52 = tail call i8* @halide_string_to_string(i8* %41, i8* %47, i8* nonnull %1) #11
  br label %53

; <label>:53:                                     ; preds = %51, %49
  %54 = phi i8* [ %52, %51 ], [ %50, %49 ]
  %55 = tail call i8* @halide_string_to_string(i8* %54, i8* %47, i8* nonnull getelementptr inbounds ([17 x i8], [17 x i8]* @.str.14.51, i32 0, i32 0)) #11
  %56 = sext i32 %4 to i64
  %57 = tail call i8* @halide_int64_to_string(i8* %55, i8* %47, i64 %56, i32 1) #11
  %58 = tail call i8* @halide_string_to_string(i8* %57, i8* %47, i8* nonnull getelementptr inbounds ([28 x i8], [28 x i8]* @.str.17.54, i32 0, i32 0)) #11
  %59 = sext i32 %6 to i64
  %60 = tail call i8* @halide_int64_to_string(i8* %58, i8* %47, i64 %59, i32 1) #11
  %61 = tail call i8* @halide_string_to_string(i8* %60, i8* %47, i8* nonnull getelementptr inbounds ([16 x i8], [16 x i8]* @.str.16.53, i32 0, i32 0)) #11
  %62 = sext i32 %2 to i64
  %63 = tail call i8* @halide_int64_to_string(i8* %61, i8* %47, i64 %62, i32 1) #11
  br i1 %43, label %.split4, label %.split6

.split4:                                          ; preds = %53
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %69

.split6:                                          ; preds = %53
  %64 = ptrtoint i8* %63 to i32
  %65 = sub i32 1, %42
  %66 = add i32 %65, %64
  %67 = sext i32 %66 to i64
  %68 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %41, i64 %67) #11
  tail call void @halide_error(i8* %0, i8* nonnull %41) #11
  tail call void @halide_free(i8* %0, i8* nonnull %41) #11
  br label %69

; <label>:69:                                     ; preds = %.split4, %.split6, %.split, %.split2, %38
  ret i32 -4
}

; Function Attrs: nounwind
define weak i32 @halide_error_buffer_allocation_too_large(i8*, i8*, i64, i64) local_unnamed_addr #0 {
  %5 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %6 = ptrtoint i8* %5 to i32
  %7 = icmp eq i8* %5, null
  br i1 %7, label %.split, label %.split2

.split:                                           ; preds = %4
  %8 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([29 x i8], [29 x i8]* @.str.18.55, i32 0, i32 0)) #11
  br label %11

.split2:                                          ; preds = %4
  %9 = getelementptr inbounds i8, i8* %5, i32 1023
  store i8 0, i8* %9, align 1, !tbaa !17
  %10 = tail call i8* @halide_string_to_string(i8* nonnull %5, i8* nonnull %9, i8* nonnull getelementptr inbounds ([29 x i8], [29 x i8]* @.str.18.55, i32 0, i32 0)) #11
  br label %11

; <label>:11:                                     ; preds = %.split, %.split2
  %phi.call = phi i8* [ %8, %.split ], [ %10, %.split2 ]
  %12 = phi i8* [ null, %.split ], [ %9, %.split2 ]
  %13 = icmp eq i8* %1, null
  br i1 %13, label %14, label %16

; <label>:14:                                     ; preds = %11
  %15 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %12, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %18

; <label>:16:                                     ; preds = %11
  %17 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %12, i8* nonnull %1) #11
  br label %18

; <label>:18:                                     ; preds = %16, %14
  %19 = phi i8* [ %17, %16 ], [ %15, %14 ]
  %20 = tail call i8* @halide_string_to_string(i8* %19, i8* %12, i8* nonnull getelementptr inbounds ([5 x i8], [5 x i8]* @.str.19.56, i32 0, i32 0)) #11
  %21 = tail call i8* @halide_uint64_to_string(i8* %20, i8* %12, i64 %2, i32 1) #11
  %22 = tail call i8* @halide_string_to_string(i8* %21, i8* %12, i8* nonnull getelementptr inbounds ([37 x i8], [37 x i8]* @.str.20.57, i32 0, i32 0)) #11
  %23 = tail call i8* @halide_uint64_to_string(i8* %22, i8* %12, i64 %3, i32 1) #11
  br i1 %7, label %.split4, label %.split6

.split4:                                          ; preds = %18
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %29

.split6:                                          ; preds = %18
  %24 = ptrtoint i8* %23 to i32
  %25 = sub i32 1, %6
  %26 = add i32 %25, %24
  %27 = sext i32 %26 to i64
  %28 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %5, i64 %27) #11
  tail call void @halide_error(i8* %0, i8* nonnull %5) #11
  tail call void @halide_free(i8* %0, i8* nonnull %5) #11
  br label %29

; <label>:29:                                     ; preds = %.split6, %.split4
  ret i32 -5
}

; Function Attrs: nounwind
define weak i32 @halide_error_buffer_extents_negative(i8*, i8*, i32, i32) local_unnamed_addr #0 {
  %5 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %6 = ptrtoint i8* %5 to i32
  %7 = icmp eq i8* %5, null
  br i1 %7, label %.split, label %.split2

.split:                                           ; preds = %4
  %8 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([24 x i8], [24 x i8]* @.str.21.58, i32 0, i32 0)) #11
  br label %11

.split2:                                          ; preds = %4
  %9 = getelementptr inbounds i8, i8* %5, i32 1023
  store i8 0, i8* %9, align 1, !tbaa !17
  %10 = tail call i8* @halide_string_to_string(i8* nonnull %5, i8* nonnull %9, i8* nonnull getelementptr inbounds ([24 x i8], [24 x i8]* @.str.21.58, i32 0, i32 0)) #11
  br label %11

; <label>:11:                                     ; preds = %.split, %.split2
  %phi.call = phi i8* [ %8, %.split ], [ %10, %.split2 ]
  %12 = phi i8* [ null, %.split ], [ %9, %.split2 ]
  %13 = icmp eq i8* %1, null
  br i1 %13, label %14, label %16

; <label>:14:                                     ; preds = %11
  %15 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %12, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %18

; <label>:16:                                     ; preds = %11
  %17 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %12, i8* nonnull %1) #11
  br label %18

; <label>:18:                                     ; preds = %16, %14
  %19 = phi i8* [ %17, %16 ], [ %15, %14 ]
  %20 = tail call i8* @halide_string_to_string(i8* %19, i8* %12, i8* nonnull getelementptr inbounds ([12 x i8], [12 x i8]* @.str.22.59, i32 0, i32 0)) #11
  %21 = sext i32 %2 to i64
  %22 = tail call i8* @halide_int64_to_string(i8* %20, i8* %12, i64 %21, i32 1) #11
  %23 = tail call i8* @halide_string_to_string(i8* %22, i8* %12, i8* nonnull getelementptr inbounds ([15 x i8], [15 x i8]* @.str.23.60, i32 0, i32 0)) #11
  %24 = sext i32 %3 to i64
  %25 = tail call i8* @halide_int64_to_string(i8* %23, i8* %12, i64 %24, i32 1) #11
  %26 = tail call i8* @halide_string_to_string(i8* %25, i8* %12, i8* nonnull getelementptr inbounds ([2 x i8], [2 x i8]* @.str.5.94, i32 0, i32 0)) #11
  br i1 %7, label %.split4, label %.split6

.split4:                                          ; preds = %18
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %32

.split6:                                          ; preds = %18
  %27 = ptrtoint i8* %26 to i32
  %28 = sub i32 1, %6
  %29 = add i32 %28, %27
  %30 = sext i32 %29 to i64
  %31 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %5, i64 %30) #11
  tail call void @halide_error(i8* %0, i8* nonnull %5) #11
  tail call void @halide_free(i8* %0, i8* nonnull %5) #11
  br label %32

; <label>:32:                                     ; preds = %.split6, %.split4
  ret i32 -28
}

; Function Attrs: nounwind
define weak i32 @halide_error_buffer_extents_too_large(i8*, i8*, i64, i64) local_unnamed_addr #0 {
  %5 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %6 = ptrtoint i8* %5 to i32
  %7 = icmp eq i8* %5, null
  br i1 %7, label %.split, label %.split2

.split:                                           ; preds = %4
  %8 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([31 x i8], [31 x i8]* @.str.24.61, i32 0, i32 0)) #11
  br label %11

.split2:                                          ; preds = %4
  %9 = getelementptr inbounds i8, i8* %5, i32 1023
  store i8 0, i8* %9, align 1, !tbaa !17
  %10 = tail call i8* @halide_string_to_string(i8* nonnull %5, i8* nonnull %9, i8* nonnull getelementptr inbounds ([31 x i8], [31 x i8]* @.str.24.61, i32 0, i32 0)) #11
  br label %11

; <label>:11:                                     ; preds = %.split, %.split2
  %phi.call = phi i8* [ %8, %.split ], [ %10, %.split2 ]
  %12 = phi i8* [ null, %.split ], [ %9, %.split2 ]
  %13 = icmp eq i8* %1, null
  br i1 %13, label %14, label %16

; <label>:14:                                     ; preds = %11
  %15 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %12, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %18

; <label>:16:                                     ; preds = %11
  %17 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %12, i8* nonnull %1) #11
  br label %18

; <label>:18:                                     ; preds = %16, %14
  %19 = phi i8* [ %17, %16 ], [ %15, %14 ]
  %20 = tail call i8* @halide_string_to_string(i8* %19, i8* %12, i8* nonnull getelementptr inbounds ([5 x i8], [5 x i8]* @.str.19.56, i32 0, i32 0)) #11
  %21 = tail call i8* @halide_int64_to_string(i8* %20, i8* %12, i64 %2, i32 1) #11
  %22 = tail call i8* @halide_string_to_string(i8* %21, i8* %12, i8* nonnull getelementptr inbounds ([37 x i8], [37 x i8]* @.str.20.57, i32 0, i32 0)) #11
  %23 = tail call i8* @halide_int64_to_string(i8* %22, i8* %12, i64 %3, i32 1) #11
  br i1 %7, label %.split4, label %.split6

.split4:                                          ; preds = %18
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %29

.split6:                                          ; preds = %18
  %24 = ptrtoint i8* %23 to i32
  %25 = sub i32 1, %6
  %26 = add i32 %25, %24
  %27 = sext i32 %26 to i64
  %28 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %5, i64 %27) #11
  tail call void @halide_error(i8* %0, i8* nonnull %5) #11
  tail call void @halide_free(i8* %0, i8* nonnull %5) #11
  br label %29

; <label>:29:                                     ; preds = %.split6, %.split4
  ret i32 -6
}

; Function Attrs: nounwind
define weak i32 @halide_error_constraints_make_required_region_smaller(i8*, i8*, i32, i32, i32, i32, i32) local_unnamed_addr #0 {
  %8 = add i32 %5, -1
  %9 = add i32 %8, %6
  %10 = add i32 %3, -1
  %11 = add i32 %10, %4
  %12 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %13 = ptrtoint i8* %12 to i32
  %14 = icmp eq i8* %12, null
  br i1 %14, label %.split, label %.split2

.split:                                           ; preds = %7
  %15 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([29 x i8], [29 x i8]* @.str.25.62, i32 0, i32 0)) #11
  br label %18

.split2:                                          ; preds = %7
  %16 = getelementptr inbounds i8, i8* %12, i32 1023
  store i8 0, i8* %16, align 1, !tbaa !17
  %17 = tail call i8* @halide_string_to_string(i8* nonnull %12, i8* nonnull %16, i8* nonnull getelementptr inbounds ([29 x i8], [29 x i8]* @.str.25.62, i32 0, i32 0)) #11
  br label %18

; <label>:18:                                     ; preds = %.split, %.split2
  %phi.call = phi i8* [ %15, %.split ], [ %17, %.split2 ]
  %19 = phi i8* [ null, %.split ], [ %16, %.split2 ]
  %20 = icmp eq i8* %1, null
  br i1 %20, label %21, label %23

; <label>:21:                                     ; preds = %18
  %22 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %19, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %25

; <label>:23:                                     ; preds = %18
  %24 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %19, i8* nonnull %1) #11
  br label %25

; <label>:25:                                     ; preds = %23, %21
  %26 = phi i8* [ %24, %23 ], [ %22, %21 ]
  %27 = tail call i8* @halide_string_to_string(i8* %26, i8* %19, i8* nonnull getelementptr inbounds ([54 x i8], [54 x i8]* @.str.26.63, i32 0, i32 0)) #11
  %28 = sext i32 %2 to i64
  %29 = tail call i8* @halide_int64_to_string(i8* %27, i8* %19, i64 %28, i32 1) #11
  %30 = tail call i8* @halide_string_to_string(i8* %29, i8* %19, i8* nonnull getelementptr inbounds ([3 x i8], [3 x i8]* @.str.27.64, i32 0, i32 0)) #11
  %31 = tail call i8* @halide_string_to_string(i8* %30, i8* %19, i8* nonnull getelementptr inbounds ([16 x i8], [16 x i8]* @.str.28.65, i32 0, i32 0)) #11
  %32 = sext i32 %5 to i64
  %33 = tail call i8* @halide_int64_to_string(i8* %31, i8* %19, i64 %32, i32 1) #11
  %34 = tail call i8* @halide_string_to_string(i8* %33, i8* %19, i8* nonnull getelementptr inbounds ([5 x i8], [5 x i8]* @.str.6.43, i32 0, i32 0)) #11
  %35 = sext i32 %9 to i64
  %36 = tail call i8* @halide_int64_to_string(i8* %34, i8* %19, i64 %35, i32 1) #11
  %37 = tail call i8* @halide_string_to_string(i8* %36, i8* %19, i8* nonnull getelementptr inbounds ([3 x i8], [3 x i8]* @.str.27.64, i32 0, i32 0)) #11
  %38 = tail call i8* @halide_string_to_string(i8* %37, i8* %19, i8* nonnull getelementptr inbounds ([19 x i8], [19 x i8]* @.str.29.66, i32 0, i32 0)) #11
  %39 = sext i32 %3 to i64
  %40 = tail call i8* @halide_int64_to_string(i8* %38, i8* %19, i64 %39, i32 1) #11
  %41 = tail call i8* @halide_string_to_string(i8* %40, i8* %19, i8* nonnull getelementptr inbounds ([5 x i8], [5 x i8]* @.str.6.43, i32 0, i32 0)) #11
  %42 = sext i32 %11 to i64
  %43 = tail call i8* @halide_int64_to_string(i8* %41, i8* %19, i64 %42, i32 1) #11
  %44 = tail call i8* @halide_string_to_string(i8* %43, i8* %19, i8* nonnull getelementptr inbounds ([2 x i8], [2 x i8]* @.str.30.67, i32 0, i32 0)) #11
  br i1 %14, label %.split4, label %.split6

.split4:                                          ; preds = %25
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %50

.split6:                                          ; preds = %25
  %45 = ptrtoint i8* %44 to i32
  %46 = sub i32 1, %13
  %47 = add i32 %46, %45
  %48 = sext i32 %47 to i64
  %49 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %12, i64 %48) #11
  tail call void @halide_error(i8* %0, i8* nonnull %12) #11
  tail call void @halide_free(i8* %0, i8* nonnull %12) #11
  br label %50

; <label>:50:                                     ; preds = %.split6, %.split4
  ret i32 -7
}

; Function Attrs: nounwind
define weak i32 @halide_error_constraint_violated(i8*, i8*, i32, i8*, i32) local_unnamed_addr #0 {
  %6 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %7 = ptrtoint i8* %6 to i32
  %8 = icmp eq i8* %6, null
  br i1 %8, label %.split, label %.split2

.split:                                           ; preds = %5
  %9 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([22 x i8], [22 x i8]* @.str.31.68, i32 0, i32 0)) #11
  br label %12

.split2:                                          ; preds = %5
  %10 = getelementptr inbounds i8, i8* %6, i32 1023
  store i8 0, i8* %10, align 1, !tbaa !17
  %11 = tail call i8* @halide_string_to_string(i8* nonnull %6, i8* nonnull %10, i8* nonnull getelementptr inbounds ([22 x i8], [22 x i8]* @.str.31.68, i32 0, i32 0)) #11
  br label %12

; <label>:12:                                     ; preds = %.split, %.split2
  %phi.call = phi i8* [ %9, %.split ], [ %11, %.split2 ]
  %13 = phi i8* [ null, %.split ], [ %10, %.split2 ]
  %14 = icmp eq i8* %1, null
  br i1 %14, label %15, label %17

; <label>:15:                                     ; preds = %12
  %16 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %13, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %19

; <label>:17:                                     ; preds = %12
  %18 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %13, i8* nonnull %1) #11
  br label %19

; <label>:19:                                     ; preds = %17, %15
  %20 = phi i8* [ %18, %17 ], [ %16, %15 ]
  %21 = tail call i8* @halide_string_to_string(i8* %20, i8* %13, i8* nonnull getelementptr inbounds ([3 x i8], [3 x i8]* @.str.32.69, i32 0, i32 0)) #11
  %22 = sext i32 %2 to i64
  %23 = tail call i8* @halide_int64_to_string(i8* %21, i8* %13, i64 %22, i32 1) #11
  %24 = tail call i8* @halide_string_to_string(i8* %23, i8* %13, i8* nonnull getelementptr inbounds ([6 x i8], [6 x i8]* @.str.33.70, i32 0, i32 0)) #11
  %25 = icmp eq i8* %3, null
  br i1 %25, label %26, label %28

; <label>:26:                                     ; preds = %19
  %27 = tail call i8* @halide_string_to_string(i8* %24, i8* %13, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %30

; <label>:28:                                     ; preds = %19
  %29 = tail call i8* @halide_string_to_string(i8* %24, i8* %13, i8* nonnull %3) #11
  br label %30

; <label>:30:                                     ; preds = %28, %26
  %31 = phi i8* [ %29, %28 ], [ %27, %26 ]
  %32 = tail call i8* @halide_string_to_string(i8* %31, i8* %13, i8* nonnull getelementptr inbounds ([3 x i8], [3 x i8]* @.str.32.69, i32 0, i32 0)) #11
  %33 = sext i32 %4 to i64
  %34 = tail call i8* @halide_int64_to_string(i8* %32, i8* %13, i64 %33, i32 1) #11
  %35 = tail call i8* @halide_string_to_string(i8* %34, i8* %13, i8* nonnull getelementptr inbounds ([2 x i8], [2 x i8]* @.str.5.94, i32 0, i32 0)) #11
  br i1 %8, label %.split4, label %.split6

.split4:                                          ; preds = %30
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %41

.split6:                                          ; preds = %30
  %36 = ptrtoint i8* %35 to i32
  %37 = sub i32 1, %7
  %38 = add i32 %37, %36
  %39 = sext i32 %38 to i64
  %40 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %6, i64 %39) #11
  tail call void @halide_error(i8* %0, i8* nonnull %6) #11
  tail call void @halide_free(i8* %0, i8* nonnull %6) #11
  br label %41

; <label>:41:                                     ; preds = %.split6, %.split4
  ret i32 -8
}

; Function Attrs: nounwind
define weak i32 @halide_error_param_too_small_i64(i8*, i8*, i64, i64) local_unnamed_addr #0 {
  %5 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %6 = ptrtoint i8* %5 to i32
  %7 = icmp eq i8* %5, null
  br i1 %7, label %.split, label %.split2

.split:                                           ; preds = %4
  %8 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.71, i32 0, i32 0)) #11
  br label %11

.split2:                                          ; preds = %4
  %9 = getelementptr inbounds i8, i8* %5, i32 1023
  store i8 0, i8* %9, align 1, !tbaa !17
  %10 = tail call i8* @halide_string_to_string(i8* nonnull %5, i8* nonnull %9, i8* nonnull getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.71, i32 0, i32 0)) #11
  br label %11

; <label>:11:                                     ; preds = %.split, %.split2
  %phi.call = phi i8* [ %8, %.split ], [ %10, %.split2 ]
  %12 = phi i8* [ null, %.split ], [ %9, %.split2 ]
  %13 = icmp eq i8* %1, null
  br i1 %13, label %14, label %16

; <label>:14:                                     ; preds = %11
  %15 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %12, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %18

; <label>:16:                                     ; preds = %11
  %17 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %12, i8* nonnull %1) #11
  br label %18

; <label>:18:                                     ; preds = %16, %14
  %19 = phi i8* [ %17, %16 ], [ %15, %14 ]
  %20 = tail call i8* @halide_string_to_string(i8* %19, i8* %12, i8* nonnull getelementptr inbounds ([5 x i8], [5 x i8]* @.str.19.56, i32 0, i32 0)) #11
  %21 = tail call i8* @halide_int64_to_string(i8* %20, i8* %12, i64 %2, i32 1) #11
  %22 = tail call i8* @halide_string_to_string(i8* %21, i8* %12, i8* nonnull getelementptr inbounds ([23 x i8], [23 x i8]* @.str.35.72, i32 0, i32 0)) #11
  %23 = tail call i8* @halide_int64_to_string(i8* %22, i8* %12, i64 %3, i32 1) #11
  br i1 %7, label %.split4, label %.split6

.split4:                                          ; preds = %18
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %29

.split6:                                          ; preds = %18
  %24 = ptrtoint i8* %23 to i32
  %25 = sub i32 1, %6
  %26 = add i32 %25, %24
  %27 = sext i32 %26 to i64
  %28 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %5, i64 %27) #11
  tail call void @halide_error(i8* %0, i8* nonnull %5) #11
  tail call void @halide_free(i8* %0, i8* nonnull %5) #11
  br label %29

; <label>:29:                                     ; preds = %.split6, %.split4
  ret i32 -9
}

; Function Attrs: nounwind
define weak i32 @halide_error_param_too_small_u64(i8*, i8*, i64, i64) local_unnamed_addr #0 {
  %5 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %6 = ptrtoint i8* %5 to i32
  %7 = icmp eq i8* %5, null
  br i1 %7, label %.split, label %.split2

.split:                                           ; preds = %4
  %8 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.71, i32 0, i32 0)) #11
  br label %11

.split2:                                          ; preds = %4
  %9 = getelementptr inbounds i8, i8* %5, i32 1023
  store i8 0, i8* %9, align 1, !tbaa !17
  %10 = tail call i8* @halide_string_to_string(i8* nonnull %5, i8* nonnull %9, i8* nonnull getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.71, i32 0, i32 0)) #11
  br label %11

; <label>:11:                                     ; preds = %.split, %.split2
  %phi.call = phi i8* [ %8, %.split ], [ %10, %.split2 ]
  %12 = phi i8* [ null, %.split ], [ %9, %.split2 ]
  %13 = icmp eq i8* %1, null
  br i1 %13, label %14, label %16

; <label>:14:                                     ; preds = %11
  %15 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %12, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %18

; <label>:16:                                     ; preds = %11
  %17 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %12, i8* nonnull %1) #11
  br label %18

; <label>:18:                                     ; preds = %16, %14
  %19 = phi i8* [ %17, %16 ], [ %15, %14 ]
  %20 = tail call i8* @halide_string_to_string(i8* %19, i8* %12, i8* nonnull getelementptr inbounds ([5 x i8], [5 x i8]* @.str.19.56, i32 0, i32 0)) #11
  %21 = tail call i8* @halide_uint64_to_string(i8* %20, i8* %12, i64 %2, i32 1) #11
  %22 = tail call i8* @halide_string_to_string(i8* %21, i8* %12, i8* nonnull getelementptr inbounds ([23 x i8], [23 x i8]* @.str.35.72, i32 0, i32 0)) #11
  %23 = tail call i8* @halide_uint64_to_string(i8* %22, i8* %12, i64 %3, i32 1) #11
  br i1 %7, label %.split4, label %.split6

.split4:                                          ; preds = %18
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %29

.split6:                                          ; preds = %18
  %24 = ptrtoint i8* %23 to i32
  %25 = sub i32 1, %6
  %26 = add i32 %25, %24
  %27 = sext i32 %26 to i64
  %28 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %5, i64 %27) #11
  tail call void @halide_error(i8* %0, i8* nonnull %5) #11
  tail call void @halide_free(i8* %0, i8* nonnull %5) #11
  br label %29

; <label>:29:                                     ; preds = %.split6, %.split4
  ret i32 -9
}

; Function Attrs: nounwind
define weak i32 @halide_error_param_too_small_f64(i8*, i8*, double, double) local_unnamed_addr #0 {
  %5 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %6 = ptrtoint i8* %5 to i32
  %7 = icmp eq i8* %5, null
  br i1 %7, label %.split, label %.split2

.split:                                           ; preds = %4
  %8 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.71, i32 0, i32 0)) #11
  br label %11

.split2:                                          ; preds = %4
  %9 = getelementptr inbounds i8, i8* %5, i32 1023
  store i8 0, i8* %9, align 1, !tbaa !17
  %10 = tail call i8* @halide_string_to_string(i8* nonnull %5, i8* nonnull %9, i8* nonnull getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.71, i32 0, i32 0)) #11
  br label %11

; <label>:11:                                     ; preds = %.split, %.split2
  %phi.call = phi i8* [ %8, %.split ], [ %10, %.split2 ]
  %12 = phi i8* [ null, %.split ], [ %9, %.split2 ]
  %13 = icmp eq i8* %1, null
  br i1 %13, label %14, label %16

; <label>:14:                                     ; preds = %11
  %15 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %12, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %18

; <label>:16:                                     ; preds = %11
  %17 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %12, i8* nonnull %1) #11
  br label %18

; <label>:18:                                     ; preds = %16, %14
  %19 = phi i8* [ %17, %16 ], [ %15, %14 ]
  %20 = tail call i8* @halide_string_to_string(i8* %19, i8* %12, i8* nonnull getelementptr inbounds ([5 x i8], [5 x i8]* @.str.19.56, i32 0, i32 0)) #11
  %21 = tail call i8* @halide_double_to_string(i8* %20, i8* %12, double %2, i32 1) #11
  %22 = tail call i8* @halide_string_to_string(i8* %21, i8* %12, i8* nonnull getelementptr inbounds ([23 x i8], [23 x i8]* @.str.35.72, i32 0, i32 0)) #11
  %23 = tail call i8* @halide_double_to_string(i8* %22, i8* %12, double %3, i32 1) #11
  br i1 %7, label %.split4, label %.split6

.split4:                                          ; preds = %18
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %29

.split6:                                          ; preds = %18
  %24 = ptrtoint i8* %23 to i32
  %25 = sub i32 1, %6
  %26 = add i32 %25, %24
  %27 = sext i32 %26 to i64
  %28 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %5, i64 %27) #11
  tail call void @halide_error(i8* %0, i8* nonnull %5) #11
  tail call void @halide_free(i8* %0, i8* nonnull %5) #11
  br label %29

; <label>:29:                                     ; preds = %.split6, %.split4
  ret i32 -9
}

; Function Attrs: nounwind
define weak i32 @halide_error_param_too_large_i64(i8*, i8*, i64, i64) local_unnamed_addr #0 {
  %5 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %6 = ptrtoint i8* %5 to i32
  %7 = icmp eq i8* %5, null
  br i1 %7, label %.split, label %.split2

.split:                                           ; preds = %4
  %8 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.71, i32 0, i32 0)) #11
  br label %11

.split2:                                          ; preds = %4
  %9 = getelementptr inbounds i8, i8* %5, i32 1023
  store i8 0, i8* %9, align 1, !tbaa !17
  %10 = tail call i8* @halide_string_to_string(i8* nonnull %5, i8* nonnull %9, i8* nonnull getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.71, i32 0, i32 0)) #11
  br label %11

; <label>:11:                                     ; preds = %.split, %.split2
  %phi.call = phi i8* [ %8, %.split ], [ %10, %.split2 ]
  %12 = phi i8* [ null, %.split ], [ %9, %.split2 ]
  %13 = icmp eq i8* %1, null
  br i1 %13, label %14, label %16

; <label>:14:                                     ; preds = %11
  %15 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %12, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %18

; <label>:16:                                     ; preds = %11
  %17 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %12, i8* nonnull %1) #11
  br label %18

; <label>:18:                                     ; preds = %16, %14
  %19 = phi i8* [ %17, %16 ], [ %15, %14 ]
  %20 = tail call i8* @halide_string_to_string(i8* %19, i8* %12, i8* nonnull getelementptr inbounds ([5 x i8], [5 x i8]* @.str.19.56, i32 0, i32 0)) #11
  %21 = tail call i8* @halide_int64_to_string(i8* %20, i8* %12, i64 %2, i32 1) #11
  %22 = tail call i8* @halide_string_to_string(i8* %21, i8* %12, i8* nonnull getelementptr inbounds ([22 x i8], [22 x i8]* @.str.36.73, i32 0, i32 0)) #11
  %23 = tail call i8* @halide_int64_to_string(i8* %22, i8* %12, i64 %3, i32 1) #11
  br i1 %7, label %.split4, label %.split6

.split4:                                          ; preds = %18
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %29

.split6:                                          ; preds = %18
  %24 = ptrtoint i8* %23 to i32
  %25 = sub i32 1, %6
  %26 = add i32 %25, %24
  %27 = sext i32 %26 to i64
  %28 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %5, i64 %27) #11
  tail call void @halide_error(i8* %0, i8* nonnull %5) #11
  tail call void @halide_free(i8* %0, i8* nonnull %5) #11
  br label %29

; <label>:29:                                     ; preds = %.split6, %.split4
  ret i32 -10
}

; Function Attrs: nounwind
define weak i32 @halide_error_param_too_large_u64(i8*, i8*, i64, i64) local_unnamed_addr #0 {
  %5 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %6 = ptrtoint i8* %5 to i32
  %7 = icmp eq i8* %5, null
  br i1 %7, label %.split, label %.split2

.split:                                           ; preds = %4
  %8 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.71, i32 0, i32 0)) #11
  br label %11

.split2:                                          ; preds = %4
  %9 = getelementptr inbounds i8, i8* %5, i32 1023
  store i8 0, i8* %9, align 1, !tbaa !17
  %10 = tail call i8* @halide_string_to_string(i8* nonnull %5, i8* nonnull %9, i8* nonnull getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.71, i32 0, i32 0)) #11
  br label %11

; <label>:11:                                     ; preds = %.split, %.split2
  %phi.call = phi i8* [ %8, %.split ], [ %10, %.split2 ]
  %12 = phi i8* [ null, %.split ], [ %9, %.split2 ]
  %13 = icmp eq i8* %1, null
  br i1 %13, label %14, label %16

; <label>:14:                                     ; preds = %11
  %15 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %12, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %18

; <label>:16:                                     ; preds = %11
  %17 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %12, i8* nonnull %1) #11
  br label %18

; <label>:18:                                     ; preds = %16, %14
  %19 = phi i8* [ %17, %16 ], [ %15, %14 ]
  %20 = tail call i8* @halide_string_to_string(i8* %19, i8* %12, i8* nonnull getelementptr inbounds ([5 x i8], [5 x i8]* @.str.19.56, i32 0, i32 0)) #11
  %21 = tail call i8* @halide_uint64_to_string(i8* %20, i8* %12, i64 %2, i32 1) #11
  %22 = tail call i8* @halide_string_to_string(i8* %21, i8* %12, i8* nonnull getelementptr inbounds ([22 x i8], [22 x i8]* @.str.36.73, i32 0, i32 0)) #11
  %23 = tail call i8* @halide_uint64_to_string(i8* %22, i8* %12, i64 %3, i32 1) #11
  br i1 %7, label %.split4, label %.split6

.split4:                                          ; preds = %18
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %29

.split6:                                          ; preds = %18
  %24 = ptrtoint i8* %23 to i32
  %25 = sub i32 1, %6
  %26 = add i32 %25, %24
  %27 = sext i32 %26 to i64
  %28 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %5, i64 %27) #11
  tail call void @halide_error(i8* %0, i8* nonnull %5) #11
  tail call void @halide_free(i8* %0, i8* nonnull %5) #11
  br label %29

; <label>:29:                                     ; preds = %.split6, %.split4
  ret i32 -10
}

; Function Attrs: nounwind
define weak i32 @halide_error_param_too_large_f64(i8*, i8*, double, double) local_unnamed_addr #0 {
  %5 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %6 = ptrtoint i8* %5 to i32
  %7 = icmp eq i8* %5, null
  br i1 %7, label %.split, label %.split2

.split:                                           ; preds = %4
  %8 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.71, i32 0, i32 0)) #11
  br label %11

.split2:                                          ; preds = %4
  %9 = getelementptr inbounds i8, i8* %5, i32 1023
  store i8 0, i8* %9, align 1, !tbaa !17
  %10 = tail call i8* @halide_string_to_string(i8* nonnull %5, i8* nonnull %9, i8* nonnull getelementptr inbounds ([11 x i8], [11 x i8]* @.str.34.71, i32 0, i32 0)) #11
  br label %11

; <label>:11:                                     ; preds = %.split, %.split2
  %phi.call = phi i8* [ %8, %.split ], [ %10, %.split2 ]
  %12 = phi i8* [ null, %.split ], [ %9, %.split2 ]
  %13 = icmp eq i8* %1, null
  br i1 %13, label %14, label %16

; <label>:14:                                     ; preds = %11
  %15 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %12, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %18

; <label>:16:                                     ; preds = %11
  %17 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %12, i8* nonnull %1) #11
  br label %18

; <label>:18:                                     ; preds = %16, %14
  %19 = phi i8* [ %17, %16 ], [ %15, %14 ]
  %20 = tail call i8* @halide_string_to_string(i8* %19, i8* %12, i8* nonnull getelementptr inbounds ([5 x i8], [5 x i8]* @.str.19.56, i32 0, i32 0)) #11
  %21 = tail call i8* @halide_double_to_string(i8* %20, i8* %12, double %2, i32 1) #11
  %22 = tail call i8* @halide_string_to_string(i8* %21, i8* %12, i8* nonnull getelementptr inbounds ([22 x i8], [22 x i8]* @.str.36.73, i32 0, i32 0)) #11
  %23 = tail call i8* @halide_double_to_string(i8* %22, i8* %12, double %3, i32 1) #11
  br i1 %7, label %.split4, label %.split6

.split4:                                          ; preds = %18
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %29

.split6:                                          ; preds = %18
  %24 = ptrtoint i8* %23 to i32
  %25 = sub i32 1, %6
  %26 = add i32 %25, %24
  %27 = sext i32 %26 to i64
  %28 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %5, i64 %27) #11
  tail call void @halide_error(i8* %0, i8* nonnull %5) #11
  tail call void @halide_free(i8* %0, i8* nonnull %5) #11
  br label %29

; <label>:29:                                     ; preds = %.split6, %.split4
  ret i32 -10
}

; Function Attrs: nounwind
define weak i32 @halide_error_out_of_memory(i8*) local_unnamed_addr #0 {
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([44 x i8], [44 x i8]* @.str.37.74, i32 0, i32 0)) #11
  ret i32 -11
}

; Function Attrs: nounwind
define weak i32 @halide_error_buffer_argument_is_null(i8*, i8*) local_unnamed_addr #0 {
  %3 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %4 = ptrtoint i8* %3 to i32
  %5 = icmp eq i8* %3, null
  br i1 %5, label %.split, label %.split2

.split:                                           ; preds = %2
  %6 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([17 x i8], [17 x i8]* @.str.38, i32 0, i32 0)) #11
  br label %9

.split2:                                          ; preds = %2
  %7 = getelementptr inbounds i8, i8* %3, i32 1023
  store i8 0, i8* %7, align 1, !tbaa !17
  %8 = tail call i8* @halide_string_to_string(i8* nonnull %3, i8* nonnull %7, i8* nonnull getelementptr inbounds ([17 x i8], [17 x i8]* @.str.38, i32 0, i32 0)) #11
  br label %9

; <label>:9:                                      ; preds = %.split, %.split2
  %phi.call = phi i8* [ %6, %.split ], [ %8, %.split2 ]
  %10 = phi i8* [ null, %.split ], [ %7, %.split2 ]
  %11 = icmp eq i8* %1, null
  br i1 %11, label %12, label %14

; <label>:12:                                     ; preds = %9
  %13 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %10, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %16

; <label>:14:                                     ; preds = %9
  %15 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %10, i8* nonnull %1) #11
  br label %16

; <label>:16:                                     ; preds = %14, %12
  %17 = phi i8* [ %15, %14 ], [ %13, %12 ]
  %18 = tail call i8* @halide_string_to_string(i8* %17, i8* %10, i8* nonnull getelementptr inbounds ([9 x i8], [9 x i8]* @.str.39, i32 0, i32 0)) #11
  br i1 %5, label %.split4, label %.split6

.split4:                                          ; preds = %16
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %24

.split6:                                          ; preds = %16
  %19 = ptrtoint i8* %18 to i32
  %20 = sub i32 1, %4
  %21 = add i32 %20, %19
  %22 = sext i32 %21 to i64
  %23 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %3, i64 %22) #11
  tail call void @halide_error(i8* %0, i8* nonnull %3) #11
  tail call void @halide_free(i8* %0, i8* nonnull %3) #11
  br label %24

; <label>:24:                                     ; preds = %.split6, %.split4
  ret i32 -12
}

; Function Attrs: nounwind
define weak i32 @halide_error_debug_to_file_failed(i8*, i8*, i8*, i32) local_unnamed_addr #0 {
  %5 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %6 = ptrtoint i8* %5 to i32
  %7 = icmp eq i8* %5, null
  br i1 %7, label %.split, label %.split2

.split:                                           ; preds = %4
  %8 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([25 x i8], [25 x i8]* @.str.40, i32 0, i32 0)) #11
  br label %11

.split2:                                          ; preds = %4
  %9 = getelementptr inbounds i8, i8* %5, i32 1023
  store i8 0, i8* %9, align 1, !tbaa !17
  %10 = tail call i8* @halide_string_to_string(i8* nonnull %5, i8* nonnull %9, i8* nonnull getelementptr inbounds ([25 x i8], [25 x i8]* @.str.40, i32 0, i32 0)) #11
  br label %11

; <label>:11:                                     ; preds = %.split, %.split2
  %phi.call = phi i8* [ %8, %.split ], [ %10, %.split2 ]
  %12 = phi i8* [ null, %.split ], [ %9, %.split2 ]
  %13 = icmp eq i8* %1, null
  br i1 %13, label %14, label %16

; <label>:14:                                     ; preds = %11
  %15 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %12, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %18

; <label>:16:                                     ; preds = %11
  %17 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %12, i8* nonnull %1) #11
  br label %18

; <label>:18:                                     ; preds = %16, %14
  %19 = phi i8* [ %17, %16 ], [ %15, %14 ]
  %20 = tail call i8* @halide_string_to_string(i8* %19, i8* %12, i8* nonnull getelementptr inbounds ([10 x i8], [10 x i8]* @.str.41, i32 0, i32 0)) #11
  %21 = icmp eq i8* %2, null
  br i1 %21, label %22, label %24

; <label>:22:                                     ; preds = %18
  %23 = tail call i8* @halide_string_to_string(i8* %20, i8* %12, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %26

; <label>:24:                                     ; preds = %18
  %25 = tail call i8* @halide_string_to_string(i8* %20, i8* %12, i8* nonnull %2) #11
  br label %26

; <label>:26:                                     ; preds = %24, %22
  %27 = phi i8* [ %25, %24 ], [ %23, %22 ]
  %28 = tail call i8* @halide_string_to_string(i8* %27, i8* %12, i8* nonnull getelementptr inbounds ([13 x i8], [13 x i8]* @.str.42.75, i32 0, i32 0)) #11
  %29 = sext i32 %3 to i64
  %30 = tail call i8* @halide_int64_to_string(i8* %28, i8* %12, i64 %29, i32 1) #11
  br i1 %7, label %.split4, label %.split6

.split4:                                          ; preds = %26
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %36

.split6:                                          ; preds = %26
  %31 = ptrtoint i8* %30 to i32
  %32 = sub i32 1, %6
  %33 = add i32 %32, %31
  %34 = sext i32 %33 to i64
  %35 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %5, i64 %34) #11
  tail call void @halide_error(i8* %0, i8* nonnull %5) #11
  tail call void @halide_free(i8* %0, i8* nonnull %5) #11
  br label %36

; <label>:36:                                     ; preds = %.split6, %.split4
  ret i32 -13
}

; Function Attrs: nounwind
define weak i32 @halide_error_failed_to_upgrade_buffer_t(i8*, i8*, i8*) local_unnamed_addr #0 {
  %4 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %5 = ptrtoint i8* %4 to i32
  %6 = icmp eq i8* %4, null
  br i1 %6, label %.split, label %.split2

.split:                                           ; preds = %3
  %7 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([51 x i8], [51 x i8]* @.str.43, i32 0, i32 0)) #11
  br label %10

.split2:                                          ; preds = %3
  %8 = getelementptr inbounds i8, i8* %4, i32 1023
  store i8 0, i8* %8, align 1, !tbaa !17
  %9 = tail call i8* @halide_string_to_string(i8* nonnull %4, i8* nonnull %8, i8* nonnull getelementptr inbounds ([51 x i8], [51 x i8]* @.str.43, i32 0, i32 0)) #11
  br label %10

; <label>:10:                                     ; preds = %.split, %.split2
  %phi.call = phi i8* [ %7, %.split ], [ %9, %.split2 ]
  %11 = phi i8* [ null, %.split ], [ %8, %.split2 ]
  %12 = icmp eq i8* %1, null
  br i1 %12, label %13, label %15

; <label>:13:                                     ; preds = %10
  %14 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %11, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %17

; <label>:15:                                     ; preds = %10
  %16 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %11, i8* nonnull %1) #11
  br label %17

; <label>:17:                                     ; preds = %15, %13
  %18 = phi i8* [ %16, %15 ], [ %14, %13 ]
  %19 = tail call i8* @halide_string_to_string(i8* %18, i8* %11, i8* nonnull getelementptr inbounds ([3 x i8], [3 x i8]* @.str.44, i32 0, i32 0)) #11
  %20 = icmp eq i8* %2, null
  br i1 %20, label %21, label %23

; <label>:21:                                     ; preds = %17
  %22 = tail call i8* @halide_string_to_string(i8* %19, i8* %11, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %25

; <label>:23:                                     ; preds = %17
  %24 = tail call i8* @halide_string_to_string(i8* %19, i8* %11, i8* nonnull %2) #11
  br label %25

; <label>:25:                                     ; preds = %23, %21
  %26 = phi i8* [ %24, %23 ], [ %22, %21 ]
  br i1 %6, label %.split4, label %.split6

.split4:                                          ; preds = %25
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %32

.split6:                                          ; preds = %25
  %27 = ptrtoint i8* %26 to i32
  %28 = sub i32 1, %5
  %29 = add i32 %28, %27
  %30 = sext i32 %29 to i64
  %31 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %4, i64 %30) #11
  tail call void @halide_error(i8* %0, i8* nonnull %4) #11
  tail call void @halide_free(i8* %0, i8* nonnull %4) #11
  br label %32

; <label>:32:                                     ; preds = %.split6, %.split4
  ret i32 -29
}

; Function Attrs: nounwind
define weak i32 @halide_error_failed_to_downgrade_buffer_t(i8*, i8*, i8*) local_unnamed_addr #0 {
  %4 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %5 = ptrtoint i8* %4 to i32
  %6 = icmp eq i8* %4, null
  br i1 %6, label %.split, label %.split2

.split:                                           ; preds = %3
  %7 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([53 x i8], [53 x i8]* @.str.45, i32 0, i32 0)) #11
  br label %10

.split2:                                          ; preds = %3
  %8 = getelementptr inbounds i8, i8* %4, i32 1023
  store i8 0, i8* %8, align 1, !tbaa !17
  %9 = tail call i8* @halide_string_to_string(i8* nonnull %4, i8* nonnull %8, i8* nonnull getelementptr inbounds ([53 x i8], [53 x i8]* @.str.45, i32 0, i32 0)) #11
  br label %10

; <label>:10:                                     ; preds = %.split, %.split2
  %phi.call = phi i8* [ %7, %.split ], [ %9, %.split2 ]
  %11 = phi i8* [ null, %.split ], [ %8, %.split2 ]
  %12 = icmp eq i8* %1, null
  br i1 %12, label %13, label %15

; <label>:13:                                     ; preds = %10
  %14 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %11, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %17

; <label>:15:                                     ; preds = %10
  %16 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %11, i8* nonnull %1) #11
  br label %17

; <label>:17:                                     ; preds = %15, %13
  %18 = phi i8* [ %16, %15 ], [ %14, %13 ]
  %19 = tail call i8* @halide_string_to_string(i8* %18, i8* %11, i8* nonnull getelementptr inbounds ([3 x i8], [3 x i8]* @.str.44, i32 0, i32 0)) #11
  %20 = icmp eq i8* %2, null
  br i1 %20, label %21, label %23

; <label>:21:                                     ; preds = %17
  %22 = tail call i8* @halide_string_to_string(i8* %19, i8* %11, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %25

; <label>:23:                                     ; preds = %17
  %24 = tail call i8* @halide_string_to_string(i8* %19, i8* %11, i8* nonnull %2) #11
  br label %25

; <label>:25:                                     ; preds = %23, %21
  %26 = phi i8* [ %24, %23 ], [ %22, %21 ]
  br i1 %6, label %.split4, label %.split6

.split4:                                          ; preds = %25
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %32

.split6:                                          ; preds = %25
  %27 = ptrtoint i8* %26 to i32
  %28 = sub i32 1, %5
  %29 = add i32 %28, %27
  %30 = sext i32 %29 to i64
  %31 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %4, i64 %30) #11
  tail call void @halide_error(i8* %0, i8* nonnull %4) #11
  tail call void @halide_free(i8* %0, i8* nonnull %4) #11
  br label %32

; <label>:32:                                     ; preds = %.split6, %.split4
  ret i32 -30
}

; Function Attrs: nounwind
define weak i32 @halide_error_unaligned_host_ptr(i8*, i8*, i32) local_unnamed_addr #0 {
  %4 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %5 = ptrtoint i8* %4 to i32
  %6 = icmp eq i8* %4, null
  br i1 %6, label %.split, label %.split2

.split:                                           ; preds = %3
  %7 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([21 x i8], [21 x i8]* @.str.46, i32 0, i32 0)) #11
  br label %10

.split2:                                          ; preds = %3
  %8 = getelementptr inbounds i8, i8* %4, i32 1023
  store i8 0, i8* %8, align 1, !tbaa !17
  %9 = tail call i8* @halide_string_to_string(i8* nonnull %4, i8* nonnull %8, i8* nonnull getelementptr inbounds ([21 x i8], [21 x i8]* @.str.46, i32 0, i32 0)) #11
  br label %10

; <label>:10:                                     ; preds = %.split, %.split2
  %phi.call = phi i8* [ %7, %.split ], [ %9, %.split2 ]
  %11 = phi i8* [ null, %.split ], [ %8, %.split2 ]
  %12 = icmp eq i8* %1, null
  br i1 %12, label %13, label %15

; <label>:13:                                     ; preds = %10
  %14 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %11, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %17

; <label>:15:                                     ; preds = %10
  %16 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %11, i8* nonnull %1) #11
  br label %17

; <label>:17:                                     ; preds = %15, %13
  %18 = phi i8* [ %16, %15 ], [ %14, %13 ]
  %19 = tail call i8* @halide_string_to_string(i8* %18, i8* %11, i8* nonnull getelementptr inbounds ([22 x i8], [22 x i8]* @.str.47, i32 0, i32 0)) #11
  %20 = sext i32 %2 to i64
  %21 = tail call i8* @halide_int64_to_string(i8* %19, i8* %11, i64 %20, i32 1) #11
  %22 = tail call i8* @halide_string_to_string(i8* %21, i8* %11, i8* nonnull getelementptr inbounds ([17 x i8], [17 x i8]* @.str.48, i32 0, i32 0)) #11
  br i1 %6, label %.split4, label %.split6

.split4:                                          ; preds = %17
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %28

.split6:                                          ; preds = %17
  %23 = ptrtoint i8* %22 to i32
  %24 = sub i32 1, %5
  %25 = add i32 %24, %23
  %26 = sext i32 %25 to i64
  %27 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %4, i64 %26) #11
  tail call void @halide_error(i8* %0, i8* nonnull %4) #11
  tail call void @halide_free(i8* %0, i8* nonnull %4) #11
  br label %28

; <label>:28:                                     ; preds = %.split6, %.split4
  ret i32 -24
}

; Function Attrs: nounwind
define weak i32 @halide_error_host_is_null(i8*, i8*) local_unnamed_addr #0 {
  %3 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %4 = ptrtoint i8* %3 to i32
  %5 = icmp eq i8* %3, null
  br i1 %5, label %.split, label %.split2

.split:                                           ; preds = %2
  %6 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([21 x i8], [21 x i8]* @.str.46, i32 0, i32 0)) #11
  br label %9

.split2:                                          ; preds = %2
  %7 = getelementptr inbounds i8, i8* %3, i32 1023
  store i8 0, i8* %7, align 1, !tbaa !17
  %8 = tail call i8* @halide_string_to_string(i8* nonnull %3, i8* nonnull %7, i8* nonnull getelementptr inbounds ([21 x i8], [21 x i8]* @.str.46, i32 0, i32 0)) #11
  br label %9

; <label>:9:                                      ; preds = %.split, %.split2
  %phi.call = phi i8* [ %6, %.split ], [ %8, %.split2 ]
  %10 = phi i8* [ null, %.split ], [ %7, %.split2 ]
  %11 = icmp eq i8* %1, null
  br i1 %11, label %12, label %14

; <label>:12:                                     ; preds = %9
  %13 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %10, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %16

; <label>:14:                                     ; preds = %9
  %15 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %10, i8* nonnull %1) #11
  br label %16

; <label>:16:                                     ; preds = %14, %12
  %17 = phi i8* [ %15, %14 ], [ %13, %12 ]
  %18 = tail call i8* @halide_string_to_string(i8* %17, i8* %10, i8* nonnull getelementptr inbounds ([55 x i8], [55 x i8]* @.str.49, i32 0, i32 0)) #11
  br i1 %5, label %.split4, label %.split6

.split4:                                          ; preds = %16
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %24

.split6:                                          ; preds = %16
  %19 = ptrtoint i8* %18 to i32
  %20 = sub i32 1, %4
  %21 = add i32 %20, %19
  %22 = sext i32 %21 to i64
  %23 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %3, i64 %22) #11
  tail call void @halide_error(i8* %0, i8* nonnull %3) #11
  tail call void @halide_free(i8* %0, i8* nonnull %3) #11
  br label %24

; <label>:24:                                     ; preds = %.split6, %.split4
  ret i32 -34
}

; Function Attrs: nounwind
define weak i32 @halide_error_bad_fold(i8*, i8*, i8*, i8*) local_unnamed_addr #0 {
  %5 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %6 = ptrtoint i8* %5 to i32
  %7 = icmp eq i8* %5, null
  br i1 %7, label %.split, label %.split2

.split:                                           ; preds = %4
  %8 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([30 x i8], [30 x i8]* @.str.50, i32 0, i32 0)) #11
  br label %11

.split2:                                          ; preds = %4
  %9 = getelementptr inbounds i8, i8* %5, i32 1023
  store i8 0, i8* %9, align 1, !tbaa !17
  %10 = tail call i8* @halide_string_to_string(i8* nonnull %5, i8* nonnull %9, i8* nonnull getelementptr inbounds ([30 x i8], [30 x i8]* @.str.50, i32 0, i32 0)) #11
  br label %11

; <label>:11:                                     ; preds = %.split, %.split2
  %phi.call = phi i8* [ %8, %.split ], [ %10, %.split2 ]
  %12 = phi i8* [ null, %.split ], [ %9, %.split2 ]
  %13 = icmp eq i8* %2, null
  br i1 %13, label %14, label %16

; <label>:14:                                     ; preds = %11
  %15 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %12, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %18

; <label>:16:                                     ; preds = %11
  %17 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %12, i8* nonnull %2) #11
  br label %18

; <label>:18:                                     ; preds = %16, %14
  %19 = phi i8* [ %17, %16 ], [ %15, %14 ]
  %20 = tail call i8* @halide_string_to_string(i8* %19, i8* %12, i8* nonnull getelementptr inbounds ([5 x i8], [5 x i8]* @.str.51, i32 0, i32 0)) #11
  %21 = icmp eq i8* %1, null
  br i1 %21, label %22, label %24

; <label>:22:                                     ; preds = %18
  %23 = tail call i8* @halide_string_to_string(i8* %20, i8* %12, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %26

; <label>:24:                                     ; preds = %18
  %25 = tail call i8* @halide_string_to_string(i8* %20, i8* %12, i8* nonnull %1) #11
  br label %26

; <label>:26:                                     ; preds = %24, %22
  %27 = phi i8* [ %25, %24 ], [ %23, %22 ]
  %28 = tail call i8* @halide_string_to_string(i8* %27, i8* %12, i8* nonnull getelementptr inbounds ([36 x i8], [36 x i8]* @.str.52, i32 0, i32 0)) #11
  %29 = icmp eq i8* %3, null
  br i1 %29, label %30, label %32

; <label>:30:                                     ; preds = %26
  %31 = tail call i8* @halide_string_to_string(i8* %28, i8* %12, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %34

; <label>:32:                                     ; preds = %26
  %33 = tail call i8* @halide_string_to_string(i8* %28, i8* %12, i8* nonnull %3) #11
  br label %34

; <label>:34:                                     ; preds = %32, %30
  %35 = phi i8* [ %33, %32 ], [ %31, %30 ]
  %36 = tail call i8* @halide_string_to_string(i8* %35, i8* %12, i8* nonnull getelementptr inbounds ([2 x i8], [2 x i8]* @.str.30.67, i32 0, i32 0)) #11
  br i1 %7, label %.split4, label %.split6

.split4:                                          ; preds = %34
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %42

.split6:                                          ; preds = %34
  %37 = ptrtoint i8* %36 to i32
  %38 = sub i32 1, %6
  %39 = add i32 %38, %37
  %40 = sext i32 %39 to i64
  %41 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %5, i64 %40) #11
  tail call void @halide_error(i8* %0, i8* nonnull %5) #11
  tail call void @halide_free(i8* %0, i8* nonnull %5) #11
  br label %42

; <label>:42:                                     ; preds = %.split6, %.split4
  ret i32 -25
}

; Function Attrs: nounwind
define weak i32 @halide_error_bad_extern_fold(i8*, i8*, i32, i32, i32, i32, i32) local_unnamed_addr #0 {
  %8 = icmp slt i32 %3, %5
  br i1 %8, label %13, label %9

; <label>:9:                                      ; preds = %7
  %10 = add nsw i32 %4, %3
  %11 = add nsw i32 %6, %5
  %12 = icmp sgt i32 %10, %11
  br i1 %12, label %13, label %55

; <label>:13:                                     ; preds = %9, %7
  %14 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %15 = ptrtoint i8* %14 to i32
  %16 = icmp eq i8* %14, null
  br i1 %16, label %.split, label %.split2

.split:                                           ; preds = %13
  %17 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([23 x i8], [23 x i8]* @.str.53, i32 0, i32 0)) #11
  br label %20

.split2:                                          ; preds = %13
  %18 = getelementptr inbounds i8, i8* %14, i32 1023
  store i8 0, i8* %18, align 1, !tbaa !17
  %19 = tail call i8* @halide_string_to_string(i8* nonnull %14, i8* nonnull %18, i8* nonnull getelementptr inbounds ([23 x i8], [23 x i8]* @.str.53, i32 0, i32 0)) #11
  br label %20

; <label>:20:                                     ; preds = %.split, %.split2
  %phi.call = phi i8* [ %17, %.split ], [ %19, %.split2 ]
  %21 = phi i8* [ null, %.split ], [ %18, %.split2 ]
  %22 = sext i32 %2 to i64
  %23 = tail call i8* @halide_int64_to_string(i8* %phi.call, i8* %21, i64 %22, i32 1) #11
  %24 = tail call i8* @halide_string_to_string(i8* %23, i8* %21, i8* nonnull getelementptr inbounds ([5 x i8], [5 x i8]* @.str.51, i32 0, i32 0)) #11
  %25 = icmp eq i8* %1, null
  br i1 %25, label %26, label %28

; <label>:26:                                     ; preds = %20
  %27 = tail call i8* @halide_string_to_string(i8* %24, i8* %21, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %30

; <label>:28:                                     ; preds = %20
  %29 = tail call i8* @halide_string_to_string(i8* %24, i8* %21, i8* nonnull %1) #11
  br label %30

; <label>:30:                                     ; preds = %28, %26
  %31 = phi i8* [ %29, %28 ], [ %27, %26 ]
  %32 = tail call i8* @halide_string_to_string(i8* %31, i8* %21, i8* nonnull getelementptr inbounds ([36 x i8], [36 x i8]* @.str.54, i32 0, i32 0)) #11
  %33 = sext i32 %3 to i64
  %34 = tail call i8* @halide_int64_to_string(i8* %32, i8* %21, i64 %33, i32 1) #11
  %35 = tail call i8* @halide_string_to_string(i8* %34, i8* %21, i8* nonnull getelementptr inbounds ([3 x i8], [3 x i8]* @.str.55, i32 0, i32 0)) #11
  %36 = add nsw i32 %4, %3
  %37 = add nsw i32 %36, -1
  %38 = sext i32 %37 to i64
  %39 = tail call i8* @halide_int64_to_string(i8* %35, i8* %21, i64 %38, i32 1) #11
  %40 = tail call i8* @halide_string_to_string(i8* %39, i8* %21, i8* nonnull getelementptr inbounds ([3 x i8], [3 x i8]* @.str.56, i32 0, i32 0)) #11
  %41 = tail call i8* @halide_string_to_string(i8* %40, i8* %21, i8* nonnull getelementptr inbounds ([47 x i8], [47 x i8]* @.str.57, i32 0, i32 0)) #11
  %42 = sext i32 %5 to i64
  %43 = tail call i8* @halide_int64_to_string(i8* %41, i8* %21, i64 %42, i32 1) #11
  %44 = tail call i8* @halide_string_to_string(i8* %43, i8* %21, i8* nonnull getelementptr inbounds ([3 x i8], [3 x i8]* @.str.55, i32 0, i32 0)) #11
  %45 = add nsw i32 %6, %5
  %46 = add nsw i32 %45, -1
  %47 = sext i32 %46 to i64
  %48 = tail call i8* @halide_int64_to_string(i8* %44, i8* %21, i64 %47, i32 1) #11
  %49 = tail call i8* @halide_string_to_string(i8* %48, i8* %21, i8* nonnull getelementptr inbounds ([3 x i8], [3 x i8]* @.str.58.76, i32 0, i32 0)) #11
  br i1 %16, label %.split4, label %.split6

.split4:                                          ; preds = %30
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %92

.split6:                                          ; preds = %30
  %50 = ptrtoint i8* %49 to i32
  %51 = sub i32 1, %15
  %52 = add i32 %51, %50
  %53 = sext i32 %52 to i64
  %54 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %14, i64 %53) #11
  tail call void @halide_error(i8* %0, i8* nonnull %14) #11
  tail call void @halide_free(i8* %0, i8* nonnull %14) #11
  br label %92

; <label>:55:                                     ; preds = %9
  %56 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %57 = ptrtoint i8* %56 to i32
  %58 = icmp eq i8* %56, null
  br i1 %58, label %.split7, label %.split9

.split7:                                          ; preds = %55
  %59 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([23 x i8], [23 x i8]* @.str.53, i32 0, i32 0)) #11
  br label %62

.split9:                                          ; preds = %55
  %60 = getelementptr inbounds i8, i8* %56, i32 1023
  store i8 0, i8* %60, align 1, !tbaa !17
  %61 = tail call i8* @halide_string_to_string(i8* nonnull %56, i8* nonnull %60, i8* nonnull getelementptr inbounds ([23 x i8], [23 x i8]* @.str.53, i32 0, i32 0)) #11
  br label %62

; <label>:62:                                     ; preds = %.split7, %.split9
  %phi.call10 = phi i8* [ %59, %.split7 ], [ %61, %.split9 ]
  %63 = phi i8* [ null, %.split7 ], [ %60, %.split9 ]
  %64 = sext i32 %2 to i64
  %65 = tail call i8* @halide_int64_to_string(i8* %phi.call10, i8* %63, i64 %64, i32 1) #11
  %66 = tail call i8* @halide_string_to_string(i8* %65, i8* %63, i8* nonnull getelementptr inbounds ([5 x i8], [5 x i8]* @.str.51, i32 0, i32 0)) #11
  %67 = icmp eq i8* %1, null
  br i1 %67, label %68, label %70

; <label>:68:                                     ; preds = %62
  %69 = tail call i8* @halide_string_to_string(i8* %66, i8* %63, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %72

; <label>:70:                                     ; preds = %62
  %71 = tail call i8* @halide_string_to_string(i8* %66, i8* %63, i8* nonnull %1) #11
  br label %72

; <label>:72:                                     ; preds = %70, %68
  %73 = phi i8* [ %71, %70 ], [ %69, %68 ]
  %74 = tail call i8* @halide_string_to_string(i8* %73, i8* %63, i8* nonnull getelementptr inbounds ([36 x i8], [36 x i8]* @.str.54, i32 0, i32 0)) #11
  %75 = sext i32 %3 to i64
  %76 = tail call i8* @halide_int64_to_string(i8* %74, i8* %63, i64 %75, i32 1) #11
  %77 = tail call i8* @halide_string_to_string(i8* %76, i8* %63, i8* nonnull getelementptr inbounds ([3 x i8], [3 x i8]* @.str.55, i32 0, i32 0)) #11
  %78 = add nsw i32 %10, -1
  %79 = sext i32 %78 to i64
  %80 = tail call i8* @halide_int64_to_string(i8* %77, i8* %63, i64 %79, i32 1) #11
  %81 = tail call i8* @halide_string_to_string(i8* %80, i8* %63, i8* nonnull getelementptr inbounds ([3 x i8], [3 x i8]* @.str.56, i32 0, i32 0)) #11
  %82 = tail call i8* @halide_string_to_string(i8* %81, i8* %63, i8* nonnull getelementptr inbounds ([47 x i8], [47 x i8]* @.str.59.77, i32 0, i32 0)) #11
  %83 = tail call i8* @halide_string_to_string(i8* %82, i8* %63, i8* nonnull getelementptr inbounds ([30 x i8], [30 x i8]* @.str.60.78, i32 0, i32 0)) #11
  %84 = sext i32 %6 to i64
  %85 = tail call i8* @halide_int64_to_string(i8* %83, i8* %63, i64 %84, i32 1) #11
  %86 = tail call i8* @halide_string_to_string(i8* %85, i8* %63, i8* nonnull getelementptr inbounds ([2 x i8], [2 x i8]* @.str.30.67, i32 0, i32 0)) #11
  br i1 %58, label %.split12, label %.split14

.split12:                                         ; preds = %72
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %92

.split14:                                         ; preds = %72
  %87 = ptrtoint i8* %86 to i32
  %88 = sub i32 1, %57
  %89 = add i32 %88, %87
  %90 = sext i32 %89 to i64
  %91 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %56, i64 %90) #11
  tail call void @halide_error(i8* %0, i8* nonnull %56) #11
  tail call void @halide_free(i8* %0, i8* nonnull %56) #11
  br label %92

; <label>:92:                                     ; preds = %.split12, %.split14, %.split4, %.split6
  ret i32 -35
}

; Function Attrs: nounwind
define weak i32 @halide_error_fold_factor_too_small(i8*, i8*, i8*, i32, i8*, i32) local_unnamed_addr #0 {
  %7 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %8 = ptrtoint i8* %7 to i32
  %9 = icmp eq i8* %7, null
  br i1 %9, label %.split, label %.split2

.split:                                           ; preds = %6
  %10 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([18 x i8], [18 x i8]* @.str.61.79, i32 0, i32 0)) #11
  br label %13

.split2:                                          ; preds = %6
  %11 = getelementptr inbounds i8, i8* %7, i32 1023
  store i8 0, i8* %11, align 1, !tbaa !17
  %12 = tail call i8* @halide_string_to_string(i8* nonnull %7, i8* nonnull %11, i8* nonnull getelementptr inbounds ([18 x i8], [18 x i8]* @.str.61.79, i32 0, i32 0)) #11
  br label %13

; <label>:13:                                     ; preds = %.split, %.split2
  %phi.call = phi i8* [ %10, %.split ], [ %12, %.split2 ]
  %14 = phi i8* [ null, %.split ], [ %11, %.split2 ]
  %15 = sext i32 %3 to i64
  %16 = tail call i8* @halide_int64_to_string(i8* %phi.call, i8* %14, i64 %15, i32 1) #11
  %17 = tail call i8* @halide_string_to_string(i8* %16, i8* %14, i8* nonnull getelementptr inbounds ([16 x i8], [16 x i8]* @.str.62, i32 0, i32 0)) #11
  %18 = icmp eq i8* %2, null
  br i1 %18, label %19, label %21

; <label>:19:                                     ; preds = %13
  %20 = tail call i8* @halide_string_to_string(i8* %17, i8* %14, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %23

; <label>:21:                                     ; preds = %13
  %22 = tail call i8* @halide_string_to_string(i8* %17, i8* %14, i8* nonnull %2) #11
  br label %23

; <label>:23:                                     ; preds = %21, %19
  %24 = phi i8* [ %22, %21 ], [ %20, %19 ]
  %25 = tail call i8* @halide_string_to_string(i8* %24, i8* %14, i8* nonnull getelementptr inbounds ([5 x i8], [5 x i8]* @.str.51, i32 0, i32 0)) #11
  %26 = icmp eq i8* %1, null
  br i1 %26, label %27, label %29

; <label>:27:                                     ; preds = %23
  %28 = tail call i8* @halide_string_to_string(i8* %25, i8* %14, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %31

; <label>:29:                                     ; preds = %23
  %30 = tail call i8* @halide_string_to_string(i8* %25, i8* %14, i8* nonnull %1) #11
  br label %31

; <label>:31:                                     ; preds = %29, %27
  %32 = phi i8* [ %30, %29 ], [ %28, %27 ]
  %33 = tail call i8* @halide_string_to_string(i8* %32, i8* %14, i8* nonnull getelementptr inbounds ([61 x i8], [61 x i8]* @.str.63, i32 0, i32 0)) #11
  %34 = icmp eq i8* %4, null
  br i1 %34, label %35, label %37

; <label>:35:                                     ; preds = %31
  %36 = tail call i8* @halide_string_to_string(i8* %33, i8* %14, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %39

; <label>:37:                                     ; preds = %31
  %38 = tail call i8* @halide_string_to_string(i8* %33, i8* %14, i8* nonnull %4) #11
  br label %39

; <label>:39:                                     ; preds = %37, %35
  %40 = phi i8* [ %38, %37 ], [ %36, %35 ]
  %41 = tail call i8* @halide_string_to_string(i8* %40, i8* %14, i8* nonnull getelementptr inbounds ([3 x i8], [3 x i8]* @.str.32.69, i32 0, i32 0)) #11
  %42 = sext i32 %5 to i64
  %43 = tail call i8* @halide_int64_to_string(i8* %41, i8* %14, i64 %42, i32 1) #11
  %44 = tail call i8* @halide_string_to_string(i8* %43, i8* %14, i8* nonnull getelementptr inbounds ([3 x i8], [3 x i8]* @.str.64.80, i32 0, i32 0)) #11
  br i1 %9, label %.split4, label %.split6

.split4:                                          ; preds = %39
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %50

.split6:                                          ; preds = %39
  %45 = ptrtoint i8* %44 to i32
  %46 = sub i32 1, %8
  %47 = add i32 %46, %45
  %48 = sext i32 %47 to i64
  %49 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %7, i64 %48) #11
  tail call void @halide_error(i8* %0, i8* nonnull %7) #11
  tail call void @halide_free(i8* %0, i8* nonnull %7) #11
  br label %50

; <label>:50:                                     ; preds = %.split6, %.split4
  ret i32 -26
}

; Function Attrs: nounwind
define weak i32 @halide_error_requirement_failed(i8*, i8*, i8*) local_unnamed_addr #0 {
  %4 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %5 = ptrtoint i8* %4 to i32
  %6 = icmp eq i8* %4, null
  br i1 %6, label %.split, label %.split2

.split:                                           ; preds = %3
  %7 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([22 x i8], [22 x i8]* @.str.65, i32 0, i32 0)) #11
  br label %10

.split2:                                          ; preds = %3
  %8 = getelementptr inbounds i8, i8* %4, i32 1023
  store i8 0, i8* %8, align 1, !tbaa !17
  %9 = tail call i8* @halide_string_to_string(i8* nonnull %4, i8* nonnull %8, i8* nonnull getelementptr inbounds ([22 x i8], [22 x i8]* @.str.65, i32 0, i32 0)) #11
  br label %10

; <label>:10:                                     ; preds = %.split, %.split2
  %phi.call = phi i8* [ %7, %.split ], [ %9, %.split2 ]
  %11 = phi i8* [ null, %.split ], [ %8, %.split2 ]
  %12 = icmp eq i8* %1, null
  br i1 %12, label %13, label %15

; <label>:13:                                     ; preds = %10
  %14 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %11, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %17

; <label>:15:                                     ; preds = %10
  %16 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %11, i8* nonnull %1) #11
  br label %17

; <label>:17:                                     ; preds = %15, %13
  %18 = phi i8* [ %16, %15 ], [ %14, %13 ]
  %19 = tail call i8* @halide_string_to_string(i8* %18, i8* %11, i8* nonnull getelementptr inbounds ([3 x i8], [3 x i8]* @.str.3.92, i32 0, i32 0)) #11
  %20 = icmp eq i8* %2, null
  br i1 %20, label %21, label %23

; <label>:21:                                     ; preds = %17
  %22 = tail call i8* @halide_string_to_string(i8* %19, i8* %11, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %25

; <label>:23:                                     ; preds = %17
  %24 = tail call i8* @halide_string_to_string(i8* %19, i8* %11, i8* nonnull %2) #11
  br label %25

; <label>:25:                                     ; preds = %23, %21
  %26 = phi i8* [ %24, %23 ], [ %22, %21 ]
  br i1 %6, label %.split4, label %.split6

.split4:                                          ; preds = %25
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %32

.split6:                                          ; preds = %25
  %27 = ptrtoint i8* %26 to i32
  %28 = sub i32 1, %5
  %29 = add i32 %28, %27
  %30 = sext i32 %29 to i64
  %31 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %4, i64 %30) #11
  tail call void @halide_error(i8* %0, i8* nonnull %4) #11
  tail call void @halide_free(i8* %0, i8* nonnull %4) #11
  br label %32

; <label>:32:                                     ; preds = %.split6, %.split4
  ret i32 -27
}

; Function Attrs: nounwind
define weak i32 @halide_error_specialize_fail(i8*, i8*) local_unnamed_addr #0 {
  %3 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %4 = ptrtoint i8* %3 to i32
  %5 = icmp eq i8* %3, null
  br i1 %5, label %.split, label %.split2

.split:                                           ; preds = %2
  %6 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([59 x i8], [59 x i8]* @.str.67, i32 0, i32 0)) #11
  br label %9

.split2:                                          ; preds = %2
  %7 = getelementptr inbounds i8, i8* %3, i32 1023
  store i8 0, i8* %7, align 1, !tbaa !17
  %8 = tail call i8* @halide_string_to_string(i8* nonnull %3, i8* nonnull %7, i8* nonnull getelementptr inbounds ([59 x i8], [59 x i8]* @.str.67, i32 0, i32 0)) #11
  br label %9

; <label>:9:                                      ; preds = %.split, %.split2
  %phi.call = phi i8* [ %6, %.split ], [ %8, %.split2 ]
  %10 = phi i8* [ null, %.split ], [ %7, %.split2 ]
  %11 = icmp eq i8* %1, null
  br i1 %11, label %12, label %14

; <label>:12:                                     ; preds = %9
  %13 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %10, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %16

; <label>:14:                                     ; preds = %9
  %15 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %10, i8* nonnull %1) #11
  br label %16

; <label>:16:                                     ; preds = %14, %12
  %17 = phi i8* [ %15, %14 ], [ %13, %12 ]
  br i1 %5, label %.split4, label %.split6

.split4:                                          ; preds = %16
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %23

.split6:                                          ; preds = %16
  %18 = ptrtoint i8* %17 to i32
  %19 = sub i32 1, %4
  %20 = add i32 %19, %18
  %21 = sext i32 %20 to i64
  %22 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %3, i64 %21) #11
  tail call void @halide_error(i8* %0, i8* nonnull %3) #11
  tail call void @halide_free(i8* %0, i8* nonnull %3) #11
  br label %23

; <label>:23:                                     ; preds = %.split6, %.split4
  ret i32 -31
}

; Function Attrs: nounwind
define weak i32 @halide_error_no_device_interface(i8*) local_unnamed_addr #0 {
  %2 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %3 = icmp eq i8* %2, null
  br i1 %3, label %.split, label %.split2

.split:                                           ; preds = %1
  %4 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([55 x i8], [55 x i8]* @.str.68, i32 0, i32 0)) #11
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %13

.split2:                                          ; preds = %1
  %5 = ptrtoint i8* %2 to i32
  %6 = getelementptr inbounds i8, i8* %2, i32 1023
  store i8 0, i8* %6, align 1, !tbaa !17
  %7 = tail call i8* @halide_string_to_string(i8* nonnull %2, i8* nonnull %6, i8* nonnull getelementptr inbounds ([55 x i8], [55 x i8]* @.str.68, i32 0, i32 0)) #11
  %8 = ptrtoint i8* %7 to i32
  %9 = sub i32 1, %5
  %10 = add i32 %9, %8
  %11 = sext i32 %10 to i64
  %12 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %2, i64 %11) #11
  tail call void @halide_error(i8* %0, i8* nonnull %2) #11
  tail call void @halide_free(i8* %0, i8* nonnull %2) #11
  br label %13

; <label>:13:                                     ; preds = %.split2, %.split
  ret i32 -19
}

; Function Attrs: nounwind
define weak i32 @halide_error_device_interface_no_device(i8*) local_unnamed_addr #0 {
  %2 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %3 = icmp eq i8* %2, null
  br i1 %3, label %.split, label %.split2

.split:                                           ; preds = %1
  %4 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([56 x i8], [56 x i8]* @.str.69, i32 0, i32 0)) #11
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %13

.split2:                                          ; preds = %1
  %5 = ptrtoint i8* %2 to i32
  %6 = getelementptr inbounds i8, i8* %2, i32 1023
  store i8 0, i8* %6, align 1, !tbaa !17
  %7 = tail call i8* @halide_string_to_string(i8* nonnull %2, i8* nonnull %6, i8* nonnull getelementptr inbounds ([56 x i8], [56 x i8]* @.str.69, i32 0, i32 0)) #11
  %8 = ptrtoint i8* %7 to i32
  %9 = sub i32 1, %5
  %10 = add i32 %9, %8
  %11 = sext i32 %10 to i64
  %12 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %2, i64 %11) #11
  tail call void @halide_error(i8* %0, i8* nonnull %2) #11
  tail call void @halide_free(i8* %0, i8* nonnull %2) #11
  br label %13

; <label>:13:                                     ; preds = %.split2, %.split
  ret i32 -36
}

; Function Attrs: nounwind
define weak i32 @halide_error_host_and_device_dirty(i8*) local_unnamed_addr #0 {
  %2 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %3 = icmp eq i8* %2, null
  br i1 %3, label %.split, label %.split2

.split:                                           ; preds = %1
  %4 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([49 x i8], [49 x i8]* @.str.70, i32 0, i32 0)) #11
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %13

.split2:                                          ; preds = %1
  %5 = ptrtoint i8* %2 to i32
  %6 = getelementptr inbounds i8, i8* %2, i32 1023
  store i8 0, i8* %6, align 1, !tbaa !17
  %7 = tail call i8* @halide_string_to_string(i8* nonnull %2, i8* nonnull %6, i8* nonnull getelementptr inbounds ([49 x i8], [49 x i8]* @.str.70, i32 0, i32 0)) #11
  %8 = ptrtoint i8* %7 to i32
  %9 = sub i32 1, %5
  %10 = add i32 %9, %8
  %11 = sext i32 %10 to i64
  %12 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %2, i64 %11) #11
  tail call void @halide_error(i8* %0, i8* nonnull %2) #11
  tail call void @halide_free(i8* %0, i8* nonnull %2) #11
  br label %13

; <label>:13:                                     ; preds = %.split2, %.split
  ret i32 -37
}

; Function Attrs: nounwind
define weak i32 @halide_error_buffer_is_null(i8*, i8*) local_unnamed_addr #0 {
  %3 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %4 = ptrtoint i8* %3 to i32
  %5 = icmp eq i8* %3, null
  br i1 %5, label %.split, label %.split2

.split:                                           ; preds = %2
  %6 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([26 x i8], [26 x i8]* @.str.71, i32 0, i32 0)) #11
  br label %9

.split2:                                          ; preds = %2
  %7 = getelementptr inbounds i8, i8* %3, i32 1023
  store i8 0, i8* %7, align 1, !tbaa !17
  %8 = tail call i8* @halide_string_to_string(i8* nonnull %3, i8* nonnull %7, i8* nonnull getelementptr inbounds ([26 x i8], [26 x i8]* @.str.71, i32 0, i32 0)) #11
  br label %9

; <label>:9:                                      ; preds = %.split, %.split2
  %phi.call = phi i8* [ %6, %.split ], [ %8, %.split2 ]
  %10 = phi i8* [ null, %.split ], [ %7, %.split2 ]
  %11 = icmp eq i8* %1, null
  br i1 %11, label %12, label %14

; <label>:12:                                     ; preds = %9
  %13 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %10, i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str.75, i32 0, i32 0)) #11
  br label %16

; <label>:14:                                     ; preds = %9
  %15 = tail call i8* @halide_string_to_string(i8* %phi.call, i8* %10, i8* nonnull %1) #11
  br label %16

; <label>:16:                                     ; preds = %14, %12
  %17 = phi i8* [ %15, %14 ], [ %13, %12 ]
  %18 = tail call i8* @halide_string_to_string(i8* %17, i8* %10, i8* nonnull getelementptr inbounds ([11 x i8], [11 x i8]* @.str.72, i32 0, i32 0)) #11
  br i1 %5, label %.split4, label %.split6

.split4:                                          ; preds = %16
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %24

.split6:                                          ; preds = %16
  %19 = ptrtoint i8* %18 to i32
  %20 = sub i32 1, %4
  %21 = add i32 %20, %19
  %22 = sext i32 %21 to i64
  %23 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %3, i64 %22) #11
  tail call void @halide_error(i8* %0, i8* nonnull %3) #11
  tail call void @halide_free(i8* %0, i8* nonnull %3) #11
  br label %24

; <label>:24:                                     ; preds = %.split6, %.split4
  ret i32 -38
}

; Function Attrs: nounwind
define weak i32 @halide_error_integer_division_by_zero(i8*) local_unnamed_addr #0 {
  %2 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %3 = icmp eq i8* %2, null
  br i1 %3, label %.split, label %.split2

.split:                                           ; preds = %1
  %4 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([37 x i8], [37 x i8]* @.str.73, i32 0, i32 0)) #11
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %13

.split2:                                          ; preds = %1
  %5 = ptrtoint i8* %2 to i32
  %6 = getelementptr inbounds i8, i8* %2, i32 1023
  store i8 0, i8* %6, align 1, !tbaa !17
  %7 = tail call i8* @halide_string_to_string(i8* nonnull %2, i8* nonnull %6, i8* nonnull getelementptr inbounds ([37 x i8], [37 x i8]* @.str.73, i32 0, i32 0)) #11
  %8 = ptrtoint i8* %7 to i32
  %9 = sub i32 1, %5
  %10 = add i32 %9, %8
  %11 = sext i32 %10 to i64
  %12 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %2, i64 %11) #11
  tail call void @halide_error(i8* %0, i8* nonnull %2) #11
  tail call void @halide_free(i8* %0, i8* nonnull %2) #11
  br label %13

; <label>:13:                                     ; preds = %.split2, %.split
  ret i32 -44
}

; Function Attrs: nounwind
define linkonce i32 @_ZN6Halide7Runtime8Internal29guess_type_and_dimensionalityEPvP8buffer_tP15halide_buffer_t(i8*, %struct.buffer_t*, %struct.halide_buffer_t*) local_unnamed_addr #0 {
  %4 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %2, i32 0, i32 5
  store i32 4, i32* %4, align 4, !tbaa !34
  %5 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %1, i32 0, i32 2, i32 0
  %6 = load i32, i32* %5, align 4, !tbaa !12
  %7 = icmp eq i32 %6, 0
  br i1 %7, label %8, label %10

; <label>:8:                                      ; preds = %37, %33, %10, %3
  %9 = phi i32 [ 0, %3 ], [ 1, %10 ], [ 2, %33 ], [ 3, %37 ]
  store i32 %9, i32* %4, align 4, !tbaa !34
  br label %14

; <label>:10:                                     ; preds = %3
  %11 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %1, i32 0, i32 2, i32 1
  %12 = load i32, i32* %11, align 4, !tbaa !12
  %13 = icmp eq i32 %12, 0
  br i1 %13, label %8, label %33

; <label>:14:                                     ; preds = %37, %8
  %15 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %1, i32 0, i32 5
  %16 = load i32, i32* %15, align 4, !tbaa !73
  switch i32 %16, label %29 [
    i32 1, label %17
    i32 2, label %20
    i32 4, label %23
    i32 8, label %26
  ]

; <label>:17:                                     ; preds = %14
  %18 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %2, i32 0, i32 4, i32 0
  %19 = bitcast i8* %18 to i32*
  store i32 67585, i32* %19, align 8
  br label %31

; <label>:20:                                     ; preds = %14
  %21 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %2, i32 0, i32 4, i32 0
  %22 = bitcast i8* %21 to i32*
  store i32 69633, i32* %22, align 8
  br label %31

; <label>:23:                                     ; preds = %14
  %24 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %2, i32 0, i32 4, i32 0
  %25 = bitcast i8* %24 to i32*
  store i32 73729, i32* %25, align 8
  br label %31

; <label>:26:                                     ; preds = %14
  %27 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %2, i32 0, i32 4, i32 0
  %28 = bitcast i8* %27 to i32*
  store i32 81921, i32* %28, align 8
  br label %31

; <label>:29:                                     ; preds = %14
  %30 = tail call i32 @halide_error_failed_to_upgrade_buffer_t(i8* %0, i8* getelementptr inbounds ([1 x i8], [1 x i8]* @.str.89, i32 0, i32 0), i8* getelementptr inbounds ([44 x i8], [44 x i8]* @.str.1.90, i32 0, i32 0)) #11
  br label %31

; <label>:31:                                     ; preds = %29, %26, %23, %20, %17
  %32 = phi i32 [ %30, %29 ], [ 0, %26 ], [ 0, %23 ], [ 0, %20 ], [ 0, %17 ]
  ret i32 %32

; <label>:33:                                     ; preds = %10
  %34 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %1, i32 0, i32 2, i32 2
  %35 = load i32, i32* %34, align 4, !tbaa !12
  %36 = icmp eq i32 %35, 0
  br i1 %36, label %8, label %37

; <label>:37:                                     ; preds = %33
  %38 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %1, i32 0, i32 2, i32 3
  %39 = load i32, i32* %38, align 4, !tbaa !12
  %40 = icmp eq i32 %39, 0
  br i1 %40, label %8, label %14
}

; Function Attrs: nounwind
define weak i32 @halide_upgrade_buffer_t(i8*, i8*, %struct.buffer_t*, %struct.halide_buffer_t*, i32) local_unnamed_addr #0 {
  %6 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 1
  %7 = load i8*, i8** %6, align 8, !tbaa !75
  %8 = icmp eq i8* %7, null
  br i1 %8, label %9, label %15

; <label>:9:                                      ; preds = %5
  %10 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 0
  %11 = load i64, i64* %10, align 8, !tbaa !76
  %12 = icmp eq i64 %11, 0
  br i1 %12, label %13, label %15

; <label>:13:                                     ; preds = %9
  %14 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 2
  store i8* null, i8** %14, align 4, !tbaa !32
  br label %69

; <label>:15:                                     ; preds = %9, %5
  %16 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 5
  %17 = load i32, i32* %16, align 4, !tbaa !73
  %18 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 4
  %19 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 4, i32 1
  %20 = load i8, i8* %19, align 1, !tbaa !27
  %21 = zext i8 %20 to i32
  %22 = add nuw nsw i32 %21, 7
  %23 = lshr i32 %22, 3
  %24 = icmp eq i32 %17, %23
  br i1 %24, label %49, label %25

; <label>:25:                                     ; preds = %15
  %26 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %27 = ptrtoint i8* %26 to i32
  %28 = icmp eq i8* %26, null
  br i1 %28, label %.split, label %.split2

.split:                                           ; preds = %25
  %29 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([33 x i8], [33 x i8]* @.str.2.91, i32 0, i32 0)) #11
  br label %32

.split2:                                          ; preds = %25
  %30 = getelementptr inbounds i8, i8* %26, i32 1023
  store i8 0, i8* %30, align 1, !tbaa !17
  %31 = tail call i8* @halide_string_to_string(i8* nonnull %26, i8* nonnull %30, i8* nonnull getelementptr inbounds ([33 x i8], [33 x i8]* @.str.2.91, i32 0, i32 0)) #11
  br label %32

; <label>:32:                                     ; preds = %.split, %.split2
  %phi.call = phi i8* [ %29, %.split ], [ %31, %.split2 ]
  %33 = phi i8* [ null, %.split ], [ %30, %.split2 ]
  %34 = load i32, i32* %16, align 4, !tbaa !73
  %35 = sext i32 %34 to i64
  %36 = tail call i8* @halide_int64_to_string(i8* %phi.call, i8* %33, i64 %35, i32 1) #11
  %37 = tail call i8* @halide_string_to_string(i8* %36, i8* %33, i8* nonnull getelementptr inbounds ([3 x i8], [3 x i8]* @.str.3.92, i32 0, i32 0)) #11
  %38 = tail call i8* @halide_string_to_string(i8* %37, i8* %33, i8* nonnull getelementptr inbounds ([20 x i8], [20 x i8]* @.str.4.93, i32 0, i32 0)) #11
  %39 = tail call i8* @halide_type_to_string(i8* %38, i8* %33, %struct.halide_type_t* nonnull %18) #11
  %40 = tail call i8* @halide_string_to_string(i8* %39, i8* %33, i8* nonnull getelementptr inbounds ([2 x i8], [2 x i8]* @.str.5.94, i32 0, i32 0)) #11
  br i1 %28, label %.split4, label %.split6

.split4:                                          ; preds = %32
  %41 = tail call i32 @halide_error_failed_to_upgrade_buffer_t(i8* %0, i8* %1, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %184

.split6:                                          ; preds = %32
  %42 = ptrtoint i8* %40 to i32
  %43 = sub i32 1, %27
  %44 = add i32 %43, %42
  %45 = sext i32 %44 to i64
  %46 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %26, i64 %45) #11
  %47 = tail call i32 @halide_error_failed_to_upgrade_buffer_t(i8* %0, i8* %1, i8* nonnull %26) #11
  %48 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %26, i64 %45) #11
  tail call void @halide_free(i8* %0, i8* nonnull %26) #11
  br label %184

; <label>:49:                                     ; preds = %15
  %50 = icmp eq i32 %4, 0
  br i1 %50, label %57, label %51

; <label>:51:                                     ; preds = %49
  %52 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 2
  %53 = load i8*, i8** %52, align 4, !tbaa !32
  %54 = icmp eq i8* %53, %7
  br i1 %54, label %184, label %55

; <label>:55:                                     ; preds = %51
  %56 = tail call i32 @halide_error_failed_to_upgrade_buffer_t(i8* %0, i8* %1, i8* getelementptr inbounds ([65 x i8], [65 x i8]* @.str.6.96, i32 0, i32 0)) #11
  br label %184

; <label>:57:                                     ; preds = %49
  %58 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 0
  %59 = load i64, i64* %58, align 8, !tbaa !76
  %60 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 2
  store i8* %7, i8** %60, align 4, !tbaa !32
  %61 = icmp eq i64 %59, 0
  br i1 %61, label %69, label %62

; <label>:62:                                     ; preds = %57
  %63 = trunc i64 %59 to i32
  %64 = inttoptr i32 %63 to %"struct.Halide::Runtime::Internal::old_dev_wrapper"*
  %65 = getelementptr inbounds %"struct.Halide::Runtime::Internal::old_dev_wrapper", %"struct.Halide::Runtime::Internal::old_dev_wrapper"* %64, i32 0, i32 0
  %66 = load i64, i64* %65, align 8, !tbaa !77
  %67 = getelementptr inbounds %"struct.Halide::Runtime::Internal::old_dev_wrapper", %"struct.Halide::Runtime::Internal::old_dev_wrapper"* %64, i32 0, i32 1
  %68 = load %struct.halide_device_interface_t*, %struct.halide_device_interface_t** %67, align 8, !tbaa !79
  br label %69

; <label>:69:                                     ; preds = %62, %57, %13
  %70 = phi i64 [ %66, %62 ], [ 0, %13 ], [ 0, %57 ]
  %71 = phi %struct.halide_device_interface_t* [ %68, %62 ], [ null, %13 ], [ null, %57 ]
  %72 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 0
  store i64 %70, i64* %72, align 8
  %73 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 1
  store %struct.halide_device_interface_t* %71, %struct.halide_device_interface_t** %73, align 8
  %74 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 5
  %75 = load i32, i32* %74, align 4, !tbaa !34
  %76 = icmp sgt i32 %75, 0
  br i1 %76, label %77, label %.loopexit

; <label>:77:                                     ; preds = %69
  %78 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 6
  %79 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %78, align 8, !tbaa !35
  %80 = add i32 %75, -1
  %xtraiter = and i32 %75, 7
  %81 = icmp ult i32 %80, 7
  br i1 %81, label %.loopexit.loopexit.unr-lcssa, label %.new

.new:                                             ; preds = %77
  %unroll_iter = sub i32 %75, %xtraiter
  br label %102

.loopexit.loopexit.unr-lcssa:                     ; preds = %102, %77
  %.unr = phi i32 [ 0, %77 ], [ %183, %102 ]
  %lcmp.mod = icmp eq i32 %xtraiter, 0
  br i1 %lcmp.mod, label %.loopexit, label %.epil.preheader

.epil.preheader:                                  ; preds = %.loopexit.loopexit.unr-lcssa, %.epil.preheader
  %82 = phi i32 [ %92, %.epil.preheader ], [ %.unr, %.loopexit.loopexit.unr-lcssa ]
  %epil.iter = phi i32 [ %epil.iter.sub, %.epil.preheader ], [ %xtraiter, %.loopexit.loopexit.unr-lcssa ]
  %83 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 4, i32 %82
  %84 = load i32, i32* %83, align 4, !tbaa !12
  %85 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %82, i32 0
  store i32 %84, i32* %85, align 4, !tbaa !36
  %86 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 2, i32 %82
  %87 = load i32, i32* %86, align 4, !tbaa !12
  %88 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %82, i32 1
  store i32 %87, i32* %88, align 4, !tbaa !38
  %89 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 3, i32 %82
  %90 = load i32, i32* %89, align 4, !tbaa !12
  %91 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %82, i32 2
  store i32 %90, i32* %91, align 4, !tbaa !39
  %92 = add nuw nsw i32 %82, 1
  %epil.iter.sub = add i32 %epil.iter, -1
  %epil.iter.cmp = icmp eq i32 %epil.iter.sub, 0
  br i1 %epil.iter.cmp, label %.loopexit, label %.epil.preheader, !llvm.loop !80

.loopexit:                                        ; preds = %.loopexit.loopexit.unr-lcssa, %.epil.preheader, %69
  %93 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 3
  %94 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 6
  %95 = load i8, i8* %94, align 8, !tbaa !81, !range !16
  %96 = zext i8 %95 to i64
  %97 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 7
  %98 = load i8, i8* %97, align 1, !tbaa !82, !range !16
  %99 = icmp eq i8 %98, 0
  %100 = or i64 %96, 2
  %101 = select i1 %99, i64 %96, i64 %100
  store i64 %101, i64* %93, align 8, !tbaa !33
  br label %184

; <label>:102:                                    ; preds = %102, %.new
  %103 = phi i32 [ 0, %.new ], [ %183, %102 ]
  %niter = phi i32 [ %unroll_iter, %.new ], [ %niter.nsub.7, %102 ]
  %104 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 4, i32 %103
  %105 = load i32, i32* %104, align 4, !tbaa !12
  %106 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %103, i32 0
  store i32 %105, i32* %106, align 4, !tbaa !36
  %107 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 2, i32 %103
  %108 = load i32, i32* %107, align 4, !tbaa !12
  %109 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %103, i32 1
  store i32 %108, i32* %109, align 4, !tbaa !38
  %110 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 3, i32 %103
  %111 = load i32, i32* %110, align 4, !tbaa !12
  %112 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %103, i32 2
  store i32 %111, i32* %112, align 4, !tbaa !39
  %113 = or i32 %103, 1
  %114 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 4, i32 %113
  %115 = load i32, i32* %114, align 4, !tbaa !12
  %116 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %113, i32 0
  store i32 %115, i32* %116, align 4, !tbaa !36
  %117 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 2, i32 %113
  %118 = load i32, i32* %117, align 4, !tbaa !12
  %119 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %113, i32 1
  store i32 %118, i32* %119, align 4, !tbaa !38
  %120 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 3, i32 %113
  %121 = load i32, i32* %120, align 4, !tbaa !12
  %122 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %113, i32 2
  store i32 %121, i32* %122, align 4, !tbaa !39
  %123 = or i32 %103, 2
  %124 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 4, i32 %123
  %125 = load i32, i32* %124, align 4, !tbaa !12
  %126 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %123, i32 0
  store i32 %125, i32* %126, align 4, !tbaa !36
  %127 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 2, i32 %123
  %128 = load i32, i32* %127, align 4, !tbaa !12
  %129 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %123, i32 1
  store i32 %128, i32* %129, align 4, !tbaa !38
  %130 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 3, i32 %123
  %131 = load i32, i32* %130, align 4, !tbaa !12
  %132 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %123, i32 2
  store i32 %131, i32* %132, align 4, !tbaa !39
  %133 = or i32 %103, 3
  %134 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 4, i32 %133
  %135 = load i32, i32* %134, align 4, !tbaa !12
  %136 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %133, i32 0
  store i32 %135, i32* %136, align 4, !tbaa !36
  %137 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 2, i32 %133
  %138 = load i32, i32* %137, align 4, !tbaa !12
  %139 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %133, i32 1
  store i32 %138, i32* %139, align 4, !tbaa !38
  %140 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 3, i32 %133
  %141 = load i32, i32* %140, align 4, !tbaa !12
  %142 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %133, i32 2
  store i32 %141, i32* %142, align 4, !tbaa !39
  %143 = or i32 %103, 4
  %144 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 4, i32 %143
  %145 = load i32, i32* %144, align 4, !tbaa !12
  %146 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %143, i32 0
  store i32 %145, i32* %146, align 4, !tbaa !36
  %147 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 2, i32 %143
  %148 = load i32, i32* %147, align 4, !tbaa !12
  %149 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %143, i32 1
  store i32 %148, i32* %149, align 4, !tbaa !38
  %150 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 3, i32 %143
  %151 = load i32, i32* %150, align 4, !tbaa !12
  %152 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %143, i32 2
  store i32 %151, i32* %152, align 4, !tbaa !39
  %153 = or i32 %103, 5
  %154 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 4, i32 %153
  %155 = load i32, i32* %154, align 4, !tbaa !12
  %156 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %153, i32 0
  store i32 %155, i32* %156, align 4, !tbaa !36
  %157 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 2, i32 %153
  %158 = load i32, i32* %157, align 4, !tbaa !12
  %159 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %153, i32 1
  store i32 %158, i32* %159, align 4, !tbaa !38
  %160 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 3, i32 %153
  %161 = load i32, i32* %160, align 4, !tbaa !12
  %162 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %153, i32 2
  store i32 %161, i32* %162, align 4, !tbaa !39
  %163 = or i32 %103, 6
  %164 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 4, i32 %163
  %165 = load i32, i32* %164, align 4, !tbaa !12
  %166 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %163, i32 0
  store i32 %165, i32* %166, align 4, !tbaa !36
  %167 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 2, i32 %163
  %168 = load i32, i32* %167, align 4, !tbaa !12
  %169 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %163, i32 1
  store i32 %168, i32* %169, align 4, !tbaa !38
  %170 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 3, i32 %163
  %171 = load i32, i32* %170, align 4, !tbaa !12
  %172 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %163, i32 2
  store i32 %171, i32* %172, align 4, !tbaa !39
  %173 = or i32 %103, 7
  %174 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 4, i32 %173
  %175 = load i32, i32* %174, align 4, !tbaa !12
  %176 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %173, i32 0
  store i32 %175, i32* %176, align 4, !tbaa !36
  %177 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 2, i32 %173
  %178 = load i32, i32* %177, align 4, !tbaa !12
  %179 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %173, i32 1
  store i32 %178, i32* %179, align 4, !tbaa !38
  %180 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %2, i32 0, i32 3, i32 %173
  %181 = load i32, i32* %180, align 4, !tbaa !12
  %182 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %79, i32 %173, i32 2
  store i32 %181, i32* %182, align 4, !tbaa !39
  %183 = add nuw nsw i32 %103, 8
  %niter.nsub.7 = add i32 %niter, -8
  %niter.ncmp.7 = icmp eq i32 %niter.nsub.7, 0
  br i1 %niter.ncmp.7, label %.loopexit.loopexit.unr-lcssa, label %102

; <label>:184:                                    ; preds = %.split4, %.split6, %.loopexit, %55, %51
  %185 = phi i32 [ %56, %55 ], [ 0, %.loopexit ], [ 0, %51 ], [ %47, %.split6 ], [ %41, %.split4 ]
  ret i32 %185
}

; Function Attrs: nounwind
define weak i32 @halide_downgrade_buffer_t(i8*, i8*, %struct.halide_buffer_t*, %struct.buffer_t*) local_unnamed_addr #0 {
  %5 = bitcast %struct.buffer_t* %3 to i8*
  %6 = tail call i8* @memset(i8* %5, i32 0, i32 72) #11
  %7 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %2, i32 0, i32 5
  %8 = load i32, i32* %7, align 4, !tbaa !34
  %9 = icmp sgt i32 %8, 4
  br i1 %9, label %10, label %12

; <label>:10:                                     ; preds = %4
  %11 = tail call i32 @halide_error_failed_to_downgrade_buffer_t(i8* %0, i8* %1, i8* getelementptr inbounds ([37 x i8], [37 x i8]* @.str.7.97, i32 0, i32 0)) #11
  br label %43

; <label>:12:                                     ; preds = %4
  %13 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %2, i32 0, i32 2
  %14 = bitcast i8** %13 to i32*
  %15 = load i32, i32* %14, align 4, !tbaa !32
  %16 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %3, i32 0, i32 1
  %17 = bitcast i8** %16 to i32*
  store i32 %15, i32* %17, align 8, !tbaa !75
  %18 = icmp sgt i32 %8, 0
  br i1 %18, label %19, label %.loopexit

; <label>:19:                                     ; preds = %12
  %20 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %2, i32 0, i32 6
  %21 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %20, align 8, !tbaa !35
  br label %29

.loopexit:                                        ; preds = %29, %12
  %22 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %2, i32 0, i32 4, i32 1
  %23 = load i8, i8* %22, align 1, !tbaa !27
  %24 = zext i8 %23 to i32
  %25 = add nuw nsw i32 %24, 7
  %26 = lshr i32 %25, 3
  %27 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %3, i32 0, i32 5
  store i32 %26, i32* %27, align 4, !tbaa !73
  %28 = tail call i32 @halide_downgrade_buffer_t_device_fields(i8* %0, i8* %1, %struct.halide_buffer_t* nonnull %2, %struct.buffer_t* nonnull %3) #12
  br label %43

; <label>:29:                                     ; preds = %29, %19
  %30 = phi i32 [ 0, %19 ], [ %40, %29 ]
  %31 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %21, i32 %30, i32 0
  %32 = load i32, i32* %31, align 4, !tbaa !36
  %33 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %3, i32 0, i32 4, i32 %30
  store i32 %32, i32* %33, align 4, !tbaa !12
  %34 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %21, i32 %30, i32 1
  %35 = load i32, i32* %34, align 4, !tbaa !38
  %36 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %3, i32 0, i32 2, i32 %30
  store i32 %35, i32* %36, align 4, !tbaa !12
  %37 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %21, i32 %30, i32 2
  %38 = load i32, i32* %37, align 4, !tbaa !39
  %39 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %3, i32 0, i32 3, i32 %30
  store i32 %38, i32* %39, align 4, !tbaa !12
  %40 = add nuw nsw i32 %30, 1
  %41 = load i32, i32* %7, align 4, !tbaa !34
  %42 = icmp slt i32 %40, %41
  br i1 %42, label %29, label %.loopexit

; <label>:43:                                     ; preds = %.loopexit, %10
  %44 = phi i32 [ %11, %10 ], [ %28, %.loopexit ]
  ret i32 %44
}

declare i8* @memset(i8*, i32, i32) local_unnamed_addr #2

; Function Attrs: nounwind
define weak i32 @halide_downgrade_buffer_t_device_fields(i8*, i8*, %struct.halide_buffer_t*, %struct.buffer_t*) local_unnamed_addr #0 {
  %5 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %2, i32 0, i32 3
  %6 = load i64, i64* %5, align 8, !tbaa !33
  %7 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %3, i32 0, i32 6
  %8 = trunc i64 %6 to i8
  %9 = and i8 %8, 1
  store i8 %9, i8* %7, align 8, !tbaa !81
  %10 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %3, i32 0, i32 7
  %11 = lshr i64 %6, 1
  %12 = trunc i64 %11 to i8
  %13 = and i8 %12, 1
  store i8 %13, i8* %10, align 1, !tbaa !82
  %14 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %2, i32 0, i32 0
  %15 = load i64, i64* %14, align 8, !tbaa !29
  %16 = icmp eq i64 %15, 0
  %17 = getelementptr inbounds %struct.buffer_t, %struct.buffer_t* %3, i32 0, i32 0
  %18 = load i64, i64* %17, align 8, !tbaa !76
  %19 = icmp ne i64 %18, 0
  br i1 %16, label %41, label %20

; <label>:20:                                     ; preds = %4
  br i1 %19, label %21, label %30

; <label>:21:                                     ; preds = %20
  %22 = trunc i64 %18 to i32
  %23 = inttoptr i32 %22 to %"struct.Halide::Runtime::Internal::old_dev_wrapper"*
  %24 = getelementptr inbounds %"struct.Halide::Runtime::Internal::old_dev_wrapper", %"struct.Halide::Runtime::Internal::old_dev_wrapper"* %23, i32 0, i32 0
  store i64 %15, i64* %24, align 8, !tbaa !77
  %25 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %2, i32 0, i32 1
  %26 = bitcast %struct.halide_device_interface_t** %25 to i32*
  %27 = load i32, i32* %26, align 8, !tbaa !31
  %28 = getelementptr inbounds %"struct.Halide::Runtime::Internal::old_dev_wrapper", %"struct.Halide::Runtime::Internal::old_dev_wrapper"* %23, i32 0, i32 1
  %29 = bitcast %struct.halide_device_interface_t** %28 to i32*
  store i32 %27, i32* %29, align 8, !tbaa !79
  br label %45

; <label>:30:                                     ; preds = %20
  %31 = tail call i8* @malloc(i32 16) #11
  %32 = load i64, i64* %14, align 8, !tbaa !29
  %33 = bitcast i8* %31 to i64*
  store i64 %32, i64* %33, align 8, !tbaa !77
  %34 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %2, i32 0, i32 1
  %35 = bitcast %struct.halide_device_interface_t** %34 to i32*
  %36 = load i32, i32* %35, align 8, !tbaa !31
  %37 = getelementptr inbounds i8, i8* %31, i32 8
  %38 = bitcast i8* %37 to i32*
  store i32 %36, i32* %38, align 8, !tbaa !79
  %39 = ptrtoint i8* %31 to i32
  %40 = zext i32 %39 to i64
  store i64 %40, i64* %17, align 8, !tbaa !76
  br label %45

; <label>:41:                                     ; preds = %4
  br i1 %19, label %42, label %45

; <label>:42:                                     ; preds = %41
  %43 = trunc i64 %18 to i32
  %44 = inttoptr i32 %43 to i8*
  tail call void @free(i8* %44) #11
  store i64 0, i64* %17, align 8, !tbaa !76
  br label %45

; <label>:45:                                     ; preds = %42, %41, %30, %21
  ret i32 0
}

; Function Attrs: nounwind
define weak i32 @halide_copy_to_host_legacy(i8*, %struct.buffer_t*) local_unnamed_addr #0 {
  %3 = alloca %struct.halide_buffer_t, align 8
  %4 = alloca [4 x %struct.halide_dimension_t], align 4
  %5 = bitcast %struct.halide_buffer_t* %3 to i8*
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %5) #9
  %6 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 6
  %7 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 7
  store i8* null, i8** %7, align 4, !tbaa !83
  %8 = bitcast [4 x %struct.halide_dimension_t]* %4 to i8*
  %9 = bitcast %struct.halide_buffer_t* %3 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %9, i8 0, i64 32, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %8) #9
  %10 = getelementptr inbounds [4 x %struct.halide_dimension_t], [4 x %struct.halide_dimension_t]* %4, i32 0, i32 0
  %11 = bitcast [4 x %struct.halide_dimension_t]* %4 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 4 %11, i8 0, i64 64, i1 false)
  store %struct.halide_dimension_t* %10, %struct.halide_dimension_t** %6, align 8, !tbaa !35
  %12 = call i32 @_ZN6Halide7Runtime8Internal29guess_type_and_dimensionalityEPvP8buffer_tP15halide_buffer_t(i8* %0, %struct.buffer_t* %1, %struct.halide_buffer_t* nonnull %3) #12
  %13 = icmp eq i32 %12, 0
  br i1 %13, label %14, label %24

; <label>:14:                                     ; preds = %2
  %15 = call i32 @halide_upgrade_buffer_t(i8* %0, i8* getelementptr inbounds ([1 x i8], [1 x i8]* @.str.89, i32 0, i32 0), %struct.buffer_t* %1, %struct.halide_buffer_t* nonnull %3, i32 0) #12
  %16 = icmp eq i32 %15, 0
  br i1 %16, label %17, label %24

; <label>:17:                                     ; preds = %14
  %18 = call i32 @halide_copy_to_host(i8* %0, %struct.halide_buffer_t* nonnull %3) #11
  %19 = icmp eq i32 %18, 0
  br i1 %19, label %20, label %24

; <label>:20:                                     ; preds = %17
  %21 = call i32 @halide_downgrade_buffer_t_device_fields(i8* %0, i8* getelementptr inbounds ([1 x i8], [1 x i8]* @.str.89, i32 0, i32 0), %struct.halide_buffer_t* nonnull %3, %struct.buffer_t* %1) #12
  %22 = icmp ne i32 %21, 0
  %23 = zext i1 %22 to i32
  br label %24

; <label>:24:                                     ; preds = %20, %17, %14, %2
  %25 = phi i32 [ 1, %17 ], [ %23, %20 ], [ 1, %14 ], [ 1, %2 ]
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %8) #9
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %5) #9
  ret i32 %25
}

; Function Attrs: nounwind
define weak i32 @halide_copy_to_device_legacy(i8*, %struct.buffer_t*, %struct.halide_device_interface_t*) local_unnamed_addr #0 {
  %4 = alloca %struct.halide_buffer_t, align 8
  %5 = alloca [4 x %struct.halide_dimension_t], align 4
  %6 = bitcast %struct.halide_buffer_t* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %6) #9
  %7 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %4, i32 0, i32 6
  %8 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %4, i32 0, i32 7
  store i8* null, i8** %8, align 4, !tbaa !83
  %9 = bitcast [4 x %struct.halide_dimension_t]* %5 to i8*
  %10 = bitcast %struct.halide_buffer_t* %4 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %10, i8 0, i64 32, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %9) #9
  %11 = getelementptr inbounds [4 x %struct.halide_dimension_t], [4 x %struct.halide_dimension_t]* %5, i32 0, i32 0
  %12 = bitcast [4 x %struct.halide_dimension_t]* %5 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 4 %12, i8 0, i64 64, i1 false)
  store %struct.halide_dimension_t* %11, %struct.halide_dimension_t** %7, align 8, !tbaa !35
  %13 = call i32 @_ZN6Halide7Runtime8Internal29guess_type_and_dimensionalityEPvP8buffer_tP15halide_buffer_t(i8* %0, %struct.buffer_t* %1, %struct.halide_buffer_t* nonnull %4) #12
  %14 = icmp eq i32 %13, 0
  br i1 %14, label %15, label %25

; <label>:15:                                     ; preds = %3
  %16 = call i32 @halide_upgrade_buffer_t(i8* %0, i8* getelementptr inbounds ([1 x i8], [1 x i8]* @.str.89, i32 0, i32 0), %struct.buffer_t* %1, %struct.halide_buffer_t* nonnull %4, i32 0) #12
  %17 = icmp eq i32 %16, 0
  br i1 %17, label %18, label %25

; <label>:18:                                     ; preds = %15
  %19 = call i32 @halide_copy_to_device(i8* %0, %struct.halide_buffer_t* nonnull %4, %struct.halide_device_interface_t* %2) #11
  %20 = icmp eq i32 %19, 0
  br i1 %20, label %21, label %25

; <label>:21:                                     ; preds = %18
  %22 = call i32 @halide_downgrade_buffer_t_device_fields(i8* %0, i8* getelementptr inbounds ([1 x i8], [1 x i8]* @.str.89, i32 0, i32 0), %struct.halide_buffer_t* nonnull %4, %struct.buffer_t* %1) #12
  %23 = icmp ne i32 %22, 0
  %24 = zext i1 %23 to i32
  br label %25

; <label>:25:                                     ; preds = %21, %18, %15, %3
  %26 = phi i32 [ 1, %18 ], [ %24, %21 ], [ 1, %15 ], [ 1, %3 ]
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %9) #9
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %6) #9
  ret i32 %26
}

; Function Attrs: nounwind
define weak i32 @halide_device_sync_legacy(i8*, %struct.buffer_t*) local_unnamed_addr #0 {
  %3 = alloca %struct.halide_buffer_t, align 8
  %4 = alloca [4 x %struct.halide_dimension_t], align 4
  %5 = bitcast %struct.halide_buffer_t* %3 to i8*
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %5) #9
  %6 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 6
  %7 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 7
  store i8* null, i8** %7, align 4, !tbaa !83
  %8 = bitcast [4 x %struct.halide_dimension_t]* %4 to i8*
  %9 = bitcast %struct.halide_buffer_t* %3 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %9, i8 0, i64 32, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %8) #9
  %10 = getelementptr inbounds [4 x %struct.halide_dimension_t], [4 x %struct.halide_dimension_t]* %4, i32 0, i32 0
  %11 = bitcast [4 x %struct.halide_dimension_t]* %4 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 4 %11, i8 0, i64 64, i1 false)
  store %struct.halide_dimension_t* %10, %struct.halide_dimension_t** %6, align 8, !tbaa !35
  %12 = call i32 @_ZN6Halide7Runtime8Internal29guess_type_and_dimensionalityEPvP8buffer_tP15halide_buffer_t(i8* %0, %struct.buffer_t* %1, %struct.halide_buffer_t* nonnull %3) #12
  %13 = icmp eq i32 %12, 0
  br i1 %13, label %14, label %24

; <label>:14:                                     ; preds = %2
  %15 = call i32 @halide_upgrade_buffer_t(i8* %0, i8* getelementptr inbounds ([1 x i8], [1 x i8]* @.str.89, i32 0, i32 0), %struct.buffer_t* %1, %struct.halide_buffer_t* nonnull %3, i32 0) #12
  %16 = icmp eq i32 %15, 0
  br i1 %16, label %17, label %24

; <label>:17:                                     ; preds = %14
  %18 = call i32 @halide_device_sync(i8* %0, %struct.halide_buffer_t* nonnull %3) #11
  %19 = icmp eq i32 %18, 0
  br i1 %19, label %20, label %24

; <label>:20:                                     ; preds = %17
  %21 = call i32 @halide_downgrade_buffer_t_device_fields(i8* %0, i8* getelementptr inbounds ([1 x i8], [1 x i8]* @.str.89, i32 0, i32 0), %struct.halide_buffer_t* nonnull %3, %struct.buffer_t* %1) #12
  %22 = icmp ne i32 %21, 0
  %23 = zext i1 %22 to i32
  br label %24

; <label>:24:                                     ; preds = %20, %17, %14, %2
  %25 = phi i32 [ 1, %17 ], [ %23, %20 ], [ 1, %14 ], [ 1, %2 ]
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %8) #9
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %5) #9
  ret i32 %25
}

; Function Attrs: nounwind
define weak i32 @halide_device_malloc_legacy(i8*, %struct.buffer_t*, %struct.halide_device_interface_t*) local_unnamed_addr #0 {
  %4 = alloca %struct.halide_buffer_t, align 8
  %5 = alloca [4 x %struct.halide_dimension_t], align 4
  %6 = bitcast %struct.halide_buffer_t* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %6) #9
  %7 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %4, i32 0, i32 6
  %8 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %4, i32 0, i32 7
  store i8* null, i8** %8, align 4, !tbaa !83
  %9 = bitcast [4 x %struct.halide_dimension_t]* %5 to i8*
  %10 = bitcast %struct.halide_buffer_t* %4 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %10, i8 0, i64 32, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %9) #9
  %11 = getelementptr inbounds [4 x %struct.halide_dimension_t], [4 x %struct.halide_dimension_t]* %5, i32 0, i32 0
  %12 = bitcast [4 x %struct.halide_dimension_t]* %5 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 4 %12, i8 0, i64 64, i1 false)
  store %struct.halide_dimension_t* %11, %struct.halide_dimension_t** %7, align 8, !tbaa !35
  %13 = call i32 @_ZN6Halide7Runtime8Internal29guess_type_and_dimensionalityEPvP8buffer_tP15halide_buffer_t(i8* %0, %struct.buffer_t* %1, %struct.halide_buffer_t* nonnull %4) #12
  %14 = icmp eq i32 %13, 0
  br i1 %14, label %15, label %25

; <label>:15:                                     ; preds = %3
  %16 = call i32 @halide_upgrade_buffer_t(i8* %0, i8* getelementptr inbounds ([1 x i8], [1 x i8]* @.str.89, i32 0, i32 0), %struct.buffer_t* %1, %struct.halide_buffer_t* nonnull %4, i32 0) #12
  %17 = icmp eq i32 %16, 0
  br i1 %17, label %18, label %25

; <label>:18:                                     ; preds = %15
  %19 = call i32 @halide_device_malloc(i8* %0, %struct.halide_buffer_t* nonnull %4, %struct.halide_device_interface_t* %2) #11
  %20 = icmp eq i32 %19, 0
  br i1 %20, label %21, label %25

; <label>:21:                                     ; preds = %18
  %22 = call i32 @halide_downgrade_buffer_t_device_fields(i8* %0, i8* getelementptr inbounds ([1 x i8], [1 x i8]* @.str.89, i32 0, i32 0), %struct.halide_buffer_t* nonnull %4, %struct.buffer_t* %1) #12
  %23 = icmp ne i32 %22, 0
  %24 = zext i1 %23 to i32
  br label %25

; <label>:25:                                     ; preds = %21, %18, %15, %3
  %26 = phi i32 [ 1, %18 ], [ %24, %21 ], [ 1, %15 ], [ 1, %3 ]
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %9) #9
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %6) #9
  ret i32 %26
}

; Function Attrs: nounwind
define weak i32 @halide_device_free_legacy(i8*, %struct.buffer_t*) local_unnamed_addr #0 {
  %3 = alloca %struct.halide_buffer_t, align 8
  %4 = alloca [4 x %struct.halide_dimension_t], align 4
  %5 = bitcast %struct.halide_buffer_t* %3 to i8*
  call void @llvm.lifetime.start.p0i8(i64 40, i8* nonnull %5) #9
  %6 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 6
  %7 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %3, i32 0, i32 7
  store i8* null, i8** %7, align 4, !tbaa !83
  %8 = bitcast [4 x %struct.halide_dimension_t]* %4 to i8*
  %9 = bitcast %struct.halide_buffer_t* %3 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %9, i8 0, i64 32, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %8) #9
  %10 = getelementptr inbounds [4 x %struct.halide_dimension_t], [4 x %struct.halide_dimension_t]* %4, i32 0, i32 0
  %11 = bitcast [4 x %struct.halide_dimension_t]* %4 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 4 %11, i8 0, i64 64, i1 false)
  store %struct.halide_dimension_t* %10, %struct.halide_dimension_t** %6, align 8, !tbaa !35
  %12 = call i32 @_ZN6Halide7Runtime8Internal29guess_type_and_dimensionalityEPvP8buffer_tP15halide_buffer_t(i8* %0, %struct.buffer_t* %1, %struct.halide_buffer_t* nonnull %3) #12
  %13 = icmp eq i32 %12, 0
  br i1 %13, label %14, label %24

; <label>:14:                                     ; preds = %2
  %15 = call i32 @halide_upgrade_buffer_t(i8* %0, i8* getelementptr inbounds ([1 x i8], [1 x i8]* @.str.89, i32 0, i32 0), %struct.buffer_t* %1, %struct.halide_buffer_t* nonnull %3, i32 0) #12
  %16 = icmp eq i32 %15, 0
  br i1 %16, label %17, label %24

; <label>:17:                                     ; preds = %14
  %18 = call i32 @halide_device_free(i8* %0, %struct.halide_buffer_t* nonnull %3) #11
  %19 = icmp eq i32 %18, 0
  br i1 %19, label %20, label %24

; <label>:20:                                     ; preds = %17
  %21 = call i32 @halide_downgrade_buffer_t_device_fields(i8* %0, i8* getelementptr inbounds ([1 x i8], [1 x i8]* @.str.89, i32 0, i32 0), %struct.halide_buffer_t* nonnull %3, %struct.buffer_t* %1) #12
  %22 = icmp ne i32 %21, 0
  %23 = zext i1 %22 to i32
  br label %24

; <label>:24:                                     ; preds = %20, %17, %14, %2
  %25 = phi i32 [ 1, %17 ], [ %23, %20 ], [ 1, %14 ], [ 1, %2 ]
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %8) #9
  call void @llvm.lifetime.end.p0i8(i64 40, i8* nonnull %5) #9
  ret i32 %25
}

; Function Attrs: norecurse nounwind
define weak i32 @halide_msan_annotate_memory_is_initialized(i8*, i8*, i64) local_unnamed_addr #3 {
  ret i32 0
}

; Function Attrs: norecurse nounwind
define weak i32 @halide_msan_annotate_buffer_is_initialized(i8*, %struct.halide_buffer_t.23*) local_unnamed_addr #3 {
  ret i32 0
}

; Function Attrs: norecurse nounwind
define weak void @halide_msan_annotate_buffer_is_initialized_as_destructor(i8*, i8*) local_unnamed_addr #3 {
  ret void
}

; Function Attrs: nounwind
define weak i32 @halide_qurt_hvx_lock(i8*, i32) local_unnamed_addr #0 {
  switch i32 %1, label %4 [
    i32 64, label %16
    i32 128, label %3
  ]

; <label>:3:                                      ; preds = %2
  br label %16

; <label>:4:                                      ; preds = %2
  %5 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %6 = icmp eq i8* %5, null
  br i1 %6, label %.split, label %.split2

.split:                                           ; preds = %4
  %7 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([34 x i8], [34 x i8]* @.str.102, i32 0, i32 0)) #11
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %32

.split2:                                          ; preds = %4
  %8 = ptrtoint i8* %5 to i32
  %9 = getelementptr inbounds i8, i8* %5, i32 1023
  store i8 0, i8* %9, align 1, !tbaa !17
  %10 = tail call i8* @halide_string_to_string(i8* nonnull %5, i8* nonnull %9, i8* nonnull getelementptr inbounds ([34 x i8], [34 x i8]* @.str.102, i32 0, i32 0)) #11
  %11 = ptrtoint i8* %10 to i32
  %12 = sub i32 1, %8
  %13 = add i32 %12, %11
  %14 = sext i32 %13 to i64
  %15 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %5, i64 %14) #11
  tail call void @halide_error(i8* %0, i8* nonnull %5) #11
  tail call void @halide_free(i8* %0, i8* nonnull %5) #11
  br label %32

; <label>:16:                                     ; preds = %3, %2
  %17 = phi i32 [ 1, %3 ], [ 0, %2 ]
  %18 = tail call i32 @qurt_hvx_lock(i32 %17) #11
  %19 = icmp eq i32 %18, 0
  br i1 %19, label %32, label %20

; <label>:20:                                     ; preds = %16
  %21 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %22 = icmp eq i8* %21, null
  br i1 %22, label %.split4, label %.split6

.split4:                                          ; preds = %20
  %23 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([22 x i8], [22 x i8]* @.str.5.104, i32 0, i32 0)) #11
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %32

.split6:                                          ; preds = %20
  %24 = ptrtoint i8* %21 to i32
  %25 = getelementptr inbounds i8, i8* %21, i32 1023
  store i8 0, i8* %25, align 1, !tbaa !17
  %26 = tail call i8* @halide_string_to_string(i8* nonnull %21, i8* nonnull %25, i8* nonnull getelementptr inbounds ([22 x i8], [22 x i8]* @.str.5.104, i32 0, i32 0)) #11
  %27 = ptrtoint i8* %26 to i32
  %28 = sub i32 1, %24
  %29 = add i32 %28, %27
  %30 = sext i32 %29 to i64
  %31 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %21, i64 %30) #11
  tail call void @halide_error(i8* %0, i8* nonnull %21) #11
  tail call void @halide_free(i8* %0, i8* nonnull %21) #11
  br label %32

; <label>:32:                                     ; preds = %.split4, %.split6, %.split, %.split2, %16
  %33 = phi i32 [ 0, %16 ], [ -1, %.split2 ], [ -1, %.split ], [ -1, %.split6 ], [ -1, %.split4 ]
  ret i32 %33
}

declare i32 @qurt_hvx_lock(i32) local_unnamed_addr #2

; Function Attrs: nounwind
define weak i32 @halide_qurt_hvx_unlock(i8*) local_unnamed_addr #0 {
  %2 = tail call i32 @qurt_hvx_unlock() #11
  %3 = icmp eq i32 %2, 0
  br i1 %3, label %16, label %4

; <label>:4:                                      ; preds = %1
  %5 = tail call i8* @halide_malloc(i8* %0, i32 1024) #11
  %6 = icmp eq i8* %5, null
  br i1 %6, label %.split, label %.split2

.split:                                           ; preds = %4
  %7 = tail call i8* @halide_string_to_string(i8* null, i8* null, i8* nonnull getelementptr inbounds ([24 x i8], [24 x i8]* @.str.7.105, i32 0, i32 0)) #11
  tail call void @halide_error(i8* %0, i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str.8.103, i32 0, i32 0)) #11
  tail call void @halide_free(i8* %0, i8* null) #11
  br label %16

.split2:                                          ; preds = %4
  %8 = ptrtoint i8* %5 to i32
  %9 = getelementptr inbounds i8, i8* %5, i32 1023
  store i8 0, i8* %9, align 1, !tbaa !17
  %10 = tail call i8* @halide_string_to_string(i8* nonnull %5, i8* nonnull %9, i8* nonnull getelementptr inbounds ([24 x i8], [24 x i8]* @.str.7.105, i32 0, i32 0)) #11
  %11 = ptrtoint i8* %10 to i32
  %12 = sub i32 1, %8
  %13 = add i32 %12, %11
  %14 = sext i32 %13 to i64
  %15 = tail call i32 @halide_msan_annotate_memory_is_initialized(i8* %0, i8* nonnull %5, i64 %14) #11
  tail call void @halide_error(i8* %0, i8* nonnull %5) #11
  tail call void @halide_free(i8* %0, i8* nonnull %5) #11
  br label %16

; <label>:16:                                     ; preds = %.split, %.split2, %1
  %17 = phi i32 [ 0, %1 ], [ -1, %.split2 ], [ -1, %.split ]
  ret i32 %17
}

declare i32 @qurt_hvx_unlock() local_unnamed_addr #2

; Function Attrs: nounwind
define weak void @halide_qurt_hvx_unlock_as_destructor(i8*, i8*) local_unnamed_addr #0 {
  %3 = tail call i32 @halide_qurt_hvx_unlock(i8* %0) #12
  ret void
}

; Function Attrs: nounwind readnone
declare <16 x i32> @llvm.hexagon.V6.lo(<32 x i32>) #7

; Function Attrs: nounwind readnone
declare <16 x i32> @llvm.hexagon.V6.hi(<32 x i32>) #7

; Function Attrs: nounwind readnone
declare <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32>, <16 x i32>, i32) #7

; Function Attrs: nounwind readnone
declare <32 x i32> @llvm.hexagon.V6.vdealvdd(<16 x i32>, <16 x i32>, i32) #7

; Function Attrs: nounwind readnone
declare <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32>, <16 x i32>) #7

; Function Attrs: nounwind readnone
declare <32 x i32> @llvm.hexagon.V6.vmpahb.acc(<32 x i32>, <32 x i32>, i32) #7

; Function Attrs: nounwind
define weak i32 @halide_default_can_use_target_features(i32, i64*) #0 {
  %3 = alloca %"struct.Halide::Runtime::Internal::CpuFeatures", align 8
  tail call void @halide_mutex_lock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal36halide_cpu_features_initialized_lockE) #11
  %4 = load i8, i8* @_ZN6Halide7Runtime8Internal31halide_cpu_features_initializedE, align 1, !tbaa !14, !range !16
  %5 = icmp eq i8 %4, 0
  br i1 %5, label %6, label %9

; <label>:6:                                      ; preds = %2
  %7 = bitcast %"struct.Halide::Runtime::Internal::CpuFeatures"* %3 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %7) #9
  call void @_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv(%"struct.Halide::Runtime::Internal::CpuFeatures"* nonnull sret %3) #11
  %8 = call i8* @memcpy(i8* bitcast ([4 x i64]* @_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE to i8*), i8* nonnull %7, i32 32) #11
  store i8 1, i8* @_ZN6Halide7Runtime8Internal31halide_cpu_features_initializedE, align 1, !tbaa !14
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %7) #9
  br label %9

; <label>:9:                                      ; preds = %6, %2
  call void @halide_mutex_unlock(%struct.halide_mutex* nonnull @_ZN6Halide7Runtime8Internal36halide_cpu_features_initialized_lockE) #11
  %10 = icmp eq i32 %0, 2
  br i1 %10, label %12, label %11

; <label>:11:                                     ; preds = %9
  call void @halide_error(i8* null, i8* getelementptr inbounds ([81 x i8], [81 x i8]* @.str.106, i32 0, i32 0)) #11
  br label %12

; <label>:12:                                     ; preds = %11, %9
  %13 = load i64, i64* %1, align 8, !tbaa !20
  %14 = load i64, i64* getelementptr inbounds ([4 x i64], [4 x i64]* @_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE, i32 0, i32 0), align 8, !tbaa !20
  %15 = and i64 %14, %13
  %16 = icmp eq i64 %15, 0
  br i1 %16, label %21, label %17

; <label>:17:                                     ; preds = %12
  %18 = load i64, i64* getelementptr inbounds ([4 x i64], [4 x i64]* @_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE, i32 0, i32 2), align 8, !tbaa !20
  %19 = and i64 %18, %15
  %20 = icmp eq i64 %19, %15
  br i1 %20, label %21, label %27

; <label>:21:                                     ; preds = %17, %12
  %22 = getelementptr inbounds i64, i64* %1, i32 1
  %23 = load i64, i64* %22, align 8, !tbaa !20
  %24 = load i64, i64* getelementptr inbounds ([4 x i64], [4 x i64]* @_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE, i32 0, i32 1), align 8, !tbaa !20
  %25 = and i64 %24, %23
  %26 = icmp eq i64 %25, 0
  br i1 %26, label %33, label %29

; <label>:27:                                     ; preds = %33, %29, %17
  %28 = phi i32 [ 0, %17 ], [ 0, %29 ], [ 1, %33 ]
  ret i32 %28

; <label>:29:                                     ; preds = %21
  %30 = load i64, i64* getelementptr inbounds ([4 x i64], [4 x i64]* @_ZN6Halide7Runtime8Internal27halide_cpu_features_storageE, i32 0, i32 3), align 8, !tbaa !20
  %31 = and i64 %30, %25
  %32 = icmp eq i64 %31, %25
  br i1 %32, label %33, label %27

; <label>:33:                                     ; preds = %29, %21
  br label %27
}

; Function Attrs: norecurse nounwind
define weak i32 (i32, i64*)* @halide_set_custom_can_use_target_features(i32 (i32, i64*)*) local_unnamed_addr #3 {
  %2 = load i32 (i32, i64*)*, i32 (i32, i64*)** @_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE, align 4, !tbaa !8
  store i32 (i32, i64*)* %0, i32 (i32, i64*)** @_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE, align 4, !tbaa !8
  ret i32 (i32, i64*)* %2
}

; Function Attrs: nounwind
define weak i32 @halide_can_use_target_features(i32, i64*) local_unnamed_addr #0 {
  %3 = load i32 (i32, i64*)*, i32 (i32, i64*)** @_ZN6Halide7Runtime8Internal30custom_can_use_target_featuresE, align 4, !tbaa !8
  %4 = tail call i32 %3(i32 %0, i64* %1) #11
  ret i32 %4
}

; Function Attrs: norecurse nounwind
define linkonce void @_ZN6Halide7Runtime8Internal23halide_get_cpu_featuresEv(%"struct.Halide::Runtime::Internal::CpuFeatures"* noalias sret) local_unnamed_addr #3 {
  %2 = bitcast %"struct.Halide::Runtime::Internal::CpuFeatures"* %0 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 8 %2, i8 0, i64 32, i1 false)
  ret void
}

declare void @abort() local_unnamed_addr #2

; Function Attrs: nounwind readnone
declare <32 x i32> @llvm.hexagon.V6.vunpackub(<16 x i32>) #7

; Function Attrs: nounwind readnone
declare <32 x i32> @llvm.hexagon.V6.vunpackuh(<16 x i32>) #7

; Function Attrs: nounwind readnone
declare <16 x i32> @llvm.hexagon.V6.vpackhub.sat(<16 x i32>, <16 x i32>) #7

; Function Attrs: nounwind readnone
declare <32 x i32> @llvm.hexagon.V6.vaddw.dv(<32 x i32>, <32 x i32>) #7

; Function Attrs: nounwind readnone
declare <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32>, <16 x i32>, i32) #7

; Function Attrs: nounwind readnone
declare <16 x i32> @llvm.hexagon.V6.vrmpybus.acc(<16 x i32>, <16 x i32>, i32) #7

; Function Attrs: nounwind readnone
declare <16 x i32> @llvm.hexagon.V6.vasrwhsat(<16 x i32>, <16 x i32>, i32) #7

; Function Attrs: nounwind
define i32 @gaussian7x7_hvx64(%struct.halide_buffer_t* noalias nocapture readonly %input.buffer, %struct.halide_buffer_t* noalias nocapture readonly %output.buffer) local_unnamed_addr #8 {
entry:
  %hvx_lock_result = tail call i32 @halide_qurt_hvx_lock(i8* null, i32 64) #9
  %0 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %input.buffer, i32 0, i32 2
  %1 = load i8*, i8** %0, align 4, !tbaa !32
  %2 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %input.buffer, i32 0, i32 6
  %3 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %2, align 8, !tbaa !35
  %4 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %3, i32 0, i32 0
  %5 = load i32, i32* %4, align 4, !tbaa !36
  %6 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %3, i32 0, i32 2
  %7 = load i32, i32* %6, align 4, !tbaa !39
  %8 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %3, i32 1, i32 0
  %9 = load i32, i32* %8, align 4, !tbaa !36
  %10 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %3, i32 1, i32 2
  %11 = load i32, i32* %10, align 4, !tbaa !39
  %12 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %output.buffer, i32 0, i32 2
  %13 = load i8*, i8** %12, align 4, !tbaa !32
  %14 = getelementptr inbounds %struct.halide_buffer_t, %struct.halide_buffer_t* %output.buffer, i32 0, i32 6
  %15 = load %struct.halide_dimension_t*, %struct.halide_dimension_t** %14, align 8, !tbaa !35
  %16 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %15, i32 0, i32 0
  %17 = load i32, i32* %16, align 4, !tbaa !36
  %18 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %15, i32 0, i32 1
  %19 = load i32, i32* %18, align 4, !tbaa !38
  %20 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %15, i32 0, i32 2
  %21 = load i32, i32* %20, align 4, !tbaa !39
  %22 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %15, i32 1, i32 0
  %23 = load i32, i32* %22, align 4, !tbaa !36
  %24 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %15, i32 1, i32 1
  %25 = load i32, i32* %24, align 4, !tbaa !38
  %26 = getelementptr inbounds %struct.halide_dimension_t, %struct.halide_dimension_t* %15, i32 1, i32 2
  %27 = load i32, i32* %26, align 4, !tbaa !39
  %28 = icmp ne i32 %7, 1
  %29 = zext i1 %28 to i64
  %30 = icmp ne i32 %5, 0
  %31 = zext i1 %30 to i64
  %32 = shl nuw nsw i64 %31, 1
  %33 = and i32 %11, 63
  %34 = icmp ne i32 %33, 0
  %35 = zext i1 %34 to i64
  %36 = shl nuw nsw i64 %35, 2
  %37 = icmp ne i32 %9, 0
  %38 = zext i1 %37 to i64
  %39 = shl nuw nsw i64 %38, 3
  %40 = icmp ne i32 %21, 1
  %41 = zext i1 %40 to i64
  %42 = shl nuw nsw i64 %41, 4
  %43 = icmp ne i32 %17, 0
  %44 = zext i1 %43 to i64
  %45 = shl nuw nsw i64 %44, 5
  %46 = and i32 %27, 63
  %47 = icmp ne i32 %46, 0
  %48 = zext i1 %47 to i64
  %49 = shl nuw nsw i64 %48, 6
  %50 = icmp ne i32 %23, 0
  %51 = zext i1 %50 to i64
  %52 = shl nuw nsw i64 %51, 7
  %53 = or i64 %32, %29
  %54 = or i64 %53, %39
  %55 = or i64 %54, %36
  %56 = or i64 %55, %45
  %57 = or i64 %56, %42
  %58 = or i64 %57, %52
  %59 = or i64 %58, %49
  %60 = or i64 %59, -9223372036854775808
  %61 = tail call i64 @llvm.cttz.i64(i64 %60, i1 true), !range !84
  %62 = trunc i64 %61 to i32
  %switch = icmp ult i32 %62, 8
  br i1 %switch, label %assert_failed, label %"produce output", !prof !85

assert_failed:                                    ; preds = %entry
  ret i32 0

"produce output":                                 ; preds = %entry
  %63 = mul nsw i32 %11, 3
  %t29 = sub nsw i32 -3, %63
  %64 = shl nsw i32 %11, 1
  %t32 = sub nsw i32 -3, %64
  %65 = add nsw i32 %25, 3
  %66 = ashr i32 %65, 2
  %67 = icmp sgt i32 %25, 0
  br i1 %67, label %"for output.s0.y.y.preheader", label %call_destructor.exit96, !prof !86

"for output.s0.y.y.preheader":                    ; preds = %"produce output"
  %68 = mul nsw i32 %11, 5
  %69 = add i32 %68, -3
  %70 = bitcast i8* %1 to i32*
  %71 = add nsw i32 %19, 63
  %72 = ashr i32 %71, 6
  %73 = add i32 %19, 127
  %74 = and i32 %73, -64
  %75 = shl nsw i32 %74, 2
  %76 = shl nsw i32 %11, 2
  %77 = zext i32 %76 to i64
  %78 = shl nuw i64 %77, 32
  %79 = sext i32 %75 to i64
  %80 = shl nsw i64 %79, 16
  %81 = or i64 %78, %80
  %82 = or i64 %81, 281474976710666
  %83 = icmp sgt i32 %72, -1
  %t133 = select i1 %83, i32 %72, i32 -1
  %84 = shl i32 %t133, 10
  %85 = add i32 %84, 1024
  %86 = or i32 %85, 68
  %87 = ashr i32 %73, 6
  %88 = icmp sgt i32 %73, 63
  %89 = add i32 %63, -3
  %90 = add i32 %11, -3
  %91 = shl nsw i32 %72, 1
  %92 = mul nsw i32 %72, 3
  %93 = icmp sgt i32 %19, 0
  %94 = mul nsw i32 %72, 192
  %95 = add nsw i32 %94, 192
  %96 = shl i32 %72, 7
  %97 = add nsw i32 %96, 128
  br label %"for output.s0.y.y"

"for output.s0.y.y":                              ; preds = %"for output.s0.y.y.preheader", %call_destructor.exit
  %output.s0.y.y = phi i32 [ %454, %call_destructor.exit ], [ 0, %"for output.s0.y.y.preheader" ]
  %98 = mul nsw i32 %output.s0.y.y, %11
  %99 = shl i32 %98, 2
  %100 = add i32 %69, %99
  %101 = getelementptr inbounds i32, i32* %70, i32 %100
  %102 = bitcast i32* %101 to i8*
  tail call void asm sideeffect "l2fetch($0,$1)", "r,r"(i8* %102, i64 %82) #9, !srcloc !87
  %103 = tail call i8* @halide_malloc(i8* null, i32 %86)
  %rows = bitcast i8* %103 to i32*
  br i1 %88, label %"for rows.s0.x.x.preheader", label %"consume rows", !prof !86

"for rows.s0.x.x.preheader":                      ; preds = %"for output.s0.y.y"
  %104 = shl nsw i32 %output.s0.y.y, 2
  %105 = or i32 %104, 1
  %106 = mul nsw i32 %105, %11
  %107 = or i32 %104, 2
  %108 = mul nsw i32 %107, %11
  %109 = or i32 %104, 3
  %110 = mul nsw i32 %109, %11
  br label %"for rows.s0.x.x"

call_destructor.exit96:                           ; preds = %call_destructor.exit, %"produce output"
  tail call void @halide_qurt_hvx_unlock_as_destructor(i8* null, i8* nonnull inttoptr (i32 1 to i8*)) #11
  ret i32 0

"for rows.s0.x.x":                                ; preds = %"for rows.s0.x.x.preheader", %"for rows.s0.x.x"
  %rows.s0.x.x = phi i32 [ %432, %"for rows.s0.x.x" ], [ 0, %"for rows.s0.x.x.preheader" ]
  %111 = shl nsw i32 %rows.s0.x.x, 4
  %t59 = add nsw i32 %111, %98
  %112 = shl nsw i32 %t59, 2
  %113 = add i32 %89, %112
  %114 = getelementptr inbounds i8, i8* %1, i32 %113
  %115 = bitcast i8* %114 to <16 x i32>*
  %116 = load <16 x i32>, <16 x i32>* %115, align 1, !tbaa !88
  %117 = tail call <32 x i32> @llvm.hexagon.V6.vunpackub(<16 x i32> %116) #9
  %118 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %117)
  %119 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %117)
  %120 = tail call <32 x i32> @llvm.hexagon.V6.vunpackuh(<16 x i32> %119) #9
  %121 = tail call <32 x i32> @llvm.hexagon.V6.vunpackuh(<16 x i32> %118) #9
  %122 = shl nsw i32 %t59, 1
  %123 = sub nsw i32 %122, %11
  %124 = shl i32 %123, 1
  %125 = add nsw i32 %124, -3
  %126 = getelementptr inbounds i8, i8* %1, i32 %125
  %127 = bitcast i8* %126 to <16 x i32>*
  %128 = load <16 x i32>, <16 x i32>* %127, align 1, !tbaa !88
  %129 = add i32 %112, -3
  %130 = sub i32 %129, %11
  %131 = getelementptr inbounds i8, i8* %1, i32 %130
  %132 = bitcast i8* %131 to <16 x i32>*
  %133 = load <16 x i32>, <16 x i32>* %132, align 1, !tbaa !88
  %134 = getelementptr inbounds i8, i8* %1, i32 %129
  %135 = bitcast i8* %134 to <16 x i32>*
  %136 = load <16 x i32>, <16 x i32>* %135, align 1, !tbaa !88
  %137 = add nsw i32 %122, %11
  %138 = shl i32 %137, 1
  %139 = add nsw i32 %138, -3
  %140 = getelementptr inbounds i8, i8* %1, i32 %139
  %141 = bitcast i8* %140 to <16 x i32>*
  %142 = load <16 x i32>, <16 x i32>* %141, align 1, !tbaa !88
  %143 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %136, <16 x i32> %128, i32 -1)
  %144 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %142, <16 x i32> %133, i32 -1)
  %145 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %143)
  %146 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %143)
  %147 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %144)
  %148 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %144)
  %149 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %148, <16 x i32> %146, i32 -1)
  %150 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %147, <16 x i32> %145, i32 -1)
  %151 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %120)
  %152 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %149)
  %153 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %149)
  %154 = tail call <16 x i32> @llvm.hexagon.V6.vrmpybus.acc(<16 x i32> %151, <16 x i32> %153, i32 101977862) #9
  %155 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %120)
  %156 = tail call <16 x i32> @llvm.hexagon.V6.vrmpybus.acc(<16 x i32> %155, <16 x i32> %152, i32 101977862) #9
  %157 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %121)
  %158 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %150)
  %159 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %150)
  %160 = tail call <16 x i32> @llvm.hexagon.V6.vrmpybus.acc(<16 x i32> %157, <16 x i32> %159, i32 101977862) #9
  %161 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %121)
  %162 = tail call <16 x i32> @llvm.hexagon.V6.vrmpybus.acc(<16 x i32> %161, <16 x i32> %158, i32 101977862) #9
  %163 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %156, <16 x i32> %154)
  %164 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %162, <16 x i32> %160)
  %e.i93 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %163) #9
  %o.i94 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %163) #9
  %r.i95 = tail call <32 x i32> @llvm.hexagon.V6.vdealvdd(<16 x i32> %o.i94, <16 x i32> %e.i93, i32 -4) #9
  %e.i90 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %164) #9
  %o.i91 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %164) #9
  %r.i92 = tail call <32 x i32> @llvm.hexagon.V6.vdealvdd(<16 x i32> %o.i91, <16 x i32> %e.i90, i32 -4) #9
  %165 = add i32 %90, %112
  %166 = getelementptr inbounds i8, i8* %1, i32 %165
  %167 = bitcast i8* %166 to <16 x i32>*
  %168 = load <16 x i32>, <16 x i32>* %167, align 1, !tbaa !88
  %169 = tail call <32 x i32> @llvm.hexagon.V6.vunpackub(<16 x i32> %168) #9
  %170 = add nsw i32 %112, %t29
  %171 = getelementptr inbounds i8, i8* %1, i32 %170
  %172 = bitcast i8* %171 to <16 x i32>*
  %173 = load <16 x i32>, <16 x i32>* %172, align 1, !tbaa !88
  %174 = tail call <32 x i32> @llvm.hexagon.V6.vunpackub(<16 x i32> %173) #9
  %175 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %169)
  %176 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %169)
  %177 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %174)
  %178 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %174)
  %dv1.i88 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %178, <16 x i32> %176) #9
  %res.i89 = tail call <32 x i32> @llvm.hexagon.V6.vmpahb.acc(<32 x i32> %r.i95, <32 x i32> %dv1.i88, i32 17760527) #9
  %dv1.i86 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %177, <16 x i32> %175) #9
  %res.i87 = tail call <32 x i32> @llvm.hexagon.V6.vmpahb.acc(<32 x i32> %r.i92, <32 x i32> %dv1.i86, i32 17760527) #9
  %e.i83 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %res.i89) #9
  %o.i84 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %res.i89) #9
  %r.i85 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %o.i84, <16 x i32> %e.i83, i32 -4) #9
  %e.i80 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %res.i87) #9
  %o.i81 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %res.i87) #9
  %r.i82 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %o.i81, <16 x i32> %e.i80, i32 -4) #9
  %179 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %r.i85)
  %180 = shl nsw i32 %rows.s0.x.x, 6
  %181 = getelementptr inbounds i32, i32* %rows, i32 %180
  %182 = bitcast i32* %181 to <16 x i32>*
  store <16 x i32> %179, <16 x i32>* %182, align 64, !tbaa !91
  %183 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %r.i85)
  %184 = or i32 %180, 16
  %185 = getelementptr inbounds i32, i32* %rows, i32 %184
  %186 = bitcast i32* %185 to <16 x i32>*
  store <16 x i32> %183, <16 x i32>* %186, align 64, !tbaa !91
  %187 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %r.i82)
  %188 = or i32 %180, 32
  %189 = getelementptr inbounds i32, i32* %rows, i32 %188
  %190 = bitcast i32* %189 to <16 x i32>*
  store <16 x i32> %187, <16 x i32>* %190, align 64, !tbaa !91
  %191 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %r.i82)
  %192 = or i32 %180, 48
  %193 = getelementptr inbounds i32, i32* %rows, i32 %192
  %194 = bitcast i32* %193 to <16 x i32>*
  store <16 x i32> %191, <16 x i32>* %194, align 64, !tbaa !91
  %t60 = add nsw i32 %180, %106
  %195 = add i32 %89, %t60
  %196 = getelementptr inbounds i8, i8* %1, i32 %195
  %197 = bitcast i8* %196 to <16 x i32>*
  %198 = load <16 x i32>, <16 x i32>* %197, align 1, !tbaa !88
  %199 = tail call <32 x i32> @llvm.hexagon.V6.vunpackub(<16 x i32> %198) #9
  %200 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %199)
  %201 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %199)
  %202 = tail call <32 x i32> @llvm.hexagon.V6.vunpackuh(<16 x i32> %201) #9
  %203 = tail call <32 x i32> @llvm.hexagon.V6.vunpackuh(<16 x i32> %200) #9
  %204 = add nsw i32 %t60, %t32
  %205 = getelementptr inbounds i8, i8* %1, i32 %204
  %206 = bitcast i8* %205 to <16 x i32>*
  %207 = load <16 x i32>, <16 x i32>* %206, align 1, !tbaa !88
  %208 = add i32 %t60, -3
  %209 = sub i32 %208, %11
  %210 = getelementptr inbounds i8, i8* %1, i32 %209
  %211 = bitcast i8* %210 to <16 x i32>*
  %212 = load <16 x i32>, <16 x i32>* %211, align 1, !tbaa !88
  %213 = getelementptr inbounds i8, i8* %1, i32 %208
  %214 = bitcast i8* %213 to <16 x i32>*
  %215 = load <16 x i32>, <16 x i32>* %214, align 1, !tbaa !88
  %216 = add i32 %208, %64
  %217 = getelementptr inbounds i8, i8* %1, i32 %216
  %218 = bitcast i8* %217 to <16 x i32>*
  %219 = load <16 x i32>, <16 x i32>* %218, align 1, !tbaa !88
  %220 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %215, <16 x i32> %207, i32 -1)
  %221 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %219, <16 x i32> %212, i32 -1)
  %222 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %220)
  %223 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %220)
  %224 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %221)
  %225 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %221)
  %226 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %225, <16 x i32> %223, i32 -1)
  %227 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %224, <16 x i32> %222, i32 -1)
  %228 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %202)
  %229 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %226)
  %230 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %226)
  %231 = tail call <16 x i32> @llvm.hexagon.V6.vrmpybus.acc(<16 x i32> %228, <16 x i32> %230, i32 101977862) #9
  %232 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %202)
  %233 = tail call <16 x i32> @llvm.hexagon.V6.vrmpybus.acc(<16 x i32> %232, <16 x i32> %229, i32 101977862) #9
  %234 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %203)
  %235 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %227)
  %236 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %227)
  %237 = tail call <16 x i32> @llvm.hexagon.V6.vrmpybus.acc(<16 x i32> %234, <16 x i32> %236, i32 101977862) #9
  %238 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %203)
  %239 = tail call <16 x i32> @llvm.hexagon.V6.vrmpybus.acc(<16 x i32> %238, <16 x i32> %235, i32 101977862) #9
  %240 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %233, <16 x i32> %231)
  %241 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %239, <16 x i32> %237)
  %e.i77 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %240) #9
  %o.i78 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %240) #9
  %r.i79 = tail call <32 x i32> @llvm.hexagon.V6.vdealvdd(<16 x i32> %o.i78, <16 x i32> %e.i77, i32 -4) #9
  %e.i74 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %241) #9
  %o.i75 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %241) #9
  %r.i76 = tail call <32 x i32> @llvm.hexagon.V6.vdealvdd(<16 x i32> %o.i75, <16 x i32> %e.i74, i32 -4) #9
  %242 = add i32 %90, %t60
  %243 = getelementptr inbounds i8, i8* %1, i32 %242
  %244 = bitcast i8* %243 to <16 x i32>*
  %245 = load <16 x i32>, <16 x i32>* %244, align 1, !tbaa !88
  %246 = tail call <32 x i32> @llvm.hexagon.V6.vunpackub(<16 x i32> %245) #9
  %247 = add nsw i32 %t60, %t29
  %248 = getelementptr inbounds i8, i8* %1, i32 %247
  %249 = bitcast i8* %248 to <16 x i32>*
  %250 = load <16 x i32>, <16 x i32>* %249, align 1, !tbaa !88
  %251 = tail call <32 x i32> @llvm.hexagon.V6.vunpackub(<16 x i32> %250) #9
  %252 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %246)
  %253 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %246)
  %254 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %251)
  %255 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %251)
  %dv1.i72 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %255, <16 x i32> %253) #9
  %res.i73 = tail call <32 x i32> @llvm.hexagon.V6.vmpahb.acc(<32 x i32> %r.i79, <32 x i32> %dv1.i72, i32 17760527) #9
  %dv1.i70 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %254, <16 x i32> %252) #9
  %res.i71 = tail call <32 x i32> @llvm.hexagon.V6.vmpahb.acc(<32 x i32> %r.i76, <32 x i32> %dv1.i70, i32 17760527) #9
  %e.i67 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %res.i73) #9
  %o.i68 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %res.i73) #9
  %r.i69 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %o.i68, <16 x i32> %e.i67, i32 -4) #9
  %e.i64 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %res.i71) #9
  %o.i65 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %res.i71) #9
  %r.i66 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %o.i65, <16 x i32> %e.i64, i32 -4) #9
  %256 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %r.i69)
  %257 = add nsw i32 %rows.s0.x.x, %72
  %258 = shl nsw i32 %257, 6
  %259 = add nsw i32 %258, 64
  %260 = getelementptr inbounds i32, i32* %rows, i32 %259
  %261 = bitcast i32* %260 to <16 x i32>*
  store <16 x i32> %256, <16 x i32>* %261, align 64, !tbaa !91
  %262 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %r.i69)
  %263 = add nsw i32 %258, 80
  %264 = getelementptr inbounds i32, i32* %rows, i32 %263
  %265 = bitcast i32* %264 to <16 x i32>*
  store <16 x i32> %262, <16 x i32>* %265, align 64, !tbaa !91
  %266 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %r.i66)
  %267 = add nsw i32 %258, 96
  %268 = getelementptr inbounds i32, i32* %rows, i32 %267
  %269 = bitcast i32* %268 to <16 x i32>*
  store <16 x i32> %266, <16 x i32>* %269, align 64, !tbaa !91
  %270 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %r.i66)
  %271 = add nsw i32 %258, 112
  %272 = getelementptr inbounds i32, i32* %rows, i32 %271
  %273 = bitcast i32* %272 to <16 x i32>*
  store <16 x i32> %270, <16 x i32>* %273, align 64, !tbaa !91
  %t61 = add nsw i32 %180, %108
  %274 = add i32 %89, %t61
  %275 = getelementptr inbounds i8, i8* %1, i32 %274
  %276 = bitcast i8* %275 to <16 x i32>*
  %277 = load <16 x i32>, <16 x i32>* %276, align 1, !tbaa !88
  %278 = tail call <32 x i32> @llvm.hexagon.V6.vunpackub(<16 x i32> %277) #9
  %279 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %278)
  %280 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %278)
  %281 = tail call <32 x i32> @llvm.hexagon.V6.vunpackuh(<16 x i32> %280) #9
  %282 = tail call <32 x i32> @llvm.hexagon.V6.vunpackuh(<16 x i32> %279) #9
  %283 = add nsw i32 %t61, %t32
  %284 = getelementptr inbounds i8, i8* %1, i32 %283
  %285 = bitcast i8* %284 to <16 x i32>*
  %286 = load <16 x i32>, <16 x i32>* %285, align 1, !tbaa !88
  %287 = add i32 %t61, -3
  %288 = sub i32 %287, %11
  %289 = getelementptr inbounds i8, i8* %1, i32 %288
  %290 = bitcast i8* %289 to <16 x i32>*
  %291 = load <16 x i32>, <16 x i32>* %290, align 1, !tbaa !88
  %292 = getelementptr inbounds i8, i8* %1, i32 %287
  %293 = bitcast i8* %292 to <16 x i32>*
  %294 = load <16 x i32>, <16 x i32>* %293, align 1, !tbaa !88
  %295 = add i32 %287, %64
  %296 = getelementptr inbounds i8, i8* %1, i32 %295
  %297 = bitcast i8* %296 to <16 x i32>*
  %298 = load <16 x i32>, <16 x i32>* %297, align 1, !tbaa !88
  %299 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %294, <16 x i32> %286, i32 -1)
  %300 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %298, <16 x i32> %291, i32 -1)
  %301 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %299)
  %302 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %299)
  %303 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %300)
  %304 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %300)
  %305 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %304, <16 x i32> %302, i32 -1)
  %306 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %303, <16 x i32> %301, i32 -1)
  %307 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %281)
  %308 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %305)
  %309 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %305)
  %310 = tail call <16 x i32> @llvm.hexagon.V6.vrmpybus.acc(<16 x i32> %307, <16 x i32> %309, i32 101977862) #9
  %311 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %281)
  %312 = tail call <16 x i32> @llvm.hexagon.V6.vrmpybus.acc(<16 x i32> %311, <16 x i32> %308, i32 101977862) #9
  %313 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %282)
  %314 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %306)
  %315 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %306)
  %316 = tail call <16 x i32> @llvm.hexagon.V6.vrmpybus.acc(<16 x i32> %313, <16 x i32> %315, i32 101977862) #9
  %317 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %282)
  %318 = tail call <16 x i32> @llvm.hexagon.V6.vrmpybus.acc(<16 x i32> %317, <16 x i32> %314, i32 101977862) #9
  %319 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %312, <16 x i32> %310)
  %320 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %318, <16 x i32> %316)
  %e.i61 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %319) #9
  %o.i62 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %319) #9
  %r.i63 = tail call <32 x i32> @llvm.hexagon.V6.vdealvdd(<16 x i32> %o.i62, <16 x i32> %e.i61, i32 -4) #9
  %e.i58 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %320) #9
  %o.i59 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %320) #9
  %r.i60 = tail call <32 x i32> @llvm.hexagon.V6.vdealvdd(<16 x i32> %o.i59, <16 x i32> %e.i58, i32 -4) #9
  %321 = add i32 %90, %t61
  %322 = getelementptr inbounds i8, i8* %1, i32 %321
  %323 = bitcast i8* %322 to <16 x i32>*
  %324 = load <16 x i32>, <16 x i32>* %323, align 1, !tbaa !88
  %325 = tail call <32 x i32> @llvm.hexagon.V6.vunpackub(<16 x i32> %324) #9
  %326 = add nsw i32 %t61, %t29
  %327 = getelementptr inbounds i8, i8* %1, i32 %326
  %328 = bitcast i8* %327 to <16 x i32>*
  %329 = load <16 x i32>, <16 x i32>* %328, align 1, !tbaa !88
  %330 = tail call <32 x i32> @llvm.hexagon.V6.vunpackub(<16 x i32> %329) #9
  %331 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %325)
  %332 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %325)
  %333 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %330)
  %334 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %330)
  %dv1.i56 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %334, <16 x i32> %332) #9
  %res.i57 = tail call <32 x i32> @llvm.hexagon.V6.vmpahb.acc(<32 x i32> %r.i63, <32 x i32> %dv1.i56, i32 17760527) #9
  %dv1.i54 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %333, <16 x i32> %331) #9
  %res.i55 = tail call <32 x i32> @llvm.hexagon.V6.vmpahb.acc(<32 x i32> %r.i60, <32 x i32> %dv1.i54, i32 17760527) #9
  %e.i51 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %res.i57) #9
  %o.i52 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %res.i57) #9
  %r.i53 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %o.i52, <16 x i32> %e.i51, i32 -4) #9
  %e.i48 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %res.i55) #9
  %o.i49 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %res.i55) #9
  %r.i50 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %o.i49, <16 x i32> %e.i48, i32 -4) #9
  %335 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %r.i53)
  %336 = add nsw i32 %rows.s0.x.x, %91
  %337 = shl nsw i32 %336, 6
  %338 = add nsw i32 %337, 128
  %339 = getelementptr inbounds i32, i32* %rows, i32 %338
  %340 = bitcast i32* %339 to <16 x i32>*
  store <16 x i32> %335, <16 x i32>* %340, align 64, !tbaa !91
  %341 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %r.i53)
  %342 = add nsw i32 %337, 144
  %343 = getelementptr inbounds i32, i32* %rows, i32 %342
  %344 = bitcast i32* %343 to <16 x i32>*
  store <16 x i32> %341, <16 x i32>* %344, align 64, !tbaa !91
  %345 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %r.i50)
  %346 = add nsw i32 %337, 160
  %347 = getelementptr inbounds i32, i32* %rows, i32 %346
  %348 = bitcast i32* %347 to <16 x i32>*
  store <16 x i32> %345, <16 x i32>* %348, align 64, !tbaa !91
  %349 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %r.i50)
  %350 = add nsw i32 %337, 176
  %351 = getelementptr inbounds i32, i32* %rows, i32 %350
  %352 = bitcast i32* %351 to <16 x i32>*
  store <16 x i32> %349, <16 x i32>* %352, align 64, !tbaa !91
  %t62 = add nsw i32 %180, %110
  %353 = add i32 %89, %t62
  %354 = getelementptr inbounds i8, i8* %1, i32 %353
  %355 = bitcast i8* %354 to <16 x i32>*
  %356 = load <16 x i32>, <16 x i32>* %355, align 1, !tbaa !88
  %357 = tail call <32 x i32> @llvm.hexagon.V6.vunpackub(<16 x i32> %356) #9
  %358 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %357)
  %359 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %357)
  %360 = tail call <32 x i32> @llvm.hexagon.V6.vunpackuh(<16 x i32> %359) #9
  %361 = tail call <32 x i32> @llvm.hexagon.V6.vunpackuh(<16 x i32> %358) #9
  %362 = add nsw i32 %t62, %t32
  %363 = getelementptr inbounds i8, i8* %1, i32 %362
  %364 = bitcast i8* %363 to <16 x i32>*
  %365 = load <16 x i32>, <16 x i32>* %364, align 1, !tbaa !88
  %366 = add i32 %t62, -3
  %367 = sub i32 %366, %11
  %368 = getelementptr inbounds i8, i8* %1, i32 %367
  %369 = bitcast i8* %368 to <16 x i32>*
  %370 = load <16 x i32>, <16 x i32>* %369, align 1, !tbaa !88
  %371 = getelementptr inbounds i8, i8* %1, i32 %366
  %372 = bitcast i8* %371 to <16 x i32>*
  %373 = load <16 x i32>, <16 x i32>* %372, align 1, !tbaa !88
  %374 = add i32 %366, %64
  %375 = getelementptr inbounds i8, i8* %1, i32 %374
  %376 = bitcast i8* %375 to <16 x i32>*
  %377 = load <16 x i32>, <16 x i32>* %376, align 1, !tbaa !88
  %378 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %373, <16 x i32> %365, i32 -1)
  %379 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %377, <16 x i32> %370, i32 -1)
  %380 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %378)
  %381 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %378)
  %382 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %379)
  %383 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %379)
  %384 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %383, <16 x i32> %381, i32 -1)
  %385 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %382, <16 x i32> %380, i32 -1)
  %386 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %360)
  %387 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %384)
  %388 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %384)
  %389 = tail call <16 x i32> @llvm.hexagon.V6.vrmpybus.acc(<16 x i32> %386, <16 x i32> %388, i32 101977862) #9
  %390 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %360)
  %391 = tail call <16 x i32> @llvm.hexagon.V6.vrmpybus.acc(<16 x i32> %390, <16 x i32> %387, i32 101977862) #9
  %392 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %361)
  %393 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %385)
  %394 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %385)
  %395 = tail call <16 x i32> @llvm.hexagon.V6.vrmpybus.acc(<16 x i32> %392, <16 x i32> %394, i32 101977862) #9
  %396 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %361)
  %397 = tail call <16 x i32> @llvm.hexagon.V6.vrmpybus.acc(<16 x i32> %396, <16 x i32> %393, i32 101977862) #9
  %398 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %391, <16 x i32> %389)
  %399 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %397, <16 x i32> %395)
  %e.i45 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %398) #9
  %o.i46 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %398) #9
  %r.i47 = tail call <32 x i32> @llvm.hexagon.V6.vdealvdd(<16 x i32> %o.i46, <16 x i32> %e.i45, i32 -4) #9
  %e.i42 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %399) #9
  %o.i43 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %399) #9
  %r.i44 = tail call <32 x i32> @llvm.hexagon.V6.vdealvdd(<16 x i32> %o.i43, <16 x i32> %e.i42, i32 -4) #9
  %400 = add i32 %90, %t62
  %401 = getelementptr inbounds i8, i8* %1, i32 %400
  %402 = bitcast i8* %401 to <16 x i32>*
  %403 = load <16 x i32>, <16 x i32>* %402, align 1, !tbaa !88
  %404 = tail call <32 x i32> @llvm.hexagon.V6.vunpackub(<16 x i32> %403) #9
  %405 = add nsw i32 %t62, %t29
  %406 = getelementptr inbounds i8, i8* %1, i32 %405
  %407 = bitcast i8* %406 to <16 x i32>*
  %408 = load <16 x i32>, <16 x i32>* %407, align 1, !tbaa !88
  %409 = tail call <32 x i32> @llvm.hexagon.V6.vunpackub(<16 x i32> %408) #9
  %410 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %404)
  %411 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %404)
  %412 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %409)
  %413 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %409)
  %dv1.i40 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %413, <16 x i32> %411) #9
  %res.i41 = tail call <32 x i32> @llvm.hexagon.V6.vmpahb.acc(<32 x i32> %r.i47, <32 x i32> %dv1.i40, i32 17760527) #9
  %dv1.i = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %412, <16 x i32> %410) #9
  %res.i = tail call <32 x i32> @llvm.hexagon.V6.vmpahb.acc(<32 x i32> %r.i44, <32 x i32> %dv1.i, i32 17760527) #9
  %e.i37 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %res.i41) #9
  %o.i38 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %res.i41) #9
  %r.i39 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %o.i38, <16 x i32> %e.i37, i32 -4) #9
  %e.i34 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %res.i) #9
  %o.i35 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %res.i) #9
  %r.i36 = tail call <32 x i32> @llvm.hexagon.V6.vshuffvdd(<16 x i32> %o.i35, <16 x i32> %e.i34, i32 -4) #9
  %414 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %r.i39)
  %415 = add nsw i32 %rows.s0.x.x, %92
  %416 = shl nsw i32 %415, 6
  %417 = add nsw i32 %416, 192
  %418 = getelementptr inbounds i32, i32* %rows, i32 %417
  %419 = bitcast i32* %418 to <16 x i32>*
  store <16 x i32> %414, <16 x i32>* %419, align 64, !tbaa !91
  %420 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %r.i39)
  %421 = add nsw i32 %416, 208
  %422 = getelementptr inbounds i32, i32* %rows, i32 %421
  %423 = bitcast i32* %422 to <16 x i32>*
  store <16 x i32> %420, <16 x i32>* %423, align 64, !tbaa !91
  %424 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %r.i36)
  %425 = add nsw i32 %416, 224
  %426 = getelementptr inbounds i32, i32* %rows, i32 %425
  %427 = bitcast i32* %426 to <16 x i32>*
  store <16 x i32> %424, <16 x i32>* %427, align 64, !tbaa !91
  %428 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %r.i36)
  %429 = add nsw i32 %416, 240
  %430 = getelementptr inbounds i32, i32* %rows, i32 %429
  %431 = bitcast i32* %430 to <16 x i32>*
  store <16 x i32> %428, <16 x i32>* %431, align 64, !tbaa !91
  %432 = add nuw nsw i32 %rows.s0.x.x, 1
  %433 = icmp eq i32 %432, %87
  br i1 %433, label %"consume rows", label %"for rows.s0.x.x"

"consume rows":                                   ; preds = %"for rows.s0.x.x", %"for output.s0.y.y"
  br i1 %93, label %true_bb, label %after_bb

true_bb:                                          ; preds = %"consume rows"
  %434 = getelementptr inbounds i32, i32* %rows, i32 %95
  %435 = bitcast i32* %434 to <16 x i32>*
  %436 = load <16 x i32>, <16 x i32>* %435, align 64, !tbaa !91
  %437 = getelementptr inbounds i32, i32* %rows, i32 %97
  %438 = bitcast i32* %437 to <16 x i32>*
  %439 = load <16 x i32>, <16 x i32>* %438, align 64, !tbaa !91
  %440 = getelementptr inbounds i32, i32* %rows, i32 %74
  %441 = bitcast i32* %440 to <16 x i32>*
  %442 = load <16 x i32>, <16 x i32>* %441, align 64, !tbaa !91
  %443 = bitcast i8* %103 to <16 x i32>*
  %444 = load <16 x i32>, <16 x i32>* %443, align 64, !tbaa !93
  %445 = mul nsw i32 %output.s0.y.y, %27
  %446 = shl nsw i32 %output.s0.y.y, 2
  %447 = or i32 %446, 1
  %448 = mul nsw i32 %447, %27
  %449 = or i32 %446, 2
  %450 = mul nsw i32 %449, %27
  %451 = or i32 %446, 3
  %452 = mul nsw i32 %451, %27
  br label %"for output.s0.x.x"

after_bb:                                         ; preds = %"consume rows"
  %453 = icmp eq i8* %103, null
  br i1 %453, label %call_destructor.exit, label %after_bb.thread

after_bb.thread:                                  ; preds = %"for output.s0.x.x", %after_bb
  tail call void @halide_free(i8* null, i8* nonnull %103) #11
  br label %call_destructor.exit

call_destructor.exit:                             ; preds = %after_bb, %after_bb.thread
  %454 = add nuw nsw i32 %output.s0.y.y, 1
  %455 = icmp eq i32 %454, %66
  br i1 %455, label %call_destructor.exit96, label %"for output.s0.y.y"

"for output.s0.x.x":                              ; preds = %true_bb, %"for output.s0.x.x"
  %c210.sroa.0.0 = phi <16 x i32> [ %442, %true_bb ], [ %466, %"for output.s0.x.x" ]
  %c311.sroa.0.0 = phi <16 x i32> [ %439, %true_bb ], [ %472, %"for output.s0.x.x" ]
  %c19.sroa.0.0 = phi <16 x i32> [ %444, %true_bb ], [ %460, %"for output.s0.x.x" ]
  %c412.sroa.0.0 = phi <16 x i32> [ %436, %true_bb ], [ %478, %"for output.s0.x.x" ]
  %output.s0.x.x = phi i32 [ 0, %true_bb ], [ %793, %"for output.s0.x.x" ]
  %456 = shl nsw i32 %output.s0.x.x, 6
  %457 = add nuw nsw i32 %456, 64
  %458 = getelementptr inbounds i32, i32* %rows, i32 %457
  %459 = bitcast i32* %458 to <16 x i32>*
  %460 = load <16 x i32>, <16 x i32>* %459, align 64, !tbaa !91
  %461 = add nsw i32 %output.s0.x.x, %72
  %462 = shl nsw i32 %461, 6
  %463 = add nsw i32 %462, 128
  %464 = getelementptr inbounds i32, i32* %rows, i32 %463
  %465 = bitcast i32* %464 to <16 x i32>*
  %466 = load <16 x i32>, <16 x i32>* %465, align 64, !tbaa !91
  %467 = add nsw i32 %output.s0.x.x, %91
  %468 = shl nsw i32 %467, 6
  %469 = add nsw i32 %468, 192
  %470 = getelementptr inbounds i32, i32* %rows, i32 %469
  %471 = bitcast i32* %470 to <16 x i32>*
  %472 = load <16 x i32>, <16 x i32>* %471, align 64, !tbaa !91
  %473 = add nsw i32 %output.s0.x.x, %92
  %474 = shl nsw i32 %473, 6
  %475 = add nsw i32 %474, 256
  %476 = getelementptr inbounds i32, i32* %rows, i32 %475
  %477 = bitcast i32* %476 to <16 x i32>*
  %478 = load <16 x i32>, <16 x i32>* %477, align 64, !tbaa !91
  %479 = or i32 %456, 16
  %480 = getelementptr inbounds i32, i32* %rows, i32 %479
  %481 = bitcast i32* %480 to <16 x i32>*
  %t99 = load <16 x i32>, <16 x i32>* %481, align 64, !tbaa !91
  %482 = or i32 %456, 32
  %483 = getelementptr inbounds i32, i32* %rows, i32 %482
  %484 = bitcast i32* %483 to <16 x i32>*
  %t101 = load <16 x i32>, <16 x i32>* %484, align 64, !tbaa !91
  %485 = or i32 %456, 48
  %486 = getelementptr inbounds i32, i32* %rows, i32 %485
  %487 = bitcast i32* %486 to <16 x i32>*
  %t103 = load <16 x i32>, <16 x i32>* %487, align 64, !tbaa !91
  %488 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %t99, <16 x i32> %c19.sroa.0.0)
  %489 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %t103, <16 x i32> %t101)
  %490 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t99, <16 x i32> %c19.sroa.0.0, i32 24)
  %491 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t101, <16 x i32> %t99, i32 24)
  %492 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t103, <16 x i32> %t101, i32 24)
  %493 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %460, <16 x i32> %t103, i32 24)
  %494 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t99, <16 x i32> %c19.sroa.0.0, i32 20)
  %495 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t101, <16 x i32> %t99, i32 20)
  %496 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t103, <16 x i32> %t101, i32 20)
  %497 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %460, <16 x i32> %t103, i32 20)
  %498 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %490, <16 x i32> %494, i32 393222) #9
  %499 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %491, <16 x i32> %495, i32 393222) #9
  %500 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %492, <16 x i32> %496, i32 393222) #9
  %501 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %493, <16 x i32> %497, i32 393222) #9
  %502 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t99, <16 x i32> %c19.sroa.0.0, i32 16)
  %503 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t101, <16 x i32> %t99, i32 16)
  %504 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t103, <16 x i32> %t101, i32 16)
  %505 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %460, <16 x i32> %t103, i32 16)
  %506 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %498, <16 x i32> %502, i32 983055) #9
  %507 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %499, <16 x i32> %503, i32 983055) #9
  %508 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %500, <16 x i32> %504, i32 983055) #9
  %509 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %501, <16 x i32> %505, i32 983055) #9
  %510 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t99, <16 x i32> %c19.sroa.0.0, i32 12)
  %511 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t101, <16 x i32> %t99, i32 12)
  %512 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t103, <16 x i32> %t101, i32 12)
  %513 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %460, <16 x i32> %t103, i32 12)
  %514 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %506, <16 x i32> %510, i32 1310740) #9
  %515 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %507, <16 x i32> %511, i32 1310740) #9
  %516 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %508, <16 x i32> %512, i32 1310740) #9
  %517 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %509, <16 x i32> %513, i32 1310740) #9
  %518 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t99, <16 x i32> %c19.sroa.0.0, i32 8)
  %519 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t101, <16 x i32> %t99, i32 8)
  %520 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t103, <16 x i32> %t101, i32 8)
  %521 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %460, <16 x i32> %t103, i32 8)
  %522 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %514, <16 x i32> %518, i32 983055) #9
  %523 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %515, <16 x i32> %519, i32 983055) #9
  %524 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %516, <16 x i32> %520, i32 983055) #9
  %525 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %517, <16 x i32> %521, i32 983055) #9
  %526 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %523, <16 x i32> %522)
  %527 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %525, <16 x i32> %524)
  %528 = tail call <32 x i32> @llvm.hexagon.V6.vaddw.dv(<32 x i32> %488, <32 x i32> %526) #9
  %529 = tail call <32 x i32> @llvm.hexagon.V6.vaddw.dv(<32 x i32> %489, <32 x i32> %527) #9
  %530 = tail call <16 x i32> @llvm.hexagon.V6.valignbi(<16 x i32> %t99, <16 x i32> %c19.sroa.0.0, i32 4)
  %531 = tail call <16 x i32> @llvm.hexagon.V6.valignbi(<16 x i32> %t101, <16 x i32> %t99, i32 4)
  %532 = tail call <16 x i32> @llvm.hexagon.V6.valignbi(<16 x i32> %t103, <16 x i32> %t101, i32 4)
  %533 = tail call <16 x i32> @llvm.hexagon.V6.valignbi(<16 x i32> %460, <16 x i32> %t103, i32 4)
  %534 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %528)
  %535 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %534, <16 x i32> %530, i32 393222) #9
  %536 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %528)
  %537 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %536, <16 x i32> %531, i32 393222) #9
  %538 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %529)
  %539 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %538, <16 x i32> %532, i32 393222) #9
  %540 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %529)
  %541 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %540, <16 x i32> %533, i32 393222) #9
  %542 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %537, <16 x i32> %535)
  %543 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %541, <16 x i32> %539)
  %e.i31 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %542) #9
  %o.i32 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %542) #9
  %r.i33 = tail call <32 x i32> @llvm.hexagon.V6.vdealvdd(<16 x i32> %o.i32, <16 x i32> %e.i31, i32 -4) #9
  %e.i28 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %543) #9
  %o.i29 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %543) #9
  %r.i30 = tail call <32 x i32> @llvm.hexagon.V6.vdealvdd(<16 x i32> %o.i29, <16 x i32> %e.i28, i32 -4) #9
  %544 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %r.i33) #9
  %545 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %r.i33) #9
  %546 = tail call <16 x i32> @llvm.hexagon.V6.vasrwhsat(<16 x i32> %545, <16 x i32> %544, i32 12) #9
  %547 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %r.i30) #9
  %548 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %r.i30) #9
  %549 = tail call <16 x i32> @llvm.hexagon.V6.vasrwhsat(<16 x i32> %548, <16 x i32> %547, i32 12) #9
  %550 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %549, <16 x i32> %546)
  %551 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %550) #9
  %552 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %550) #9
  %553 = tail call <16 x i32> @llvm.hexagon.V6.vpackhub.sat(<16 x i32> %551, <16 x i32> %552) #9
  %554 = shl i32 %output.s0.x.x, 4
  %555 = add nsw i32 %554, %445
  %556 = shl nsw i32 %555, 2
  %557 = getelementptr inbounds i8, i8* %13, i32 %556
  %558 = bitcast i8* %557 to <16 x i32>*
  store <16 x i32> %553, <16 x i32>* %558, align 1, !tbaa !101
  %559 = add nsw i32 %462, 80
  %560 = getelementptr inbounds i32, i32* %rows, i32 %559
  %561 = bitcast i32* %560 to <16 x i32>*
  %t108 = load <16 x i32>, <16 x i32>* %561, align 64, !tbaa !91
  %562 = add nsw i32 %462, 96
  %563 = getelementptr inbounds i32, i32* %rows, i32 %562
  %564 = bitcast i32* %563 to <16 x i32>*
  %t110 = load <16 x i32>, <16 x i32>* %564, align 64, !tbaa !91
  %565 = add nsw i32 %462, 112
  %566 = getelementptr inbounds i32, i32* %rows, i32 %565
  %567 = bitcast i32* %566 to <16 x i32>*
  %t112 = load <16 x i32>, <16 x i32>* %567, align 64, !tbaa !91
  %568 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %t108, <16 x i32> %c210.sroa.0.0)
  %569 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %t112, <16 x i32> %t110)
  %570 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t108, <16 x i32> %c210.sroa.0.0, i32 24)
  %571 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t110, <16 x i32> %t108, i32 24)
  %572 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t112, <16 x i32> %t110, i32 24)
  %573 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %466, <16 x i32> %t112, i32 24)
  %574 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t108, <16 x i32> %c210.sroa.0.0, i32 20)
  %575 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t110, <16 x i32> %t108, i32 20)
  %576 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t112, <16 x i32> %t110, i32 20)
  %577 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %466, <16 x i32> %t112, i32 20)
  %578 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %570, <16 x i32> %574, i32 393222) #9
  %579 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %571, <16 x i32> %575, i32 393222) #9
  %580 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %572, <16 x i32> %576, i32 393222) #9
  %581 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %573, <16 x i32> %577, i32 393222) #9
  %582 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t108, <16 x i32> %c210.sroa.0.0, i32 16)
  %583 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t110, <16 x i32> %t108, i32 16)
  %584 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t112, <16 x i32> %t110, i32 16)
  %585 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %466, <16 x i32> %t112, i32 16)
  %586 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %578, <16 x i32> %582, i32 983055) #9
  %587 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %579, <16 x i32> %583, i32 983055) #9
  %588 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %580, <16 x i32> %584, i32 983055) #9
  %589 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %581, <16 x i32> %585, i32 983055) #9
  %590 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t108, <16 x i32> %c210.sroa.0.0, i32 12)
  %591 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t110, <16 x i32> %t108, i32 12)
  %592 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t112, <16 x i32> %t110, i32 12)
  %593 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %466, <16 x i32> %t112, i32 12)
  %594 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %586, <16 x i32> %590, i32 1310740) #9
  %595 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %587, <16 x i32> %591, i32 1310740) #9
  %596 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %588, <16 x i32> %592, i32 1310740) #9
  %597 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %589, <16 x i32> %593, i32 1310740) #9
  %598 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t108, <16 x i32> %c210.sroa.0.0, i32 8)
  %599 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t110, <16 x i32> %t108, i32 8)
  %600 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t112, <16 x i32> %t110, i32 8)
  %601 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %466, <16 x i32> %t112, i32 8)
  %602 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %594, <16 x i32> %598, i32 983055) #9
  %603 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %595, <16 x i32> %599, i32 983055) #9
  %604 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %596, <16 x i32> %600, i32 983055) #9
  %605 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %597, <16 x i32> %601, i32 983055) #9
  %606 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %603, <16 x i32> %602)
  %607 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %605, <16 x i32> %604)
  %608 = tail call <32 x i32> @llvm.hexagon.V6.vaddw.dv(<32 x i32> %568, <32 x i32> %606) #9
  %609 = tail call <32 x i32> @llvm.hexagon.V6.vaddw.dv(<32 x i32> %569, <32 x i32> %607) #9
  %610 = tail call <16 x i32> @llvm.hexagon.V6.valignbi(<16 x i32> %t108, <16 x i32> %c210.sroa.0.0, i32 4)
  %611 = tail call <16 x i32> @llvm.hexagon.V6.valignbi(<16 x i32> %t110, <16 x i32> %t108, i32 4)
  %612 = tail call <16 x i32> @llvm.hexagon.V6.valignbi(<16 x i32> %t112, <16 x i32> %t110, i32 4)
  %613 = tail call <16 x i32> @llvm.hexagon.V6.valignbi(<16 x i32> %466, <16 x i32> %t112, i32 4)
  %614 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %608)
  %615 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %614, <16 x i32> %610, i32 393222) #9
  %616 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %608)
  %617 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %616, <16 x i32> %611, i32 393222) #9
  %618 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %609)
  %619 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %618, <16 x i32> %612, i32 393222) #9
  %620 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %609)
  %621 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %620, <16 x i32> %613, i32 393222) #9
  %622 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %617, <16 x i32> %615)
  %623 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %621, <16 x i32> %619)
  %e.i25 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %622) #9
  %o.i26 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %622) #9
  %r.i27 = tail call <32 x i32> @llvm.hexagon.V6.vdealvdd(<16 x i32> %o.i26, <16 x i32> %e.i25, i32 -4) #9
  %e.i22 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %623) #9
  %o.i23 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %623) #9
  %r.i24 = tail call <32 x i32> @llvm.hexagon.V6.vdealvdd(<16 x i32> %o.i23, <16 x i32> %e.i22, i32 -4) #9
  %624 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %r.i27) #9
  %625 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %r.i27) #9
  %626 = tail call <16 x i32> @llvm.hexagon.V6.vasrwhsat(<16 x i32> %625, <16 x i32> %624, i32 12) #9
  %627 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %r.i24) #9
  %628 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %r.i24) #9
  %629 = tail call <16 x i32> @llvm.hexagon.V6.vasrwhsat(<16 x i32> %628, <16 x i32> %627, i32 12) #9
  %630 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %629, <16 x i32> %626)
  %631 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %630) #9
  %632 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %630) #9
  %633 = tail call <16 x i32> @llvm.hexagon.V6.vpackhub.sat(<16 x i32> %631, <16 x i32> %632) #9
  %634 = add nsw i32 %456, %448
  %635 = getelementptr inbounds i8, i8* %13, i32 %634
  %636 = bitcast i8* %635 to <16 x i32>*
  store <16 x i32> %633, <16 x i32>* %636, align 1, !tbaa !101
  %637 = add nsw i32 %468, 144
  %638 = getelementptr inbounds i32, i32* %rows, i32 %637
  %639 = bitcast i32* %638 to <16 x i32>*
  %t117 = load <16 x i32>, <16 x i32>* %639, align 64, !tbaa !91
  %640 = add nsw i32 %468, 160
  %641 = getelementptr inbounds i32, i32* %rows, i32 %640
  %642 = bitcast i32* %641 to <16 x i32>*
  %t119 = load <16 x i32>, <16 x i32>* %642, align 64, !tbaa !91
  %643 = add nsw i32 %468, 176
  %644 = getelementptr inbounds i32, i32* %rows, i32 %643
  %645 = bitcast i32* %644 to <16 x i32>*
  %t121 = load <16 x i32>, <16 x i32>* %645, align 64, !tbaa !91
  %646 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %t117, <16 x i32> %c311.sroa.0.0)
  %647 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %t121, <16 x i32> %t119)
  %648 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t117, <16 x i32> %c311.sroa.0.0, i32 24)
  %649 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t119, <16 x i32> %t117, i32 24)
  %650 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t121, <16 x i32> %t119, i32 24)
  %651 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %472, <16 x i32> %t121, i32 24)
  %652 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t117, <16 x i32> %c311.sroa.0.0, i32 20)
  %653 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t119, <16 x i32> %t117, i32 20)
  %654 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t121, <16 x i32> %t119, i32 20)
  %655 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %472, <16 x i32> %t121, i32 20)
  %656 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %648, <16 x i32> %652, i32 393222) #9
  %657 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %649, <16 x i32> %653, i32 393222) #9
  %658 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %650, <16 x i32> %654, i32 393222) #9
  %659 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %651, <16 x i32> %655, i32 393222) #9
  %660 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t117, <16 x i32> %c311.sroa.0.0, i32 16)
  %661 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t119, <16 x i32> %t117, i32 16)
  %662 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t121, <16 x i32> %t119, i32 16)
  %663 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %472, <16 x i32> %t121, i32 16)
  %664 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %656, <16 x i32> %660, i32 983055) #9
  %665 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %657, <16 x i32> %661, i32 983055) #9
  %666 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %658, <16 x i32> %662, i32 983055) #9
  %667 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %659, <16 x i32> %663, i32 983055) #9
  %668 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t117, <16 x i32> %c311.sroa.0.0, i32 12)
  %669 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t119, <16 x i32> %t117, i32 12)
  %670 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t121, <16 x i32> %t119, i32 12)
  %671 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %472, <16 x i32> %t121, i32 12)
  %672 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %664, <16 x i32> %668, i32 1310740) #9
  %673 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %665, <16 x i32> %669, i32 1310740) #9
  %674 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %666, <16 x i32> %670, i32 1310740) #9
  %675 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %667, <16 x i32> %671, i32 1310740) #9
  %676 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t117, <16 x i32> %c311.sroa.0.0, i32 8)
  %677 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t119, <16 x i32> %t117, i32 8)
  %678 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t121, <16 x i32> %t119, i32 8)
  %679 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %472, <16 x i32> %t121, i32 8)
  %680 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %672, <16 x i32> %676, i32 983055) #9
  %681 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %673, <16 x i32> %677, i32 983055) #9
  %682 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %674, <16 x i32> %678, i32 983055) #9
  %683 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %675, <16 x i32> %679, i32 983055) #9
  %684 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %681, <16 x i32> %680)
  %685 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %683, <16 x i32> %682)
  %686 = tail call <32 x i32> @llvm.hexagon.V6.vaddw.dv(<32 x i32> %646, <32 x i32> %684) #9
  %687 = tail call <32 x i32> @llvm.hexagon.V6.vaddw.dv(<32 x i32> %647, <32 x i32> %685) #9
  %688 = tail call <16 x i32> @llvm.hexagon.V6.valignbi(<16 x i32> %t117, <16 x i32> %c311.sroa.0.0, i32 4)
  %689 = tail call <16 x i32> @llvm.hexagon.V6.valignbi(<16 x i32> %t119, <16 x i32> %t117, i32 4)
  %690 = tail call <16 x i32> @llvm.hexagon.V6.valignbi(<16 x i32> %t121, <16 x i32> %t119, i32 4)
  %691 = tail call <16 x i32> @llvm.hexagon.V6.valignbi(<16 x i32> %472, <16 x i32> %t121, i32 4)
  %692 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %686)
  %693 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %692, <16 x i32> %688, i32 393222) #9
  %694 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %686)
  %695 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %694, <16 x i32> %689, i32 393222) #9
  %696 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %687)
  %697 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %696, <16 x i32> %690, i32 393222) #9
  %698 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %687)
  %699 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %698, <16 x i32> %691, i32 393222) #9
  %700 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %695, <16 x i32> %693)
  %701 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %699, <16 x i32> %697)
  %e.i19 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %700) #9
  %o.i20 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %700) #9
  %r.i21 = tail call <32 x i32> @llvm.hexagon.V6.vdealvdd(<16 x i32> %o.i20, <16 x i32> %e.i19, i32 -4) #9
  %e.i16 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %701) #9
  %o.i17 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %701) #9
  %r.i18 = tail call <32 x i32> @llvm.hexagon.V6.vdealvdd(<16 x i32> %o.i17, <16 x i32> %e.i16, i32 -4) #9
  %702 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %r.i21) #9
  %703 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %r.i21) #9
  %704 = tail call <16 x i32> @llvm.hexagon.V6.vasrwhsat(<16 x i32> %703, <16 x i32> %702, i32 12) #9
  %705 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %r.i18) #9
  %706 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %r.i18) #9
  %707 = tail call <16 x i32> @llvm.hexagon.V6.vasrwhsat(<16 x i32> %706, <16 x i32> %705, i32 12) #9
  %708 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %707, <16 x i32> %704)
  %709 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %708) #9
  %710 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %708) #9
  %711 = tail call <16 x i32> @llvm.hexagon.V6.vpackhub.sat(<16 x i32> %709, <16 x i32> %710) #9
  %712 = add nsw i32 %456, %450
  %713 = getelementptr inbounds i8, i8* %13, i32 %712
  %714 = bitcast i8* %713 to <16 x i32>*
  store <16 x i32> %711, <16 x i32>* %714, align 1, !tbaa !101
  %715 = add nsw i32 %474, 208
  %716 = getelementptr inbounds i32, i32* %rows, i32 %715
  %717 = bitcast i32* %716 to <16 x i32>*
  %t126 = load <16 x i32>, <16 x i32>* %717, align 64, !tbaa !91
  %718 = add nsw i32 %474, 224
  %719 = getelementptr inbounds i32, i32* %rows, i32 %718
  %720 = bitcast i32* %719 to <16 x i32>*
  %t128 = load <16 x i32>, <16 x i32>* %720, align 64, !tbaa !91
  %721 = add nsw i32 %474, 240
  %722 = getelementptr inbounds i32, i32* %rows, i32 %721
  %723 = bitcast i32* %722 to <16 x i32>*
  %t130 = load <16 x i32>, <16 x i32>* %723, align 64, !tbaa !91
  %724 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %t126, <16 x i32> %c412.sroa.0.0)
  %725 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %t130, <16 x i32> %t128)
  %726 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t126, <16 x i32> %c412.sroa.0.0, i32 24)
  %727 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t128, <16 x i32> %t126, i32 24)
  %728 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t130, <16 x i32> %t128, i32 24)
  %729 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %478, <16 x i32> %t130, i32 24)
  %730 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t126, <16 x i32> %c412.sroa.0.0, i32 20)
  %731 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t128, <16 x i32> %t126, i32 20)
  %732 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t130, <16 x i32> %t128, i32 20)
  %733 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %478, <16 x i32> %t130, i32 20)
  %734 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %726, <16 x i32> %730, i32 393222) #9
  %735 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %727, <16 x i32> %731, i32 393222) #9
  %736 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %728, <16 x i32> %732, i32 393222) #9
  %737 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %729, <16 x i32> %733, i32 393222) #9
  %738 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t126, <16 x i32> %c412.sroa.0.0, i32 16)
  %739 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t128, <16 x i32> %t126, i32 16)
  %740 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t130, <16 x i32> %t128, i32 16)
  %741 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %478, <16 x i32> %t130, i32 16)
  %742 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %734, <16 x i32> %738, i32 983055) #9
  %743 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %735, <16 x i32> %739, i32 983055) #9
  %744 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %736, <16 x i32> %740, i32 983055) #9
  %745 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %737, <16 x i32> %741, i32 983055) #9
  %746 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t126, <16 x i32> %c412.sroa.0.0, i32 12)
  %747 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t128, <16 x i32> %t126, i32 12)
  %748 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t130, <16 x i32> %t128, i32 12)
  %749 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %478, <16 x i32> %t130, i32 12)
  %750 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %742, <16 x i32> %746, i32 1310740) #9
  %751 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %743, <16 x i32> %747, i32 1310740) #9
  %752 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %744, <16 x i32> %748, i32 1310740) #9
  %753 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %745, <16 x i32> %749, i32 1310740) #9
  %754 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t126, <16 x i32> %c412.sroa.0.0, i32 8)
  %755 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t128, <16 x i32> %t126, i32 8)
  %756 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %t130, <16 x i32> %t128, i32 8)
  %757 = tail call <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32> %478, <16 x i32> %t130, i32 8)
  %758 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %750, <16 x i32> %754, i32 983055) #9
  %759 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %751, <16 x i32> %755, i32 983055) #9
  %760 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %752, <16 x i32> %756, i32 983055) #9
  %761 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %753, <16 x i32> %757, i32 983055) #9
  %762 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %759, <16 x i32> %758)
  %763 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %761, <16 x i32> %760)
  %764 = tail call <32 x i32> @llvm.hexagon.V6.vaddw.dv(<32 x i32> %724, <32 x i32> %762) #9
  %765 = tail call <32 x i32> @llvm.hexagon.V6.vaddw.dv(<32 x i32> %725, <32 x i32> %763) #9
  %766 = tail call <16 x i32> @llvm.hexagon.V6.valignbi(<16 x i32> %t126, <16 x i32> %c412.sroa.0.0, i32 4)
  %767 = tail call <16 x i32> @llvm.hexagon.V6.valignbi(<16 x i32> %t128, <16 x i32> %t126, i32 4)
  %768 = tail call <16 x i32> @llvm.hexagon.V6.valignbi(<16 x i32> %t130, <16 x i32> %t128, i32 4)
  %769 = tail call <16 x i32> @llvm.hexagon.V6.valignbi(<16 x i32> %478, <16 x i32> %t130, i32 4)
  %770 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %764)
  %771 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %770, <16 x i32> %766, i32 393222) #9
  %772 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %764)
  %773 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %772, <16 x i32> %767, i32 393222) #9
  %774 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %765)
  %775 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %774, <16 x i32> %768, i32 393222) #9
  %776 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %765)
  %777 = tail call <16 x i32> @llvm.hexagon.V6.vmpyiwh.acc(<16 x i32> %776, <16 x i32> %769, i32 393222) #9
  %778 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %773, <16 x i32> %771)
  %779 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %777, <16 x i32> %775)
  %e.i13 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %778) #9
  %o.i14 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %778) #9
  %r.i15 = tail call <32 x i32> @llvm.hexagon.V6.vdealvdd(<16 x i32> %o.i14, <16 x i32> %e.i13, i32 -4) #9
  %e.i = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %779) #9
  %o.i = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %779) #9
  %r.i = tail call <32 x i32> @llvm.hexagon.V6.vdealvdd(<16 x i32> %o.i, <16 x i32> %e.i, i32 -4) #9
  %780 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %r.i15) #9
  %781 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %r.i15) #9
  %782 = tail call <16 x i32> @llvm.hexagon.V6.vasrwhsat(<16 x i32> %781, <16 x i32> %780, i32 12) #9
  %783 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %r.i) #9
  %784 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %r.i) #9
  %785 = tail call <16 x i32> @llvm.hexagon.V6.vasrwhsat(<16 x i32> %784, <16 x i32> %783, i32 12) #9
  %786 = tail call <32 x i32> @llvm.hexagon.V6.vcombine(<16 x i32> %785, <16 x i32> %782)
  %787 = tail call <16 x i32> @llvm.hexagon.V6.hi(<32 x i32> %786) #9
  %788 = tail call <16 x i32> @llvm.hexagon.V6.lo(<32 x i32> %786) #9
  %789 = tail call <16 x i32> @llvm.hexagon.V6.vpackhub.sat(<16 x i32> %787, <16 x i32> %788) #9
  %790 = add nsw i32 %456, %452
  %791 = getelementptr inbounds i8, i8* %13, i32 %790
  %792 = bitcast i8* %791 to <16 x i32>*
  store <16 x i32> %789, <16 x i32>* %792, align 1, !tbaa !101
  %793 = add nuw nsw i32 %output.s0.x.x, 1
  %794 = icmp eq i32 %793, %72
  br i1 %794, label %after_bb.thread, label %"for output.s0.x.x"
}

; Function Attrs: nounwind readnone speculatable
declare i64 @llvm.cttz.i64(i64, i1) #6

; Function Attrs: nounwind readnone
declare <16 x i32> @llvm.hexagon.V6.valignb(<16 x i32>, <16 x i32>, i32) #7

; Function Attrs: nounwind readnone
declare <16 x i32> @llvm.hexagon.V6.valignbi(<16 x i32>, <16 x i32>, i32) #7

; Function Attrs: nounwind
define i32 @gaussian7x7_hvx64_argv(i8** nocapture readonly) local_unnamed_addr #9 {
entry:
  %1 = bitcast i8** %0 to %struct.halide_buffer_t**
  %2 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %1, align 4
  %3 = getelementptr i8*, i8** %0, i32 1
  %4 = bitcast i8** %3 to %struct.halide_buffer_t**
  %5 = load %struct.halide_buffer_t*, %struct.halide_buffer_t** %4, align 4
  %6 = tail call i32 @gaussian7x7_hvx64(%struct.halide_buffer_t* %2, %struct.halide_buffer_t* %5) #13
  ret i32 0
}

; Function Attrs: norecurse nounwind readnone
define nonnull %struct.halide_filter_metadata_t* @gaussian7x7_hvx64_metadata() local_unnamed_addr #10 {
entry:
  ret %struct.halide_filter_metadata_t* @gaussian7x7_hvx64_metadata_storage
}

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1) #4

attributes #0 = { nounwind "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { nounwind "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #2 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { norecurse nounwind "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #4 = { argmemonly nounwind }
attributes #5 = { nounwind readonly "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #6 = { nounwind readnone speculatable }
attributes #7 = { nounwind readnone }
attributes #8 = { nounwind "reciprocal-estimates"="none" }
attributes #9 = { nounwind }
attributes #10 = { norecurse nounwind readnone }
attributes #11 = { nobuiltin nounwind }
attributes #12 = { nobuiltin }
attributes #13 = { noinline }

!llvm.module.flags = !{!0, !1, !2, !3, !4, !5, !6}
!llvm.ident = !{!7, !7, !7, !7, !7, !7, !7, !7, !7, !7, !7, !7, !7, !7, !7, !7, !7, !7, !7, !7, !7}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 1}
!2 = !{i32 2, !"halide_use_soft_float_abi", i32 0}
!3 = !{i32 2, !"halide_mcpu", !"hexagonv60"}
!4 = !{i32 2, !"halide_mattrs", !"+hvx-length64b,+long-calls"}
!5 = !{i32 2, !"halide_use_pic", i32 1}
!6 = !{i32 2, !"halide_per_instruction_fast_math_flags", i32 0}
!7 = !{!"clang version 8.0.0-3~ubuntu18.04.2 (tags/RELEASE_800/final)"}
!8 = !{!9, !9, i64 0}
!9 = !{!"any pointer", !10, i64 0}
!10 = !{!"omnipotent char", !11, i64 0}
!11 = !{!"Simple C++ TBAA"}
!12 = !{!13, !13, i64 0}
!13 = !{!"int", !10, i64 0}
!14 = !{!15, !15, i64 0}
!15 = !{!"bool", !10, i64 0}
!16 = !{i8 0, i8 2}
!17 = !{!10, !10, i64 0}
!18 = !{!19, !19, i64 0}
!19 = !{!"double", !10, i64 0}
!20 = !{!21, !21, i64 0}
!21 = !{!"long long", !10, i64 0}
!22 = distinct !{!22, !23}
!23 = !{!"llvm.loop.unroll.disable"}
!24 = !{!25, !10, i64 0}
!25 = !{!"_ZTS13halide_type_t", !10, i64 0, !10, i64 1, !26, i64 2}
!26 = !{!"short", !10, i64 0}
!27 = !{!25, !10, i64 1}
!28 = !{!25, !26, i64 2}
!29 = !{!30, !21, i64 0}
!30 = !{!"_ZTS15halide_buffer_t", !21, i64 0, !9, i64 8, !9, i64 12, !21, i64 16, !25, i64 24, !13, i64 28, !9, i64 32, !9, i64 36}
!31 = !{!30, !9, i64 8}
!32 = !{!30, !9, i64 12}
!33 = !{!30, !21, i64 16}
!34 = !{!30, !13, i64 28}
!35 = !{!30, !9, i64 32}
!36 = !{!37, !13, i64 0}
!37 = !{!"_ZTS18halide_dimension_t", !13, i64 0, !13, i64 4, !13, i64 8, !13, i64 12}
!38 = !{!37, !13, i64 4}
!39 = !{!37, !13, i64 8}
!40 = !{!41, !9, i64 0}
!41 = !{!"_ZTS29halide_device_allocation_pool", !9, i64 0, !9, i64 4}
!42 = !{!41, !9, i64 4}
!43 = !{!44, !21, i64 0}
!44 = !{!"_ZTSN6Halide7Runtime8Internal11device_copyE", !21, i64 0, !21, i64 8, !21, i64 16, !10, i64 24, !10, i64 152, !10, i64 280, !21, i64 408}
!45 = !{!44, !21, i64 8}
!46 = !{!44, !21, i64 408}
!47 = !{!44, !21, i64 16}
!48 = distinct !{!48, !23}
!49 = distinct !{!49, !23}
!50 = !{i64 0, i64 8, !20, i64 8, i64 8, !20, i64 16, i64 8, !20, i64 24, i64 128, !17, i64 152, i64 128, !17, i64 280, i64 128, !17, i64 408, i64 8, !20}
!51 = !{!52, !9, i64 60}
!52 = !{!"_ZTS25halide_device_interface_t", !9, i64 0, !9, i64 4, !9, i64 8, !9, i64 12, !9, i64 16, !9, i64 20, !9, i64 24, !9, i64 28, !9, i64 32, !9, i64 36, !9, i64 40, !9, i64 44, !9, i64 48, !9, i64 52, !9, i64 56, !9, i64 60}
!53 = !{!54, !9, i64 24}
!54 = !{!"_ZTS30halide_device_interface_impl_t", !9, i64 0, !9, i64 4, !9, i64 8, !9, i64 12, !9, i64 16, !9, i64 20, !9, i64 24, !9, i64 28, !9, i64 32, !9, i64 36, !9, i64 40, !9, i64 44, !9, i64 48, !9, i64 52, !9, i64 56, !9, i64 60}
!55 = !{!54, !9, i64 20}
!56 = !{!54, !9, i64 28}
!57 = !{!54, !9, i64 0}
!58 = !{!54, !9, i64 8}
!59 = !{!54, !9, i64 4}
!60 = !{!54, !9, i64 16}
!61 = !{!54, !9, i64 12}
!62 = !{!54, !9, i64 32}
!63 = !{!54, !9, i64 36}
!64 = distinct !{!64, !23}
!65 = distinct !{!65, !23}
!66 = !{!54, !9, i64 56}
!67 = !{!54, !9, i64 60}
!68 = !{!54, !9, i64 40}
!69 = !{!54, !9, i64 44}
!70 = !{!54, !9, i64 48}
!71 = !{!54, !9, i64 52}
!72 = !{i32 22, i32 33}
!73 = !{!74, !13, i64 60}
!74 = !{!"_ZTS8buffer_t", !21, i64 0, !9, i64 8, !10, i64 12, !10, i64 28, !10, i64 44, !13, i64 60, !15, i64 64, !15, i64 65, !10, i64 66}
!75 = !{!74, !9, i64 8}
!76 = !{!74, !21, i64 0}
!77 = !{!78, !21, i64 0}
!78 = !{!"_ZTSN6Halide7Runtime8Internal15old_dev_wrapperE", !21, i64 0, !9, i64 8}
!79 = !{!78, !9, i64 8}
!80 = distinct !{!80, !23}
!81 = !{!74, !15, i64 64}
!82 = !{!74, !15, i64 65}
!83 = !{!30, !9, i64 36}
!84 = !{i64 0, i64 64}
!85 = !{!"branch_weights", i32 0, i32 1073741824}
!86 = !{!"branch_weights", i32 1073741824, i32 0}
!87 = !{i32 2204}
!88 = !{!89, !89, i64 0}
!89 = !{!"input", !90, i64 0}
!90 = !{!"Halide buffer"}
!91 = !{!92, !92, i64 0}
!92 = !{!"rows", !90, i64 0}
!93 = !{!94, !94, i64 0}
!94 = !{!"rows.width16.base0", !95, i64 0}
!95 = !{!"rows.width32.base0", !96, i64 0}
!96 = !{!"rows.width64.base0", !97, i64 0}
!97 = !{!"rows.width128.base0", !98, i64 0}
!98 = !{!"rows.width256.base0", !99, i64 0}
!99 = !{!"rows.width512.base0", !100, i64 0}
!100 = !{!"rows.width1024.base0", !92, i64 0}
!101 = !{!102, !102, i64 0}
!102 = !{!"output", !90, i64 0}
Module.compile(): object_name tmp/gaussian7x7_hvx64.o
emit_file.Compiling to native code...
Target triple: hexagon-unknown--elf
Cloning module gaussian7x7_hvx64
Module.compile(): assembly_name tmp/gaussian7x7_hvx64.s
emit_file.Compiling to native code...
Target triple: hexagon-unknown--elf
Cloning module gaussian7x7_hvx64
Module.compile(): bitcode_name tmp/gaussian7x7_hvx64.bc
Module.compile(): c_header_name tmp/gaussian7x7_hvx64.h
